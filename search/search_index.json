{
    "docs": [
        {
            "location": "/", 
            "text": "Getting Started\n\n\nIntroduction\n\n\nWelcome to the St. Jude Cloud documentation! Here, you'll find the\nauthoritative information for accessing data, running analysis tools,\nand exploring results files in St. Jude Cloud. For a brief overview of\neverything St. Jude Cloud provides, we recommend that you visit the video\non \nour home page\n.\n\n\n\n\nNote\n\n\nThroughout the site, you might see some \"Todo\" notices. These are notes\nto ourself about pieces of the documentation that are not complete yet.\nPlease bear with us as we fill in these gaps!\n\n\n\n\nFeatures\n\n\nYou can leverage many different capabilities of St. Jude Cloud in your research, such as:\n\n\n\n\nExplore datasets by diagnosis, publication, or curated dataset.\n\n\nRun your tools on \nour\n data by requesting data in a secure cloud environment (\nrequesting our data\n and \npackaging your tools\n).\n\n\nRun our validated end-to-end workflows on \nyour\n data (\nexample\n). All of the workflows we offer are under \"Tool Guides\" to the left of this text.\n\n\nExplore St. Jude data that we have packaged for the community in our visualizations.\n\n\nLook at \nPecan\n for pediatric cancer data.\n\n\n\n\n\n\n\n\nContact Us\n\n\nAny questions, comments, or concerns can be directed to \nour \"Contact Us\" form\n.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to the St. Jude Cloud documentation! Here, you'll find the\nauthoritative information for accessing data, running analysis tools,\nand exploring results files in St. Jude Cloud. For a brief overview of\neverything St. Jude Cloud provides, we recommend that you visit the video\non  our home page .   Note  Throughout the site, you might see some \"Todo\" notices. These are notes\nto ourself about pieces of the documentation that are not complete yet.\nPlease bear with us as we fill in these gaps!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#features", 
            "text": "You can leverage many different capabilities of St. Jude Cloud in your research, such as:   Explore datasets by diagnosis, publication, or curated dataset.  Run your tools on  our  data by requesting data in a secure cloud environment ( requesting our data  and  packaging your tools ).  Run our validated end-to-end workflows on  your  data ( example ). All of the workflows we offer are under \"Tool Guides\" to the left of this text.  Explore St. Jude data that we have packaged for the community in our visualizations.  Look at  Pecan  for pediatric cancer data.", 
            "title": "Features"
        }, 
        {
            "location": "/#contact-us", 
            "text": "Any questions, comments, or concerns can be directed to  our \"Contact Us\" form .", 
            "title": "Contact Us"
        }, 
        {
            "location": "/guides/data/data-request/", 
            "text": "Introduction\n\n\n\n\nSummary\n\n\n\n\nData in St. Jude Cloud is grouped into different \nData Access Units (DAUs)\n which usually correspond to large-scale sequencing initiatives at St. Jude. \n\n\nIndividuals can \napply for access\n to DAUs on a case-by-case basis for a specific amount of time (usually 1 year).\n\n\nAccess to data in a given DAU is assessed by the corresponding \nData Access Committee\n who reviews a variety of factors to grant access.\n\n\nThere are a number of terms of use and restrictions outlined in the \ndata access agreement\n. Everyone who will be working with the data must understand and agree to these terms.\n\n\n\n\n\n\nCreating a data request is the premier way to access raw St. Jude next \ngeneration sequencing data in the cloud. You can get a \nfree\n copy of \nthe data in a secure cloud environment powered by \nMicrosoft Azure\n and \n\nDNAnexus\n, or you can elect to download the data to your local computing \nenvironment.\n\n\n\n\nNote\n\n\nIf you would like to download the data to local storage, there are\nextra steps you'll need to follow such as \ngetting additional signatures\n\non your data access agreement. We recommend that you work with the data\nin the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow \nthis guide\n to get started.\n\n\n\n\nSelecting Data\n\n\nThere are two ways to make your data selection. You can peruse our raw genomic data by diagnosis, publication, or dataset using our \nData Browser\n, a tabular view with a number of filtering options. Or you can select samples associated with specific diagnoses, gene expression, or gene mutations while exploring curated data from the donut and bubble charts on the \nPediatric Cancer portal (PeCan)\n homepage.\n\n\nSelecting Data in the Data Browser\n\n\nGo to the Data Browser \nhere\n, or navigate there from the St. Jude Cloud home page by clicking Access Data and then Explore Data.\n\n\nFrom the Data Browser, you can view samples grouped by Diagnosis, Publication, or \nDataset\n by toggling the tabs above the table. Use the search bar to look for something specific. Search the publication tab by title or pubmed ID.\n\n\n\n\nYou can further refine your data selection by using the filters for sequencing type, sample type, file type, and tissue type on the left side bar. Filters of the same type apply using \u201cOR\u201d logic. Filters of different types apply using \u201cAND\u201d logic. Note that filtering is dynamic, so as you make selections the table will update to show all of the files we have that match your filters. Filters reset when you move from tab to tab.\n\n\n\n\nThe summary panel above the filters in the left side bar shows statistics about the data currently displayed in the table. As you can see in gifs above, this panel updates as you change what data is displayed by switching tabs, searching, filtering, or making selections.\n\n\nSelecting Data via PeCan\n\n\nGo to the \nPeCan homepage\n. A guide to interpreting the donut and bubble chart visualizations on the homepage can be found \nhere\n.\n\n\nUsing these visualizations along with ProteinPaint, you can:\n\n\n\n\nAdd samples to your cart by diagnosis.\n\n\n\n\n\n    \n\n\n\n\n\n\n\nAdd samples to your cart by gene mutation.\n\n\n\n\n\n    \n\n\n\n\n\n\n\nAdd samples to your cart by gene expression.\n\n\n\n\n\n    \n\n\n\n\n\nClicking \nSubmit to SJCloud\n from the PeCan checkout window will land you back in the Data Browser with your checked out data selected.\n\n\nRequesting Data\n\n\nOnce you have made your selections, click the red \nRequest Data\n button at the bottom of the table. \n\n\n\n\n\n\nWarning\n\n\nYou must have created an account and be logged in to make a data request. If you have not yet created an account or you are not logged in, the red \nRequest Data\n button will say \nLog In\n.\n\n\n\n\nOn the Request Data page fill out your name, institution, and project name. Give your data request a project name that makes sense to you as this will be the name of the DNAnexus project to which the data will be vended.\n\n\n\n\n\n\nData Access Approval\n\n\nIf you are requesting access to a dataset you have not yet been approved for, \nyou will see a section called \nControlled Access Data\n. Under this section, there is a bulleted list indicating the dataset(s) or \nData Access Unit(s)\n you must request access to by submitting a form called the \nData Access Agreememnt (DAA)\n. Please use this list to fill in the Datasets section of the DAA. For more information on filling out this form, see \nFilling out the DAA\n. \nYou must upload a DAA to continue.\n\n\n\n\nFinally, click the green button. If you already have access to the data you selected in the browser, the button will read \nGet Data Now\n. If you are submitting a DAA and requesting data access, the button will read \nSubmit Request\n.\n\n\nThis will direct you to the \nManage Data\n page where you can see the status of the data request you just made as well the history of any of your previous data requests. \n\n\n\n\nIf you already have access to the data that you requested, your data will be vended to you immediately. Otherwise, the status of your request will say \nPending\n while your request is routed to the respective \nData Access Committee(s)\n for evaluation. Request approval typically takes a week or two if your data access agreement is correctly and completely filled out. You will receive automated emails from notifications@stjude.cloud at the time that your request is recieved and once your request is approved.\n\n\n\n\nNote\n\n\nIf you receive an email from us that your DAA is incomplete, you may edit your DAA and upload the revised copy using the 'Add a Form' button the on Manage Data page. \n\n\n\n\nViewing your Data\n\n\nOnce your request to access data is approved, the data will be vended to your DNAnexus account in a project with the project name that you entered on the Request Data page. You can follow the link in the email from notifications@stjude.cloud to view your DNAnexus project page.", 
            "title": "Making a Data Request"
        }, 
        {
            "location": "/guides/data/data-request/#introduction", 
            "text": "Summary   Data in St. Jude Cloud is grouped into different  Data Access Units (DAUs)  which usually correspond to large-scale sequencing initiatives at St. Jude.   Individuals can  apply for access  to DAUs on a case-by-case basis for a specific amount of time (usually 1 year).  Access to data in a given DAU is assessed by the corresponding  Data Access Committee  who reviews a variety of factors to grant access.  There are a number of terms of use and restrictions outlined in the  data access agreement . Everyone who will be working with the data must understand and agree to these terms.    Creating a data request is the premier way to access raw St. Jude next \ngeneration sequencing data in the cloud. You can get a  free  copy of \nthe data in a secure cloud environment powered by  Microsoft Azure  and  DNAnexus , or you can elect to download the data to your local computing \nenvironment.   Note  If you would like to download the data to local storage, there are\nextra steps you'll need to follow such as  getting additional signatures \non your data access agreement. We recommend that you work with the data\nin the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow  this guide  to get started.", 
            "title": "Introduction"
        }, 
        {
            "location": "/guides/data/data-request/#selecting-data", 
            "text": "There are two ways to make your data selection. You can peruse our raw genomic data by diagnosis, publication, or dataset using our  Data Browser , a tabular view with a number of filtering options. Or you can select samples associated with specific diagnoses, gene expression, or gene mutations while exploring curated data from the donut and bubble charts on the  Pediatric Cancer portal (PeCan)  homepage.", 
            "title": "Selecting Data"
        }, 
        {
            "location": "/guides/data/data-request/#selecting-data-in-the-data-browser", 
            "text": "Go to the Data Browser  here , or navigate there from the St. Jude Cloud home page by clicking Access Data and then Explore Data.  From the Data Browser, you can view samples grouped by Diagnosis, Publication, or  Dataset  by toggling the tabs above the table. Use the search bar to look for something specific. Search the publication tab by title or pubmed ID.   You can further refine your data selection by using the filters for sequencing type, sample type, file type, and tissue type on the left side bar. Filters of the same type apply using \u201cOR\u201d logic. Filters of different types apply using \u201cAND\u201d logic. Note that filtering is dynamic, so as you make selections the table will update to show all of the files we have that match your filters. Filters reset when you move from tab to tab.   The summary panel above the filters in the left side bar shows statistics about the data currently displayed in the table. As you can see in gifs above, this panel updates as you change what data is displayed by switching tabs, searching, filtering, or making selections.", 
            "title": "Selecting Data in the Data Browser"
        }, 
        {
            "location": "/guides/data/data-request/#selecting-data-via-pecan", 
            "text": "Go to the  PeCan homepage . A guide to interpreting the donut and bubble chart visualizations on the homepage can be found  here .  Using these visualizations along with ProteinPaint, you can:   Add samples to your cart by diagnosis.   \n        Add samples to your cart by gene mutation.   \n        Add samples to your cart by gene expression.   \n       Clicking  Submit to SJCloud  from the PeCan checkout window will land you back in the Data Browser with your checked out data selected.", 
            "title": "Selecting Data via PeCan"
        }, 
        {
            "location": "/guides/data/data-request/#requesting-data", 
            "text": "Once you have made your selections, click the red  Request Data  button at the bottom of the table.     Warning  You must have created an account and be logged in to make a data request. If you have not yet created an account or you are not logged in, the red  Request Data  button will say  Log In .   On the Request Data page fill out your name, institution, and project name. Give your data request a project name that makes sense to you as this will be the name of the DNAnexus project to which the data will be vended.    Data Access Approval  If you are requesting access to a dataset you have not yet been approved for, \nyou will see a section called  Controlled Access Data . Under this section, there is a bulleted list indicating the dataset(s) or  Data Access Unit(s)  you must request access to by submitting a form called the  Data Access Agreememnt (DAA) . Please use this list to fill in the Datasets section of the DAA. For more information on filling out this form, see  Filling out the DAA .  You must upload a DAA to continue.   Finally, click the green button. If you already have access to the data you selected in the browser, the button will read  Get Data Now . If you are submitting a DAA and requesting data access, the button will read  Submit Request .  This will direct you to the  Manage Data  page where you can see the status of the data request you just made as well the history of any of your previous data requests.    If you already have access to the data that you requested, your data will be vended to you immediately. Otherwise, the status of your request will say  Pending  while your request is routed to the respective  Data Access Committee(s)  for evaluation. Request approval typically takes a week or two if your data access agreement is correctly and completely filled out. You will receive automated emails from notifications@stjude.cloud at the time that your request is recieved and once your request is approved.   Note  If you receive an email from us that your DAA is incomplete, you may edit your DAA and upload the revised copy using the 'Add a Form' button the on Manage Data page.", 
            "title": "Requesting Data"
        }, 
        {
            "location": "/guides/data/data-request/#viewing-your-data", 
            "text": "Once your request to access data is approved, the data will be vended to your DNAnexus account in a project with the project name that you entered on the Request Data page. You can follow the link in the email from notifications@stjude.cloud to view your DNAnexus project page.", 
            "title": "Viewing your Data"
        }, 
        {
            "location": "/guides/data/types-of-data/", 
            "text": "Data types\n\n\n\n\nNote\n\n\nThis section of the documentation is currently under construction. If your question is not answered here,\nplease \ncontact us\n!\n\n\n\n\nSt. Jude Cloud hosts both raw genomic data files and processed results files:\n\n\n\n\n\n\n\n\nFile Type\n\n\nShort Description\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nBAM\n\n\nHG38 aligned BAM files produced by \nMicrosoft Genomics Service\n\n\nClick here\n\n\n\n\n\n\ngVCF\n\n\nGenomic VCF\n files produced by \nMicrosoft Genomics Service\n\n\nClick here\n\n\n\n\n\n\nSomatic VCF\n\n\nCurated list of somatic variants produced by the St. Jude somatic variant analysis pipeline\n\n\nClick here\n\n\n\n\n\n\nCNV\n\n\nlist of somatic copy number alterations produced by St. Jude CONSERTING pipeline\n\n\nClick here\n\n\n\n\n\n\n\n\nBAM files\n\n\nIn St. Jude Cloud, we stored aligned sequence reads in the ubiquitous BAM file format. BAM files were produced by the \nMicrosoft Genomics Service\n aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to \nthe official Microsoft Genomics whitepaper\n.\n\n\nFor more information on SAM/BAM files, please refer to the \nSAM/BAM specification\n. \n\n\ngVCF files\n\n\nWe provide gVCF files produced by the \nMicrosoft Genomics Service\n. gVCF files are derived from the BAM files produced above as called by \nGATK's haplotype caller\n. Today, we defer to \nthe official specification document\n from the Broad Institute, as well as \nthis discussion\n on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to \nthe official Microsoft Genomics whitepaper\n.\n\n\nSomatic VCF files\n\n\nSomatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking:\n\n\n\n\nReads were aligned to HG19 using \nbwa backtrack\n (\nbwa aln\n + \nbwa sampe\n) using default parameters.\n\n\nPost processing of aligned reads was performed using \nPicard\n \nCleanSam\n and \nMarkDuplicates\n.\n\n\nVariants were called using the \nBambino\n variant caller (you can download by navigating \nhere\n and searching for \"Bambino package\").\n\n\nVariants were post-processed using an in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available.\n\n\nVariants were manually reviewed by analysts and published with \nthe relevant Pediatric Cancer Genome Project (PCGP) paper\n.\n\n\nPost-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the \nHG19\n INFO field.).\n\n\n\n\nFor more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed \nhere\n.\n\n\nCNV files\n\n\nCNV files contain copy number alteration (CNA) analysis results for paired tumor-normal WGS samples. Files are produced by running paired tumor-normal BAM files through the \nCONSERTING\n pipeline which identifies CNA through iterative analysis of (i) local segmentation by read depth within boundaries identified by structural variation (SV) breakpoints followed by (ii) segment merging and local SV analysis. \nCREST\n was used to identify local SV breakpoints. CNV files contain the following information:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nchrom\n\n\nchromosome\n\n\n\n\n\n\nloc.start\n\n\nstart of segment\n\n\n\n\n\n\nloc.end\n\n\nend of segment\n\n\n\n\n\n\nnum.mark\n\n\nnumber of windows retained in the segment (gaps and windows with low mappability are excluded)\n\n\n\n\n\n\nlength.ratio\n\n\nThe ratio between the length of the used windows to the genomic length\n\n\n\n\n\n\nseg.mean\n\n\nThe estimated GC corrected difference signal (2 copy gain will have a seg.mean of 1)\n\n\n\n\n\n\nGMean\n\n\nThe mean coverage in the germline sample (a value of 1 represents diploid)\n\n\n\n\n\n\nDMean\n\n\nThe mean coverage in the tumor sample\n\n\n\n\n\n\nLogRatio\n\n\nLog2 ratio between tumor and normal coverage\n\n\n\n\n\n\nQuality score\n\n\nA empirical score used in merging\n\n\n\n\n\n\nSV_Matching\n\n\nwhether the boundary of the segments were supported by SVs (3: both ends supported, 2: right end supported, 1: left end supported, 0: neither end supported)", 
            "title": "Types of Data"
        }, 
        {
            "location": "/guides/data/types-of-data/#data-types", 
            "text": "Note  This section of the documentation is currently under construction. If your question is not answered here,\nplease  contact us !   St. Jude Cloud hosts both raw genomic data files and processed results files:     File Type  Short Description  Details      BAM  HG38 aligned BAM files produced by  Microsoft Genomics Service  Click here    gVCF  Genomic VCF  files produced by  Microsoft Genomics Service  Click here    Somatic VCF  Curated list of somatic variants produced by the St. Jude somatic variant analysis pipeline  Click here    CNV  list of somatic copy number alterations produced by St. Jude CONSERTING pipeline  Click here", 
            "title": "Data types"
        }, 
        {
            "location": "/guides/data/types-of-data/#bam-files", 
            "text": "In St. Jude Cloud, we stored aligned sequence reads in the ubiquitous BAM file format. BAM files were produced by the  Microsoft Genomics Service  aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to  the official Microsoft Genomics whitepaper .  For more information on SAM/BAM files, please refer to the  SAM/BAM specification .", 
            "title": "BAM files"
        }, 
        {
            "location": "/guides/data/types-of-data/#gvcf-files", 
            "text": "We provide gVCF files produced by the  Microsoft Genomics Service . gVCF files are derived from the BAM files produced above as called by  GATK's haplotype caller . Today, we defer to  the official specification document  from the Broad Institute, as well as  this discussion  on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to  the official Microsoft Genomics whitepaper .", 
            "title": "gVCF files"
        }, 
        {
            "location": "/guides/data/types-of-data/#somatic-vcf-files", 
            "text": "Somatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking:   Reads were aligned to HG19 using  bwa backtrack  ( bwa aln  +  bwa sampe ) using default parameters.  Post processing of aligned reads was performed using  Picard   CleanSam  and  MarkDuplicates .  Variants were called using the  Bambino  variant caller (you can download by navigating  here  and searching for \"Bambino package\").  Variants were post-processed using an in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available.  Variants were manually reviewed by analysts and published with  the relevant Pediatric Cancer Genome Project (PCGP) paper .  Post-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the  HG19  INFO field.).   For more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed  here .", 
            "title": "Somatic VCF files"
        }, 
        {
            "location": "/guides/data/types-of-data/#cnv-files", 
            "text": "CNV files contain copy number alteration (CNA) analysis results for paired tumor-normal WGS samples. Files are produced by running paired tumor-normal BAM files through the  CONSERTING  pipeline which identifies CNA through iterative analysis of (i) local segmentation by read depth within boundaries identified by structural variation (SV) breakpoints followed by (ii) segment merging and local SV analysis.  CREST  was used to identify local SV breakpoints. CNV files contain the following information:     Field  Description      chrom  chromosome    loc.start  start of segment    loc.end  end of segment    num.mark  number of windows retained in the segment (gaps and windows with low mappability are excluded)    length.ratio  The ratio between the length of the used windows to the genomic length    seg.mean  The estimated GC corrected difference signal (2 copy gain will have a seg.mean of 1)    GMean  The mean coverage in the germline sample (a value of 1 represents diploid)    DMean  The mean coverage in the tumor sample    LogRatio  Log2 ratio between tumor and normal coverage    Quality score  A empirical score used in merging    SV_Matching  whether the boundary of the segments were supported by SVs (3: both ends supported, 2: right end supported, 1: left end supported, 0: neither end supported)", 
            "title": "CNV files"
        }, 
        {
            "location": "/guides/data/metadata/", 
            "text": "Metadata\n\n\nMetadata Specification\n\n\nEach data request includes a text file called \nSAMPLE_INFO.txt\n that provides a number of file level properties (sample identifiers, clinical attributes, etc).\n\n\nStandard Metadata\n\n\nBelow are the set of tags which \nmay\n exist for any given file in St. Jude Cloud. All optional metadata will have \nsj_\n prepended to their tag name.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfile_path\n\n\nThe path to the file in your St. Jude Cloud project.\n\n\n\n\n\n\nsubject_name\n\n\nA unique subject identifier assigned internally at St. Jude.\n\n\n\n\n\n\nsample_name\n\n\nA unique sample identifier assigned internally at St. Jude.\n\n\n\n\n\n\nsample_type\n\n\nOne of Autopsy, Cell line, Diagnosis, Germline, Metastasis, Relapse, or Xenograft\n\n\n\n\n\n\nsequencing_type\n\n\nWhether the file was generated from Whole Genome (WGS), Whole Exome (WES), or RNA-Seq.\n\n\n\n\n\n\nfile_type\n\n\nOne of the \nfile types\n available in St. Jude Cloud\n\n\n\n\n\n\ndescription\n\n\nOptional field that may contain additional file information.\n\n\n\n\n\n\nsj_diseases\n\n\nShort disease identifier assigned at the time of genomic sequencing. Note that this diagnosis may be refined after undergoing genomic testing. When including diagnosis in your analysis, we recommend you use \nattr_diagnosis\n, which is the most up to date value for diagnosis.\n\n\n\n\n\n\nsj_datasets\n\n\nIf present, the datasets in the data browser which this file is associated with.\n\n\n\n\n\n\nsj_pmid_accessions\n\n\nIf the file was associated with a paper, the related \nPubmed\n accession number.\n\n\n\n\n\n\nsj_ega_accessions\n\n\nIf the file was associated with a paper, the related \nEGA\n accession number.\n\n\n\n\n\n\nsj_dataset_accession\n\n\nIf present, the permanent accession number assigned in St. Jude Cloud.\n\n\n\n\n\n\nsj_embargo_date\n\n\nThe \nembargo date\n, which specifies the first date which the files can be used in a publication.\n\n\n\n\n\n\n\n\nClinical and Phenotypic Information\n\n\nAlso included is a set of phenotypic information queried from the physician or research team's records at the time of sample submission to St. Jude Cloud. These are all considered to be \noptional\n, as the level of information gathered for each sample varies. If empty, the physician or research team did not indicate a value for the field. All basic clinical or phenotypic information will have \nattr_\n prepended to their tag name.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nattr_age_at_diagnosis\n\n\nAge at first diagnosis. This field is normalized as a decimal value. If empty, the physician or research team did not indicate a value for this field.\n\n\n\n\n\n\nattr_diagnosis\n\n\nPrimary diagnosis.\n\n\n\n\n\n\nattr_ethnicity\n\n\nSelf-reported ethnicity. Values are normalized according to the \nUS Census Bureau classifications\n.\n\n\n\n\n\n\nattr_race\n\n\nSelf-reported race. Values are normalized according to the \nUS Census Bureau classifications\n.\n\n\n\n\n\n\nattr_sex\n\n\nSelf-reported sex.\n\n\n\n\n\n\n\n\nShort Disease Code Mapping\n\n\nEmbedded in both the filename and the \nSAMPLE_INFO.txt\n file that comes with your data request will be a list of short diagnosis codes (\nsj_diseases\n). These short codes were assigned at the time that the sample was sent for sequencing, and they are not necessarily the final diagnosis (\nattr_diagnosis\n). For instance, the diagnosis is often refined as the sample undergoes genomic testing. Below, we include a table of short disease code to long disease name mappings so you can interpret what these abbreviations mean.\n\n\n\n\n\n\n\n\nShort Disease Code\n\n\nLong Diagnosis Description\n\n\n\n\n\n\n\n\n\n\nACT\n\n\nAdrenocortical Carcinoma\n\n\n\n\n\n\nAEL\n\n\nAcute erythroid leukemia (AML M6)\n\n\n\n\n\n\nALCL\n\n\nAnaplastic Large Cell Lymphoma\n\n\n\n\n\n\nALL\n\n\nAcute Lymphoblastic Leukemia\n\n\n\n\n\n\nALS\n\n\nAmyotrophic Lateral Sclerosis (\"Lou Gehrig\\'s Disease\"\")\"\n\n\n\n\n\n\nALZ\n\n\nAlzheimer's Disease\n\n\n\n\n\n\nAML\n\n\nAcute Myeloid Leukemia\n\n\n\n\n\n\nAMLM\n\n\nAcute Megakaryoblastic Leukemia\n\n\n\n\n\n\nANEM\n\n\nAnemia\n\n\n\n\n\n\nASPS\n\n\nAlveolar Soft Part Sarcoma\n\n\n\n\n\n\nAUL\n\n\nAcute Undifferentiated Leukemia\n\n\n\n\n\n\nBALL\n\n\nB-cell Acute Lymphoblastic Leukemia\n\n\n\n\n\n\nBCC\n\n\nBasal Cell Carcinoma\n\n\n\n\n\n\nBLACA\n\n\nBladder Cancer\n\n\n\n\n\n\nBT\n\n\nBrain Tumor\n\n\n\n\n\n\nCA\n\n\nCarcinoma\n\n\n\n\n\n\nCBF\n\n\nAcute Myeloid Leukemia - Core Binding Factor subtype\n\n\n\n\n\n\nCLL\n\n\nChronic Lymphocytic Leukemia\n\n\n\n\n\n\nCML\n\n\nChronic Myelogenous Leukemia\n\n\n\n\n\n\nCMML\n\n\nChronic Myelomonocytic Leukemia\n\n\n\n\n\n\nCMV\n\n\nCytomegalovirus\n\n\n\n\n\n\nCNS\n\n\nCentral Nervous System\n\n\n\n\n\n\nCPC\n\n\nChoroid Plexus Carcinoma\n\n\n\n\n\n\nCRC\n\n\nColorectal Cancer\n\n\n\n\n\n\nCS\n\n\nChondrosarcoma\n\n\n\n\n\n\nCTP\n\n\nCongenital Thrombocytopenia\n\n\n\n\n\n\nDIPG\n\n\nDiffuse Intrinsic Pontine Glioma\n\n\n\n\n\n\nDLBCL\n\n\nDiffuse Large B-cell Lymphoma\n\n\n\n\n\n\nDOWN\n\n\nDown Syndrome\n\n\n\n\n\n\nDSRCT\n\n\nDesmoplastic Small Round Cell Tumor\n\n\n\n\n\n\nE2A\n\n\nB-Lineage Acute Lymphoblastic Leukemia - E2A-PBX1 subtype\n\n\n\n\n\n\nECD\n\n\nErdheim-Chester Disease\n\n\n\n\n\n\nEPD\n\n\nEpendymoma\n\n\n\n\n\n\nERG\n\n\nAcute Lymphoblastic Leukemia - ERG alteration subtype\n\n\n\n\n\n\nETV\n\n\nAcute Lymphoblastic Leukemia - ETV6-RUNX1 fusion subtype\n\n\n\n\n\n\nEWS\n\n\nEwing's Sarcoma\n\n\n\n\n\n\nGCT\n\n\nGerm Cell Tumor\n\n\n\n\n\n\nGICT\n\n\nGiant Cell Tumor\n\n\n\n\n\n\nGIST\n\n\nGastrointestinal Stromal Tumor\n\n\n\n\n\n\nHB\n\n\nHepatoblastoma\n\n\n\n\n\n\nHGG\n\n\nHigh Grade Glioma\n\n\n\n\n\n\nHGS\n\n\nHigh Grade Sarcoma\n\n\n\n\n\n\nHIST\n\n\nHistiocytosis\n\n\n\n\n\n\nHL\n\n\nHodgkin's Lymphoma\n\n\n\n\n\n\nHM\n\n\nHematopoietic Malignancies\n\n\n\n\n\n\nHS\n\n\nHidradenitis Suppurativa\n\n\n\n\n\n\nHYPER\n\n\nAcute Lymphoblastic Leukemia - Hyperdiploid subtype\n\n\n\n\n\n\nHYPO\n\n\nAcute Lymphoblastic Leukemia - Hypodiploid subtype\n\n\n\n\n\n\nIFS\n\n\nInfantile Fibromyosarcoma\n\n\n\n\n\n\nINF\n\n\nAcute Lymphoblastic Leukemia (Infant)\n\n\n\n\n\n\nITP\n\n\nIdiopathic Thrombocytopenia\n\n\n\n\n\n\nJMML\n\n\nJuvenile Myelomonocytic Leukemia\n\n\n\n\n\n\nLCH\n\n\nLangerhans Cell Histiocytocis\n\n\n\n\n\n\nLGG\n\n\nLow Grade Glioma\n\n\n\n\n\n\nLM\n\n\nLiver Malignancies\n\n\n\n\n\n\nMB\n\n\nMedulloblastoma\n\n\n\n\n\n\nMDS\n\n\nMyelodysplastic Syndrome\n\n\n\n\n\n\nMEL\n\n\nMelanoma\n\n\n\n\n\n\nMLL\n\n\nMixed Lineage Leukemia\n\n\n\n\n\n\nMM\n\n\nMultiple Myeloma\n\n\n\n\n\n\nMPAL\n\n\nAcute Lymphoblastic Leukemia - Multi-phenotypic\n\n\n\n\n\n\nMPNST\n\n\nMalignant Peripheral Nerve Sheath Tumor\n\n\n\n\n\n\nMRT\n\n\nMalignant Rhabdoid Tumour\n\n\n\n\n\n\nMYF\n\n\nMyelofibrosis\n\n\n\n\n\n\nNBL\n\n\nNeuroblastoma\n\n\n\n\n\n\nNEUTP\n\n\nNeutropenia\n\n\n\n\n\n\nNHL\n\n\nNon-Hodgkin's Lymphoma\n\n\n\n\n\n\nNM\n\n\nNon-malignancy\n\n\n\n\n\n\nNORM\n\n\nControl Sample\n\n\n\n\n\n\nNPC\n\n\nNasopharyngeal Carcinoma\n\n\n\n\n\n\nNPCA\n\n\nNasopharyngeal Carcinoma\n\n\n\n\n\n\nOS\n\n\nOsteosarcoma\n\n\n\n\n\n\nPF\n\n\nPosterior Fossa\n\n\n\n\n\n\nPGL\n\n\nParaganglioma\n\n\n\n\n\n\nPHALL\n\n\nAcute Lymphoblastic Leukemia - BCR-ABL1 fusion subtype\n\n\n\n\n\n\nPHCML\n\n\nPh+ Chronic Myeloid Leukemia\n\n\n\n\n\n\nPML\n\n\nPromyelocitic Leukemia\n\n\n\n\n\n\nPRAD\n\n\nProstate Adenocarcoma\n\n\n\n\n\n\nPSO\n\n\nPsoriasis\n\n\n\n\n\n\nRB\n\n\nRetinoblastoma\n\n\n\n\n\n\nRCC\n\n\nRenal cell carcinoma\n\n\n\n\n\n\nRECA\n\n\nRenal Cancer\n\n\n\n\n\n\nRHB\n\n\nRhabdomyosarcoma\n\n\n\n\n\n\nSBO\n\n\nSpina Bifida Occulta\n\n\n\n\n\n\nSCD\n\n\nSickle Cell Disease\n\n\n\n\n\n\nSCZ\n\n\nSchizophrenia\n\n\n\n\n\n\nSS\n\n\nSynovial Sarcoma\n\n\n\n\n\n\nST\n\n\nSolid Tumor\n\n\n\n\n\n\nSTS\n\n\nSoft Tissue Sarcoma\n\n\n\n\n\n\nTALL\n\n\nT-cell Acute Lymphoblastic Leukemia\n\n\n\n\n\n\nTCP\n\n\nThrombocytopenia\n\n\n\n\n\n\nTESCA\n\n\nTesticular Cancer\n\n\n\n\n\n\nTHCA\n\n\nThyroid Carcinoma\n\n\n\n\n\n\nWLM\n\n\nWilms' tumor", 
            "title": "Metadata"
        }, 
        {
            "location": "/guides/data/metadata/#metadata", 
            "text": "", 
            "title": "Metadata"
        }, 
        {
            "location": "/guides/data/metadata/#metadata-specification", 
            "text": "Each data request includes a text file called  SAMPLE_INFO.txt  that provides a number of file level properties (sample identifiers, clinical attributes, etc).", 
            "title": "Metadata Specification"
        }, 
        {
            "location": "/guides/data/metadata/#standard-metadata", 
            "text": "Below are the set of tags which  may  exist for any given file in St. Jude Cloud. All optional metadata will have  sj_  prepended to their tag name.     Property  Description      file_path  The path to the file in your St. Jude Cloud project.    subject_name  A unique subject identifier assigned internally at St. Jude.    sample_name  A unique sample identifier assigned internally at St. Jude.    sample_type  One of Autopsy, Cell line, Diagnosis, Germline, Metastasis, Relapse, or Xenograft    sequencing_type  Whether the file was generated from Whole Genome (WGS), Whole Exome (WES), or RNA-Seq.    file_type  One of the  file types  available in St. Jude Cloud    description  Optional field that may contain additional file information.    sj_diseases  Short disease identifier assigned at the time of genomic sequencing. Note that this diagnosis may be refined after undergoing genomic testing. When including diagnosis in your analysis, we recommend you use  attr_diagnosis , which is the most up to date value for diagnosis.    sj_datasets  If present, the datasets in the data browser which this file is associated with.    sj_pmid_accessions  If the file was associated with a paper, the related  Pubmed  accession number.    sj_ega_accessions  If the file was associated with a paper, the related  EGA  accession number.    sj_dataset_accession  If present, the permanent accession number assigned in St. Jude Cloud.    sj_embargo_date  The  embargo date , which specifies the first date which the files can be used in a publication.", 
            "title": "Standard Metadata"
        }, 
        {
            "location": "/guides/data/metadata/#clinical-and-phenotypic-information", 
            "text": "Also included is a set of phenotypic information queried from the physician or research team's records at the time of sample submission to St. Jude Cloud. These are all considered to be  optional , as the level of information gathered for each sample varies. If empty, the physician or research team did not indicate a value for the field. All basic clinical or phenotypic information will have  attr_  prepended to their tag name.     Property  Description      attr_age_at_diagnosis  Age at first diagnosis. This field is normalized as a decimal value. If empty, the physician or research team did not indicate a value for this field.    attr_diagnosis  Primary diagnosis.    attr_ethnicity  Self-reported ethnicity. Values are normalized according to the  US Census Bureau classifications .    attr_race  Self-reported race. Values are normalized according to the  US Census Bureau classifications .    attr_sex  Self-reported sex.", 
            "title": "Clinical and Phenotypic Information"
        }, 
        {
            "location": "/guides/data/metadata/#short-disease-code-mapping", 
            "text": "Embedded in both the filename and the  SAMPLE_INFO.txt  file that comes with your data request will be a list of short diagnosis codes ( sj_diseases ). These short codes were assigned at the time that the sample was sent for sequencing, and they are not necessarily the final diagnosis ( attr_diagnosis ). For instance, the diagnosis is often refined as the sample undergoes genomic testing. Below, we include a table of short disease code to long disease name mappings so you can interpret what these abbreviations mean.     Short Disease Code  Long Diagnosis Description      ACT  Adrenocortical Carcinoma    AEL  Acute erythroid leukemia (AML M6)    ALCL  Anaplastic Large Cell Lymphoma    ALL  Acute Lymphoblastic Leukemia    ALS  Amyotrophic Lateral Sclerosis (\"Lou Gehrig\\'s Disease\"\")\"    ALZ  Alzheimer's Disease    AML  Acute Myeloid Leukemia    AMLM  Acute Megakaryoblastic Leukemia    ANEM  Anemia    ASPS  Alveolar Soft Part Sarcoma    AUL  Acute Undifferentiated Leukemia    BALL  B-cell Acute Lymphoblastic Leukemia    BCC  Basal Cell Carcinoma    BLACA  Bladder Cancer    BT  Brain Tumor    CA  Carcinoma    CBF  Acute Myeloid Leukemia - Core Binding Factor subtype    CLL  Chronic Lymphocytic Leukemia    CML  Chronic Myelogenous Leukemia    CMML  Chronic Myelomonocytic Leukemia    CMV  Cytomegalovirus    CNS  Central Nervous System    CPC  Choroid Plexus Carcinoma    CRC  Colorectal Cancer    CS  Chondrosarcoma    CTP  Congenital Thrombocytopenia    DIPG  Diffuse Intrinsic Pontine Glioma    DLBCL  Diffuse Large B-cell Lymphoma    DOWN  Down Syndrome    DSRCT  Desmoplastic Small Round Cell Tumor    E2A  B-Lineage Acute Lymphoblastic Leukemia - E2A-PBX1 subtype    ECD  Erdheim-Chester Disease    EPD  Ependymoma    ERG  Acute Lymphoblastic Leukemia - ERG alteration subtype    ETV  Acute Lymphoblastic Leukemia - ETV6-RUNX1 fusion subtype    EWS  Ewing's Sarcoma    GCT  Germ Cell Tumor    GICT  Giant Cell Tumor    GIST  Gastrointestinal Stromal Tumor    HB  Hepatoblastoma    HGG  High Grade Glioma    HGS  High Grade Sarcoma    HIST  Histiocytosis    HL  Hodgkin's Lymphoma    HM  Hematopoietic Malignancies    HS  Hidradenitis Suppurativa    HYPER  Acute Lymphoblastic Leukemia - Hyperdiploid subtype    HYPO  Acute Lymphoblastic Leukemia - Hypodiploid subtype    IFS  Infantile Fibromyosarcoma    INF  Acute Lymphoblastic Leukemia (Infant)    ITP  Idiopathic Thrombocytopenia    JMML  Juvenile Myelomonocytic Leukemia    LCH  Langerhans Cell Histiocytocis    LGG  Low Grade Glioma    LM  Liver Malignancies    MB  Medulloblastoma    MDS  Myelodysplastic Syndrome    MEL  Melanoma    MLL  Mixed Lineage Leukemia    MM  Multiple Myeloma    MPAL  Acute Lymphoblastic Leukemia - Multi-phenotypic    MPNST  Malignant Peripheral Nerve Sheath Tumor    MRT  Malignant Rhabdoid Tumour    MYF  Myelofibrosis    NBL  Neuroblastoma    NEUTP  Neutropenia    NHL  Non-Hodgkin's Lymphoma    NM  Non-malignancy    NORM  Control Sample    NPC  Nasopharyngeal Carcinoma    NPCA  Nasopharyngeal Carcinoma    OS  Osteosarcoma    PF  Posterior Fossa    PGL  Paraganglioma    PHALL  Acute Lymphoblastic Leukemia - BCR-ABL1 fusion subtype    PHCML  Ph+ Chronic Myeloid Leukemia    PML  Promyelocitic Leukemia    PRAD  Prostate Adenocarcoma    PSO  Psoriasis    RB  Retinoblastoma    RCC  Renal cell carcinoma    RECA  Renal Cancer    RHB  Rhabdomyosarcoma    SBO  Spina Bifida Occulta    SCD  Sickle Cell Disease    SCZ  Schizophrenia    SS  Synovial Sarcoma    ST  Solid Tumor    STS  Soft Tissue Sarcoma    TALL  T-cell Acute Lymphoblastic Leukemia    TCP  Thrombocytopenia    TESCA  Testicular Cancer    THCA  Thyroid Carcinoma    WLM  Wilms' tumor", 
            "title": "Short Disease Code Mapping"
        }, 
        {
            "location": "/guides/data/run-your-tools/", 
            "text": "You can follow \nthis guide\n to request access to\nSt. Jude data in a secure cloud environment. Before you can begin writing your\nown tools to run on our data, you'll need to understand a bit about how\ndata vending in St. Jude Cloud works. Behind the scenes, the \nDNAnexus\n genomic ecosystem is the backbone for the computation\nand storage in St. Jude Cloud. Each data request in St. Jude Cloud corresponds to a project in DNAnexus. We'll explain what this means below, but if you're so inclined, you can read an introduction to their ecosystem \nhere\n.\n\n\nAccessing your data\n\n\nOnce your data access request is approved, the data you requested from St. Jude will automatically be distributed to a DNAnexus project with the same name as your data request. You can go to your \nManage Data\n page to see the requests you have submitted and go directly to your data.\n\n\nUsing Our Data\n\n\nSt. Jude Cloud offers data and tools for you to use. However, many researchers are interested in using our data in combination with their own data or tools. To upload your own data, we recommend using the \nData Transfer App\n, or you can use the \ncommand line\n. Anything you upload to St. Jude Cloud will be uploaded to your private, secure project in DNAnexus.\n\n\nTo upload your own tools and run them on data in you DNAnexus project you must use the \ncommand line\n.\n\n\nRefer to the \nCreating A Cloud App Tutorial\n for more information on how to package your own tools and run them in the cloud environment.\n\n\nFrequently asked questions\n\n\nQ. Can I submit my data to St. Jude Cloud?\n\n\nA.\n At this time, St. Jude Cloud does not accept data submissions from other institutions. You can upload your data to your private, secure project folder, but that is not shared with St. Jude Cloud.", 
            "title": "Working with our Data"
        }, 
        {
            "location": "/guides/data/run-your-tools/#accessing-your-data", 
            "text": "Once your data access request is approved, the data you requested from St. Jude will automatically be distributed to a DNAnexus project with the same name as your data request. You can go to your  Manage Data  page to see the requests you have submitted and go directly to your data.", 
            "title": "Accessing your data"
        }, 
        {
            "location": "/guides/data/run-your-tools/#using-our-data", 
            "text": "St. Jude Cloud offers data and tools for you to use. However, many researchers are interested in using our data in combination with their own data or tools. To upload your own data, we recommend using the  Data Transfer App , or you can use the  command line . Anything you upload to St. Jude Cloud will be uploaded to your private, secure project in DNAnexus.  To upload your own tools and run them on data in you DNAnexus project you must use the  command line .  Refer to the  Creating A Cloud App Tutorial  for more information on how to package your own tools and run them in the cloud environment.", 
            "title": "Using Our Data"
        }, 
        {
            "location": "/guides/data/run-your-tools/#frequently-asked-questions", 
            "text": "Q. Can I submit my data to St. Jude Cloud?  A.  At this time, St. Jude Cloud does not accept data submissions from other institutions. You can upload your data to your private, secure project folder, but that is not shared with St. Jude Cloud.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/data/data-transfer-app/", 
            "text": "Data Transfer App\n\n\nThe Data Transfer App is a downloadable tool with an easy to use graphical user interface that allows you to upload/download files to/from your DNAnexus projects on the cloud. \n\n\nFor users looking to upload/download files using the command line, please refer to the \ncommand line interaction guide\n.\n\n\nIf you are interested in viewing the source code, you can do so \nhere\n. If you\nwould like to file an issue you are experiencing with the application,\nyou can do so \nhere\n or let us know your feedback through our \ncontact us form\n.\n\n\nGetting Started\n\n\nClick here\n to download the latest release of the\nSt. Jude Cloud data transfer application. \n\n\nOnce you've completed installing the app, you will see a page that looks like this. \n\n\n\n\nLog in with your DNAnexus credentials (or click on \nI'm a St. Jude employee\n to log in with your St. Jude credentials). \n\n\nEach time you log in, the app will prompt you to grant it access to all files in your DNAnexus projects. Click \nGrant Access\n to proceed. Access is granted per session and will expire once you log out of the data transfer app.\n\n\n\n\nOnce you've given the app access to your DNANexus projects, you will see the projects listed in a sidebar on the left and an upload/download panel on the right.  \n\n\nBefore moving on, we encourage you to take the \nTOUR\n by clicking on the green button in the upper right corner.\n\n\n\n\nAs you will see in the tour, you have the option to \nShow All Files\n in your DNAnexus projects. It is a good idea to always have this option enabled.\n\n\n\n\nWarning\n\n\nYou can increase the concurrency (# of files that will upload or download at the same time) but this will affect the performance of the app. For example changing the concurrency from 1 to 100 will move files at roughly 100th of the speed.\n\n\n\n\nUploading Files\n\n\nSelect the DNAnexus project on the left that you would like to upload files to. Select \nUpload\n in the app's Upload/Download panel. To select files you may either (1) Click in the upload space to select files in your computer's file navigation application \n\n\n\n\nor (2) highlight all the files you want to upload, then drag and drop them into the app's upload space.\n\n\n\n\nReview the list of files to upload, and click \nUpload\n.\n\n\nDownloading Files\n\n\nSelect the DNAnexus project on the left that you would like to download files from. Select \nDownload\n in the app's Upload/Download panel. In may take a minute to display all the files in your project. Once all files are displayed, select the files you want to download and click \nDownload\n.\n\n\n\n\nNote that the Data Transfer App does not recognize any directory structure you may have within your DNAnexus projects. It is simply a dump of all the files in each project.", 
            "title": "Data Transfer Application"
        }, 
        {
            "location": "/guides/data/data-transfer-app/#data-transfer-app", 
            "text": "The Data Transfer App is a downloadable tool with an easy to use graphical user interface that allows you to upload/download files to/from your DNAnexus projects on the cloud.   For users looking to upload/download files using the command line, please refer to the  command line interaction guide .  If you are interested in viewing the source code, you can do so  here . If you\nwould like to file an issue you are experiencing with the application,\nyou can do so  here  or let us know your feedback through our  contact us form .", 
            "title": "Data Transfer App"
        }, 
        {
            "location": "/guides/data/data-transfer-app/#getting-started", 
            "text": "Click here  to download the latest release of the\nSt. Jude Cloud data transfer application.   Once you've completed installing the app, you will see a page that looks like this.    Log in with your DNAnexus credentials (or click on  I'm a St. Jude employee  to log in with your St. Jude credentials).   Each time you log in, the app will prompt you to grant it access to all files in your DNAnexus projects. Click  Grant Access  to proceed. Access is granted per session and will expire once you log out of the data transfer app.   Once you've given the app access to your DNANexus projects, you will see the projects listed in a sidebar on the left and an upload/download panel on the right.    Before moving on, we encourage you to take the  TOUR  by clicking on the green button in the upper right corner.   As you will see in the tour, you have the option to  Show All Files  in your DNAnexus projects. It is a good idea to always have this option enabled.   Warning  You can increase the concurrency (# of files that will upload or download at the same time) but this will affect the performance of the app. For example changing the concurrency from 1 to 100 will move files at roughly 100th of the speed.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/data/data-transfer-app/#uploading-files", 
            "text": "Select the DNAnexus project on the left that you would like to upload files to. Select  Upload  in the app's Upload/Download panel. To select files you may either (1) Click in the upload space to select files in your computer's file navigation application    or (2) highlight all the files you want to upload, then drag and drop them into the app's upload space.   Review the list of files to upload, and click  Upload .", 
            "title": "Uploading Files"
        }, 
        {
            "location": "/guides/data/data-transfer-app/#downloading-files", 
            "text": "Select the DNAnexus project on the left that you would like to download files from. Select  Download  in the app's Upload/Download panel. In may take a minute to display all the files in your project. Once all files are displayed, select the files you want to download and click  Download .   Note that the Data Transfer App does not recognize any directory structure you may have within your DNAnexus projects. It is simply a dump of all the files in each project.", 
            "title": "Downloading Files"
        }, 
        {
            "location": "/guides/data/command-line/", 
            "text": "Getting started\n\n\nBefore you begin interacting with St. Jude Cloud Platform from the\ncommand line, you'll need to understand some details on the underlying\narchitecture of the platform. The St. Jude Cloud Platform is built on\ntop of a genomics cloud ecosystem provided by \nDNAnexus\n. \nFor a comprehensive overview of how DNAnexus works, see \n\nthis page\n.\n\n\nOverview\n\n\nWorkspaces in DNAnexus are organized by projects, which are essentially\nfolders in the cloud. Each data request and tool in St. Jude Cloud\ncreates its own unique cloud workspace (DNAnexus project). For instance,\na data request creates a DNAnexus project behind the scenes with the\nsame name as the request name you specify when you request data.\n\n\nInstallation\n\n\nOpen-source software provided by DNAnexus called the \ndx-toolkit\n is\nused to interact with the St. Jude Cloud Platform from the command line.\nYou can use this to create these projects, upload and download data, and\nmany other operations. You'll need to install that software on your\ncomputer by following \nthis guide\n.\n\n\n\n\nTip\n\n\nA quickstart to getting up and running with the dx-toolkit:\n\n\n\n\nInstall Python 2.7.13+. Note that using the system-level Python is\n    usually not a good idea (by default, system level Python is\n    typically too old/does not support the latest security protocols\n    required). You can install using \nAnaconda\n (recommended) or using\n    the default \nPython installer\n.\n\n\nRun \npip install dxpy\n.\n\n\nType \ndx --help\n at the command line.\n\n\n\n\n\n\nA quick tour\n\n\nLogging in\n\n\nTo log in using the dx-toolkit, run the following command:\n\n\ndx login --noprojects\n\n# enter username and password when prompted\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are a St. Jude employee, you'll need to follow \nthis\nguide\n to log in instead.\n\n\n\n\nSelecting a project\n\n\nFirst, you'll need to choose which cloud workspace you would like to\naccess. This depends on if you are downloading data from a request or\nworking with input/output files from a tool. You can see the workspaces\navailable to you by running the following command in your terminal:\n\n\ndx \nselect\n\n\n\n\n\n\nThis will present with a prompt similar to the below screenshot. A list\nof your available cloud workspaces will be shown with a number out to\nthe left of each. You should enter the number corresponding to the\nworkspace you are wanting to interact with. In the example below, the\nuser has selected the Rapid RNA-Seq tool.\n\n\n\n\nSome useful commands\n\n\nMoving data back and forth between the cloud and your local computer is\nsimple once you have selected the correct project for your tool.\n\n\nYou will find that many common linux commands with \ndx\n prepended.\n\n\n# list available files for the tool for the main folder\n\ndx ls\n\n\n# list all available files for the tool\n\ndx find .\n\n\n# list all commands\n\ndx --help\n\n\n\n\n\nUploading data\n\n\nYou can use the following process to upload data to be used by St. Jude\nCloud Platform tools:\n\n\n\n\n\n\nFirst, click \"View\" on the tool you'd like to run from \nthis\n    page\n. In this example, we will\n    choose the Rapid RNA-Seq tool.\n\n\n\n\n\n\nIf you have not already, click \"Start\" on the tool you'd like to\n    run. This will create a cloud workspace for you to upload your\n    data to with the same name as the tool.\n\n\n\n\n\n\n\n\nOpen up your terminal application and select the cloud workspace\n    with the same name as the tool you are trying to run.\n\n\n\n\n\n\n\n\nLast, navigate to the local files you'd like to upload to the cloud\n    and use the \ndx upload\n command as specified in\n    [upload-download-data]{role=\"ref\"} to upload your data to St. Jude\n    Cloud.\n\n\n\n\n\n\n\n\nDownloading data\n\n\n\n\nWarning\n\n\nTo download data from a St. Jude Cloud data request, you must have\nindicated that you wished to download the data in your Data Access\nAgreement (DAA) during your submission. Any downloading of St. Jude data\nwithout completing this step is strictly PROHIBITED.\n\n\n\n\nYou can use the following steps to download data from a St. Jude Cloud\ndata request:\n\n\n\n\n\n\nComplete a data request using the St. Jude Cloud Platform. In this\n    example, we've created a request with the name \"Retinoblastoma\n    Data\".\n\n\n\n\n\n\n\n\nOpen up your terminal application and select the cloud workspace\n    relevant to your data request. For instance, in this case we\n    would type \ndx \nselect\n \nRetinoblastoma Data\n.\n\n\n\n\n\n\n\n\nYou can use typical commands like \ndx ls\n,\n    \ndx \npwd\n, and \ndx \ncd\n to navigate around\n    your cloud folder as you would a local folder. Your project may look\n    different based on what data you requested and whether you were\n    previously approved to access the data. Your data should either be\n    in the \nrestricted\n folder (if this is your first time\n    requesting access) or the \nimmediate\n folder (if you\n    were previously granted access permission).\n\n\n\n\n\n\n\n\nIn the root of every data request is a file called\n    \nSAMPLE_INFO.txt\n. This should contain all of the\n    information about the samples you checked out as well as the\n    associated metadata we provide.\n\n\n\n\n\n\nTo download data from the cloud to local storage, use the\n    \ndx download\n command as specified in\n    [upload-download-data]{role=\"ref\"}. For instance, if I wanted to\n    download all of the BAM files to my local computer, I would type\n    \ndx download immediate/bam/*\n.", 
            "title": "Command Line Interaction"
        }, 
        {
            "location": "/guides/data/command-line/#getting-started", 
            "text": "Before you begin interacting with St. Jude Cloud Platform from the\ncommand line, you'll need to understand some details on the underlying\narchitecture of the platform. The St. Jude Cloud Platform is built on\ntop of a genomics cloud ecosystem provided by  DNAnexus . \nFor a comprehensive overview of how DNAnexus works, see  this page .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/data/command-line/#overview", 
            "text": "Workspaces in DNAnexus are organized by projects, which are essentially\nfolders in the cloud. Each data request and tool in St. Jude Cloud\ncreates its own unique cloud workspace (DNAnexus project). For instance,\na data request creates a DNAnexus project behind the scenes with the\nsame name as the request name you specify when you request data.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/data/command-line/#installation", 
            "text": "Open-source software provided by DNAnexus called the  dx-toolkit  is\nused to interact with the St. Jude Cloud Platform from the command line.\nYou can use this to create these projects, upload and download data, and\nmany other operations. You'll need to install that software on your\ncomputer by following  this guide .   Tip  A quickstart to getting up and running with the dx-toolkit:   Install Python 2.7.13+. Note that using the system-level Python is\n    usually not a good idea (by default, system level Python is\n    typically too old/does not support the latest security protocols\n    required). You can install using  Anaconda  (recommended) or using\n    the default  Python installer .  Run  pip install dxpy .  Type  dx --help  at the command line.", 
            "title": "Installation"
        }, 
        {
            "location": "/guides/data/command-line/#a-quick-tour", 
            "text": "", 
            "title": "A quick tour"
        }, 
        {
            "location": "/guides/data/command-line/#logging-in", 
            "text": "To log in using the dx-toolkit, run the following command:  dx login --noprojects # enter username and password when prompted    Note  If you are a St. Jude employee, you'll need to follow  this\nguide  to log in instead.", 
            "title": "Logging in"
        }, 
        {
            "location": "/guides/data/command-line/#selecting-a-project", 
            "text": "First, you'll need to choose which cloud workspace you would like to\naccess. This depends on if you are downloading data from a request or\nworking with input/output files from a tool. You can see the workspaces\navailable to you by running the following command in your terminal:  dx  select   This will present with a prompt similar to the below screenshot. A list\nof your available cloud workspaces will be shown with a number out to\nthe left of each. You should enter the number corresponding to the\nworkspace you are wanting to interact with. In the example below, the\nuser has selected the Rapid RNA-Seq tool.", 
            "title": "Selecting a project"
        }, 
        {
            "location": "/guides/data/command-line/#some-useful-commands", 
            "text": "Moving data back and forth between the cloud and your local computer is\nsimple once you have selected the correct project for your tool.  You will find that many common linux commands with  dx  prepended.  # list available files for the tool for the main folder \ndx ls # list all available files for the tool \ndx find . # list all commands \ndx --help", 
            "title": "Some useful commands"
        }, 
        {
            "location": "/guides/data/command-line/#uploading-data", 
            "text": "You can use the following process to upload data to be used by St. Jude\nCloud Platform tools:    First, click \"View\" on the tool you'd like to run from  this\n    page . In this example, we will\n    choose the Rapid RNA-Seq tool.    If you have not already, click \"Start\" on the tool you'd like to\n    run. This will create a cloud workspace for you to upload your\n    data to with the same name as the tool.     Open up your terminal application and select the cloud workspace\n    with the same name as the tool you are trying to run.     Last, navigate to the local files you'd like to upload to the cloud\n    and use the  dx upload  command as specified in\n    [upload-download-data]{role=\"ref\"} to upload your data to St. Jude\n    Cloud.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/data/command-line/#downloading-data", 
            "text": "Warning  To download data from a St. Jude Cloud data request, you must have\nindicated that you wished to download the data in your Data Access\nAgreement (DAA) during your submission. Any downloading of St. Jude data\nwithout completing this step is strictly PROHIBITED.   You can use the following steps to download data from a St. Jude Cloud\ndata request:    Complete a data request using the St. Jude Cloud Platform. In this\n    example, we've created a request with the name \"Retinoblastoma\n    Data\".     Open up your terminal application and select the cloud workspace\n    relevant to your data request. For instance, in this case we\n    would type  dx  select   Retinoblastoma Data .     You can use typical commands like  dx ls ,\n     dx  pwd , and  dx  cd  to navigate around\n    your cloud folder as you would a local folder. Your project may look\n    different based on what data you requested and whether you were\n    previously approved to access the data. Your data should either be\n    in the  restricted  folder (if this is your first time\n    requesting access) or the  immediate  folder (if you\n    were previously granted access permission).     In the root of every data request is a file called\n     SAMPLE_INFO.txt . This should contain all of the\n    information about the samples you checked out as well as the\n    associated metadata we provide.    To download data from the cloud to local storage, use the\n     dx download  command as specified in\n    [upload-download-data]{role=\"ref\"}. For instance, if I wanted to\n    download all of the BAM files to my local computer, I would type\n     dx download immediate/bam/* .", 
            "title": "Downloading data"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/", 
            "text": "Creating a Cloud Application\n\n\nThis guide will take you through the process of writing an application for working with and manipulating the St. Jude data you've requested. By creating your own application, you will be able to wrap genomic tools and packages from external sources, as well as any tool or application you might have written yourself.\n\n\n\n\nTip\n\n\nThe complete contents of this guide is hosted on the \nSt. Jude App Tutorial\n repository on GitHub. Feel free to clone the repository and use it as a reference while following this tutorial or try building the application and running it on your own project.\n\n\n\n\nOverview\n\n\nThe biggest difference between running an application in the cloud (as opposed to running it in a local environment) is the way we access that data and manipulate it. Writing and running your own cloud application grants numerous benefits. It allows you to submit numerous jobs in parallel, access your data from anywhere with an Internet connection, and utilize resources and compute power at a fraction of the cost (when compared to building your own infrastructure).\n\n\nWriting your own application will allow you to wrap custom tools to manipulate any data that you have previously requested. When you run your application, the request gets sent to a virtualized Linux container (Ubuntu 14.04 or 16.04) where any dependencies are installed and where your script will be run. Any tools or packages that you include (either through the included package managers, or bundled together in your project) will be available locally on the virtual Linux machine.\n\n\nHowever, there are differences in how we manage our data. When a job is submitted, a virtual machine is provisioned specifically for that job request, meaning that it is spun up at-will or when needed. It also implies that once the job has completed, the virtual machine will be reprovisioned or deleted. Any job output or data must be uploaded back to the project space.\n\n\nIn this tutorial, we will be wrapping the \nFastQC\n, a quality control tool for raw sequence data, into our application. This will allow us to run FastQC on any of the St. Jude next generation sequencing data in the cloud. For specific information about how FastQC works, please refer to the \nFastQC documentation\n.\n\n\nRequesting Data\n\n\nAfter submitting a \ndata request\n, it will be sent out for evaluation. Once it has been approved, the data will be vended and it will be accessible in a DNAnexus project. You can view all the available projects and data from the \nManage Data\n page where you can view the request name, creation date, total number of files, what files you have immediate access to, and the status of your request. You will also be able to submit required documentation on the Manage Data page.\n\n\nIf you click on a request, it will take you to the DNAnexus platform, where you can view all the files available to you and your project. When a request is submitted, the project will be created, but the data will not be available until your request has been reviewed and approved. Once approved, you should be able to view all the available data from your request. When the data is vended, it will typically look something like:\n\n\nproject_space/\n\u251c\u2500\u2500 restricted/\n\u2502   \u251c\u2500\u2500 bam/\n\u2502   \u251c\u2500\u2500 gVCF/\n\u2502   \u251c\u2500\u2500 Somatic_VCF/\n\u2502   \u2514\u2500\u2500 CNV/\n\u2514\u2500\u2500 SAMPLE_INFO.txt\n\n\n\n\n\nThe \nSAMPLE_INFO.txt\n file provides all the metadata associated with the request, and the restricted folder contains all the data separated by file type (for more info, refer to the \nMetadata Provided\n section). The other folders will contain the respective file types you included in your request.\n\n\nAfter your data access request has been approved, we can begin writing our app.\n\n\nFor this tutorial, I have requested the PCGP dataset and once my access request has been approved, my project directory space will look like the following.\n\n\n\n\nWriting the Application\n\n\nRequirements\n\n\n\n\n\n\n\n\n\n\nTool\n\n\nDownload\n\n\nWebsite\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\ndx-toolkit\n\n\nSource\n\n\nDNAnexus\n\n\nv0.276.0\n\n\n\n\n\n\nFastQC\n\n\nSource\n\n\nBabraham Bioinformatics\n\n\nv0.11.8\n\n\n\n\n\n\n\n\n\n\nFor this application, we will be using the \ndx-app-wizard\n command that is included in the \ndx-toolkit\n. \ndx-app-wizard\n is an interactive prompt that creates a boilerplate project that will allow you to quickly create an application. For more on \ndx-app-wizard\n, refer to the DNAnexus wiki article on \nIntro to Building Apps\n. Before continuing, be sure to refer to the \ncommand line interaction page\n for a walkthrough on how to install \ndx-toolkit\n and how to select your project workspace.\n\n\n\n\nTip\n\n\nIt is not necessary to use \ndx-app-wizard\n. All the necessary files and project directory structure can be created manually. However, \ndx-app-wizard\n provides a quick and easy way to get started. For more information, refer to the \nAdvanced App Tutorial\n.\n\n\n\n\nAll DNAnexus project applications will have the following structure:\n\n\ndx-fastqc-example-app/\n\u251c\u2500\u2500 dxapp.json\n\u251c\u2500\u2500 resources/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\n\n\n\n\nThe \ndxapp.json\n file is a JSON file that contains metadata about the application we are writing that are needed to build and run the app on the DNAnexus Platform. Most notably, you will need to specify all of the inputs your app requires (both input files or any settings you can tune), output files, and other options such as the number of cores and memory required to run the tool. To see the full list of fields, refer to the \nDNAnexus wiki\n guide on the application metadata.\n\n\nThe \ndx-fastqc-example-app.sh\n file is a bash script is what will be executed when the application is run. Any executable binaries that accompany the application, such as other tools or scripts, are placed in the \nresources\n folder. From there, we can call the executable from within the app when it is run.\n\n\nCreating the Project\n\n\nStart by running the \ndx-app-wizard\n command from your terminal.\n\n\n\n\nInfo\n\n\nThis helper tool will create a \nlocal\n directory on your machine. Any code changes we make will be done \ninside\n this local project directory created by \ndx-app-wizard\n. This is because we can write our application locally, \nbuild the application\n, and then \nrun the application\n in the cloud.\n\n\nBuilding the application will compile \ndx-fastqc-example-app\n and then upload it into the project space on the cloud. When we run an application, it will be submitted as a job to be run in the cloud. With this process, we can write the application locally and run it on our data in the cloud, without ever having to utilize personal bandwidth and compute time.\n\n\n\n\n$ dx-app-wizard\n\n\n\n\n\nFor our inputs, we will enter the following:\n\n\n$ App Name: dx-fastqc-example-app\n...\n$ Title \n[]\n: FastQC Example Application\n...\n$ Summary \n[]\n: Uses FastQC to generate quality control reports on raw sequence data.\n...\n$ Version \n[\n0\n.0.1\n]\n: \n0\n.0.1\n...\n$ 1st input name \n(\nENTER\n to finish\n)\n: bam_file\n$ Label \n(\noptional human-readable name\n)\n \n[]\n: BAM File\n...\n$ Choose a class \n(\nTAB\n twice \nfor\n choices\n)\n: file\n$ This is an optional parameter \n[\ny/n\n]\n: n\n...\n$ 1st output name \n(\nENTER\n to finish\n)\n: fastqc_html\n$ Label \n(\noptional human-readable name\n)\n \n[]\n: FastQC HTML Report\n$ Choose a class \n(\nTAB\n twice \nfor\n choices\n)\n: file\n\n$ 2nd output name \n(\nENTER\n to finish\n)\n: fastqc_zip\n$ Label \n(\noptional human-readable name\n)\n \n[]\n: FastQC Zip File\n$ Choose a class \n(\nTAB\n twice \nfor\n choices\n)\n: file\n...\n$ Timeout policy \n[\n48h\n]\n: 48h\n...\n$ Programming language: bash\n...\n$ Will this app need access to the Internet? \n[\ny/N\n]\n: N\n...\n$ Will this app need access to the parent project? \n[\ny/N\n]\n: y\n...\n$ Choose an instance \ntype\n \nfor\n your app \n[\nmem1_ssd1_x4\n]\n: azure:mem1_ssd1_x4\n\n\n\n\n\n\n\nTip\n\n\nAlthough our app doesn't need any Internet access in this example, it may be required for your project. Also be sure to check what instance type you will need in the \nAPI Specifications\n.\n\n\n\n\nThe FastQC executable supports a variety of file formats (BAM, SAM, FastQ, etc.), and outputs a HTML report and a zip file that contains all the graphs and data. We will use that knowledge to write the input and output parameters for our application. We can also specify other parameters such as the timeout policy, programming language, and instance type. For more information, refer to the \nIO and Run Specification\n guide.\n\n\nIntegrating Tools and Packages\n\n\nOnce we have finished creating the basic FastQC application using \ndx-app-wizard\n, the project structure should look like:\n\n\ndx-fastqc-example-app/\n\u251c\u2500\u2500 Readme.developer.md\n\u251c\u2500\u2500 Readme.md\n\u251c\u2500\u2500 dxapp.json\n\u251c\u2500\u2500 resources/\n\u251c\u2500\u2500 src/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\u2514\u2500\u2500 test/\n\n\n\n\n\n\n\nInfo\n\n\nAnything in the resources folder is unpacked into the root directory (\n/\n) of the virtual Linux machine that your application will run on. If we create the directory path \ndx-fastqc-example-app/resources/usr/bin/\n, anything in the bin folder would be unpacked into \n/usr/bin/\n on the Linux machine. This is handy because that path is included in the default \n$PATH\n environment variable.\n\n\nYour application's executable will use \n/home/dnanexus/\n as its current working directory.\n\n\n\n\nThough \ndx-app-wizard\n does not create this, we can create it ourselves.  Paste the following lines into your terminal.\n\n\n$ mkdir -p dx-fastqc-example-app/resources/usr/bin\n\n\n\n\n\nPackaging FastQC\n\n\nTo incorporate FastQC into this project, we need to download the executable binary and package it within the \ndx-fastqc-example-app\n. Download the FastQC v0.11.8 (Win/Linux zip file) and unzip it. After unzipping, move the FastQC folder into the \nresources\n folder.\n\n\n$ unzip fastqc_v0.11.8.zip\n$ mv FastQC /path/to/project/dx-fastqc-example-app/resources/\n\n\n\n\n\nNow, our project will look like this:\n\n\ndx-fastqc-example-app/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 Readme.developer.md\n\u251c\u2500\u2500 dxapp.json\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 resources/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FastQC/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fastqc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\n\n\n\n\nInstalling Dependencies\n\n\n\n\nTip\n\n\nIf you are importing custom tools, or are using tools that rely on various packages and requirements, they can be specified in the \"runSpec\".\n\n\nFor more information on installing dependencies and available software packages, refer to the \nExecution Environment Reference\n.\n\n\n\n\nSome external package managers that we can leverage when building an app include:\n\n\n\n\n\n\n\n\n\n\nPackage Manager\n\n\nApplication\n\n\n\n\n\n\n\n\n\n\nAPT\n\n\nAdvanced Packaging Tool for Ubuntu\n\n\n\n\n\n\nCPAN\n\n\nComprehensive Perl Archive Network\n\n\n\n\n\n\nCRAN\n\n\nComprehensive R Archive Network\n\n\n\n\n\n\ngem\n\n\nPackage Manager for Ruby\n\n\n\n\n\n\npip\n\n\nPyPI (Python Package Index)\n\n\n\n\n\n\n\n\n\n\nOne requirement for FastQC is that it must have a suitable \nJava Runtime Environment\n. To include this in the app, we have to edit the \ndxapp.json\n file. Open \ndxapp.json\n and append the following line to \nrunSpec\n:\n\n\n  \nexecDepends\n:\n \n[\n\n    \n{\nname\n:\n \nopenjdk-7-jre-headless\n,\n\n     \npackage_manager\n:\n \napt\n}\n\n  \n]\n\n\n\n\n\n\nBe sure to add a comma at the very end of the \"file\" object line to accommodate the new \"execDepends\" lines. Now, the \nrunSpec\n object should look like the following:\n\n\n  \n...\n\n  \nrunSpec\n:\n \n{\n\n    \ntimeoutPolicy\n:\n \n{\n\n      \n*\n:\n \n{\n\n        \nhours\n:\n \n48\n\n      \n}\n\n    \n},\n\n    \ninterpreter\n:\n \nbash\n,\n\n    \nrelease\n:\n \n14.04\n,\n\n    \ndistribution\n:\n \nUbuntu\n,\n\n    \nfile\n:\n \nsrc/dx-fastqc-example-app.sh\n,\n\n    \nexecDepends\n:\n \n[\n\n      \n{\nname\n:\n \nopenjdk-7-jre-headless\n,\n\n       \npackage_manager\n:\n \napt\n}\n\n    \n]\n\n  \n}\n,\n\n  \n...\n\n\n\n\n\n\nWhen you build and run your application, the virtual environment will now download \nopenjdk-7\n from Ubuntu's APT package manager as a prerequisite. For more information on how to specify packages from Git, R, or Python, refer to the \nSoftware Packages\n wiki page.\n\n\nCalling FastQC\n\n\nThe last step is to call the FastQC executable from within the app. Open up \nsrc/dx-fastqc-example-app.sh\n with a text editor. Inside this Bash script is where we will be working with FastQC and our data. Before we dive in, its a good idea to add a few useful parameters for the script execution.\n\n\nRight after the Bash shebang (\n#!/bin/bash\n), add the following line:\n\n\nset\n -e -x\n\n\n\n\n\nBelow is a table describing what each flag does:\n\n\n\n\n\n\n\n\n\n\nFlag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-e\n\n\nExit immediately if a command exits with a non-zero status.\n\n\n\n\n\n\n-x\n\n\nPrint each command to standard error before execution.\n\n\n\n\n\n\n\n\n\n\nOur first change has to do with how our BAM file is downloaded. Although \ndx-app-wizard\n automatically generates a line that will download the input file and rename it, we want to keep the original file name because FastQC uses the input file as part of the report name. Remove the \n-o bam_file\n portion so the line looks like the following:\n\n\ndx download \n$bam_file\n       \n# Downloads our input BAM file without renaming\n\n\n\n\n\n\nAfter the application downloads the input file, we need to create the appropriate output directories and run FastQC on our BAM file. Add the following lines to the bash script within the \nmain\n function:\n\n\nmkdir ~/fastqc-out/                                    \n# FastQC Output Folder\n\n/FastQC/fastqc \n$bam_file_name\n -o ~/fastqc-out        \n# Runs FastQC on BAM File\n\n\n\n\n\n\n\n\nTip\n\n\nBe sure to use \n$bam_file_name\n as our input for FastQC. Using \n$bam_file\n only returns the DNAnexus file-id associated with the input file.\n\n\nFor more information on helper variables, refer to the \nAdvanced App Tutorial\n.\n\n\n\n\nUploading Files\n\n\nAfter FastQC finishes, the last thing to do is to upload the reports generated by FastQC to our project. These virtual Linux machines are provisioned at-will, meaning that they are only spun up when a job is submitted. When we create an application and run it in the cloud, we submit it as a job to be executed. When a job gets executed, a virtual machine will download all the necessary requirements (tools, packages, data, etc.) and run the job. Any output files on the machine must be uploaded back to the project space after a job finishes executing. Any information and data not uploaded to the project space will be inaccessible and lost.\n\n\nYou will see two lines generated for us by \ndx-app-wizard\n when we specified the outputs for our application. We need to change these to upload the correct files from our output directory that we specified for FastQC. Otherwise, it assumes they are in the home directory. Before this, we can also (optionally) rename the files to be uploaded. Add the following lines, making sure to replace the two original upload lines.\n\n\nLines to remove/overwrite:\n\n\n# Generated by dx-app-wizard\n\n\nfastqc_html\n=\n$(\ndx upload fastqc_html --brief\n)\n\n\nfastqc_zip\n=\n$(\ndx upload fastqc_zip --brief\n)\n\n\n\n\n\n\nLines to add:\n\n\n# (Optional) Renames the FastQC reports\n\nmv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\nmv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip\n\n\n# Uploads the respective HTML and Zip file (lines to change)\n\n\nfastqc_html\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.html --brief\n)\n\n\nfastqc_zip\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.zip --brief\n)\n\n\n\n\n\n\nWe are using \n$bam_file_prefix\n to help name the output report file. These helper variables are provided to help make file naming easy. For more information on helper variables, refer to the \nAdvanced App Tutorial\n.\n\n\nIn this step, we are also moving the HTML and Zip file generated by FastQC to the directories which will be uploaded.\n\n\nAfter this step, \ndx-fastqc-example-app.sh\n should look like:\n\n\n#!/bin/bash\n\n\n\nset\n -e -x\n\nmain\n()\n \n{\n\n    \necho\n \nValue of bam_file: \n$bam_file\n\n\n    \n# Downloads file from project to virtual machine workspace\n\n    dx download \n$bam_file\n\n\n    \n# Creating output directory for FastQC\n\n    mkdir ~/fastqc-out\n\n    \n# Runs FastQC on BAM file\n\n    /FastQC/fastqc \n$bam_file_name\n -o ~/fastqc-out\n\n    \n# Renames the FastQC reports to include the BAM file prefix\n\n    mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\n    mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip\n\n    \n# Uploads the respective HTML and Zip file\n\n    \nfastqc_html\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.html --brief\n)\n\n    \nfastqc_zip\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.zip --brief\n)\n\n\n    \n# Adds and formats appropriate output variables for your app\n\n    dx-jobutil-add-output fastqc_html \n$fastqc_html\n --class\n=\nfile\n    dx-jobutil-add-output fastqc_zip \n$fastqc_zip\n --class\n=\nfile\n\n}\n\n\n\n\n\n\nBuilding Your App\n\n\nBefore building, ensure that you are in the parent directory of the \nlocal\n project folder generated by \ndx-app-wizard\n. To check, if you enter the command \nls\n, you should see the project folder \ndx-fastqc-example-app/\n appear in the output.\n\n\nTo build your application, enter the following into your terminal:\n\n\n$ dx build dx-fastqc-example-app\n\n\n\n\n\nThis command will package the tools and files as an application which can then be run on the DNAnexus Platform. In the screenshot below, you can see the compiled app in our project workspace selected and highlighted in blue.\n\n\nTo verify that the build was completed successfully, you can enter \ndx ls\n. This should show you all the files in your project space in the cloud.\n\n\n# This will show what files are in your root directory for your project space in the cloud\n\n$ dx ls\n\n\n\n\n\nYou should see something along the lines of this printed out in your terminal. Note that a compiled copy of our \ndx-fastqc-example-app\n now lives in the project.\n\n\n.\n\u251c\u2500\u2500 immediate/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 dx-fastqc-example-app\n\u2514\u2500\u2500 SAMPLE_INFO.txt\n\n\n\n\n\nYou can also \nview the project\n directly from your browser. You will see a similar result.\n\n\n\n\nAny time you make any changes to the scripts or the application, you will need to rebuild the application. To overwrite a previous version of the app, specify the \n-f\n command.\n\n\nYou can also inspect and configure the application by clicking on it.\n\n\n\n\nRunning Your App\n\n\nTo run the \ndx-fastqc-example-app\n, enter the following into the terminal:\n\n\n$ dx run dx-fastqc-example-app -i \nbam_file\n=\n/path/to/\nbam-file\n.bam\n\n\n\n\n\nFor this example, I am using the PCGP dataset and my run command will look like the following:\n\n\n$ dx run dx-fastqc-example-app -i \nbam_file\n=\n/immediate/bam/SJBALL020073_D1.RNA-Seq.bam\n\n\n\n\n\nThe input path will vary depending on how the data looks inside your DNAnexus project, but it might look like the following: \n/restricted/bam/\nbam-file\n.bam\n\n\nYou will be prompted to confirm that you wish to run the application with the following JSON input and whether you would like to monitor the job in your terminal.\n\n\nUsing input JSON:\n\n{\n\n    \nbam_file\n: \n{\n\n        \n$dnanexus_link\n: \n{\n\n            \nproject\n: \nproject-FV9XFG0991ZbPVgQ2jx1vZv5\n,\n            \nid\n: \nfile-FV9gzf8991ZXQ1kv7V3BqgjV\n\n        \n}\n\n    \n}\n\n\n}\n\n\nConfirm running the executable with this input \n[\nY/n\n]\n: Y\nCalling applet-FVbY8Qj991ZQ1863BGK6x0bk with output destination project-FV9XFG0991ZbPVgQ2jx1vZv5:/\n\nJob ID: job-FVbY8Z0991ZXx5v1Fk3QgJPV\nWatch launched job now? \n[\nY/n\n]\n Y\n\nJob Log\n-------\nWatching job job-FVbY8Z0991ZXx5v1Fk3QgJPV. Press Ctrl+C to stop.\n\n\n\n\n\nYou can also monitor active jobs by going to the project space and selecting the \"Monitor\" tab.\n\n\n\n\nJob Completion\n\n\nOnce the job finishes, you will receive an email from DNAnexus (\nnotification@dnanexus.com\n) about whether the job has completed successfully or failed.\n\n\nMake sure to check that these emails don't get sent to your spam folder.\n\n\n\n\nClicking the links in the email should open up a new tab in your browser and take you to the appropriate project. Here, we can see that FastQC has run successfully and that the two files generated by FastQC have been uploaded back into our project space.\n\n\n\n\nAgain, if we run the \ndx ls\n command, we can verify that two new files titled \"fastqc-report.html\" and \"fastqc-report.zip\" are in the root directory of our project.\n\n\n.\n\u251c\u2500\u2500 immediate/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 dx-fastqc-example-app\n\u251c\u2500\u2500 fastqc-report.html\n\u251c\u2500\u2500 fastqc-report.zip\n\u2514\u2500\u2500 SAMPLE_INFO.txt\n\n\n\n\n\nConclusion\n\n\nIf you have made it this far, you have likely wrapped your first genomic analysis tool for use in the cloud. For your reference, we have included the final FastQC application at the \nSt. Jude App Tutorial Repository\n.\n\n\nIf you have any questions or suggestions on how we can improve this tutorial, please \nfile an issue\n, contact us at \nhttps://stjude.cloud/contact\n, or email us at \nsupport@stjude.cloud\n.", 
            "title": "Creating a Cloud App"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#creating-a-cloud-application", 
            "text": "This guide will take you through the process of writing an application for working with and manipulating the St. Jude data you've requested. By creating your own application, you will be able to wrap genomic tools and packages from external sources, as well as any tool or application you might have written yourself.   Tip  The complete contents of this guide is hosted on the  St. Jude App Tutorial  repository on GitHub. Feel free to clone the repository and use it as a reference while following this tutorial or try building the application and running it on your own project.", 
            "title": "Creating a Cloud Application"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#overview", 
            "text": "The biggest difference between running an application in the cloud (as opposed to running it in a local environment) is the way we access that data and manipulate it. Writing and running your own cloud application grants numerous benefits. It allows you to submit numerous jobs in parallel, access your data from anywhere with an Internet connection, and utilize resources and compute power at a fraction of the cost (when compared to building your own infrastructure).  Writing your own application will allow you to wrap custom tools to manipulate any data that you have previously requested. When you run your application, the request gets sent to a virtualized Linux container (Ubuntu 14.04 or 16.04) where any dependencies are installed and where your script will be run. Any tools or packages that you include (either through the included package managers, or bundled together in your project) will be available locally on the virtual Linux machine.  However, there are differences in how we manage our data. When a job is submitted, a virtual machine is provisioned specifically for that job request, meaning that it is spun up at-will or when needed. It also implies that once the job has completed, the virtual machine will be reprovisioned or deleted. Any job output or data must be uploaded back to the project space.  In this tutorial, we will be wrapping the  FastQC , a quality control tool for raw sequence data, into our application. This will allow us to run FastQC on any of the St. Jude next generation sequencing data in the cloud. For specific information about how FastQC works, please refer to the  FastQC documentation .", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#requesting-data", 
            "text": "After submitting a  data request , it will be sent out for evaluation. Once it has been approved, the data will be vended and it will be accessible in a DNAnexus project. You can view all the available projects and data from the  Manage Data  page where you can view the request name, creation date, total number of files, what files you have immediate access to, and the status of your request. You will also be able to submit required documentation on the Manage Data page.  If you click on a request, it will take you to the DNAnexus platform, where you can view all the files available to you and your project. When a request is submitted, the project will be created, but the data will not be available until your request has been reviewed and approved. Once approved, you should be able to view all the available data from your request. When the data is vended, it will typically look something like:  project_space/\n\u251c\u2500\u2500 restricted/\n\u2502   \u251c\u2500\u2500 bam/\n\u2502   \u251c\u2500\u2500 gVCF/\n\u2502   \u251c\u2500\u2500 Somatic_VCF/\n\u2502   \u2514\u2500\u2500 CNV/\n\u2514\u2500\u2500 SAMPLE_INFO.txt  The  SAMPLE_INFO.txt  file provides all the metadata associated with the request, and the restricted folder contains all the data separated by file type (for more info, refer to the  Metadata Provided  section). The other folders will contain the respective file types you included in your request.  After your data access request has been approved, we can begin writing our app.  For this tutorial, I have requested the PCGP dataset and once my access request has been approved, my project directory space will look like the following.", 
            "title": "Requesting Data"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#writing-the-application", 
            "text": "", 
            "title": "Writing the Application"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#requirements", 
            "text": "Tool  Download  Website  Version      dx-toolkit  Source  DNAnexus  v0.276.0    FastQC  Source  Babraham Bioinformatics  v0.11.8      For this application, we will be using the  dx-app-wizard  command that is included in the  dx-toolkit .  dx-app-wizard  is an interactive prompt that creates a boilerplate project that will allow you to quickly create an application. For more on  dx-app-wizard , refer to the DNAnexus wiki article on  Intro to Building Apps . Before continuing, be sure to refer to the  command line interaction page  for a walkthrough on how to install  dx-toolkit  and how to select your project workspace.   Tip  It is not necessary to use  dx-app-wizard . All the necessary files and project directory structure can be created manually. However,  dx-app-wizard  provides a quick and easy way to get started. For more information, refer to the  Advanced App Tutorial .   All DNAnexus project applications will have the following structure:  dx-fastqc-example-app/\n\u251c\u2500\u2500 dxapp.json\n\u251c\u2500\u2500 resources/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 dx-fastqc-example-app.sh  The  dxapp.json  file is a JSON file that contains metadata about the application we are writing that are needed to build and run the app on the DNAnexus Platform. Most notably, you will need to specify all of the inputs your app requires (both input files or any settings you can tune), output files, and other options such as the number of cores and memory required to run the tool. To see the full list of fields, refer to the  DNAnexus wiki  guide on the application metadata.  The  dx-fastqc-example-app.sh  file is a bash script is what will be executed when the application is run. Any executable binaries that accompany the application, such as other tools or scripts, are placed in the  resources  folder. From there, we can call the executable from within the app when it is run.", 
            "title": "Requirements"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#creating-the-project", 
            "text": "Start by running the  dx-app-wizard  command from your terminal.   Info  This helper tool will create a  local  directory on your machine. Any code changes we make will be done  inside  this local project directory created by  dx-app-wizard . This is because we can write our application locally,  build the application , and then  run the application  in the cloud.  Building the application will compile  dx-fastqc-example-app  and then upload it into the project space on the cloud. When we run an application, it will be submitted as a job to be run in the cloud. With this process, we can write the application locally and run it on our data in the cloud, without ever having to utilize personal bandwidth and compute time.   $ dx-app-wizard  For our inputs, we will enter the following:  $ App Name: dx-fastqc-example-app\n...\n$ Title  [] : FastQC Example Application\n...\n$ Summary  [] : Uses FastQC to generate quality control reports on raw sequence data.\n...\n$ Version  [ 0 .0.1 ] :  0 .0.1\n...\n$ 1st input name  ( ENTER  to finish ) : bam_file\n$ Label  ( optional human-readable name )   [] : BAM File\n...\n$ Choose a class  ( TAB  twice  for  choices ) : file\n$ This is an optional parameter  [ y/n ] : n\n...\n$ 1st output name  ( ENTER  to finish ) : fastqc_html\n$ Label  ( optional human-readable name )   [] : FastQC HTML Report\n$ Choose a class  ( TAB  twice  for  choices ) : file\n\n$ 2nd output name  ( ENTER  to finish ) : fastqc_zip\n$ Label  ( optional human-readable name )   [] : FastQC Zip File\n$ Choose a class  ( TAB  twice  for  choices ) : file\n...\n$ Timeout policy  [ 48h ] : 48h\n...\n$ Programming language: bash\n...\n$ Will this app need access to the Internet?  [ y/N ] : N\n...\n$ Will this app need access to the parent project?  [ y/N ] : y\n...\n$ Choose an instance  type   for  your app  [ mem1_ssd1_x4 ] : azure:mem1_ssd1_x4   Tip  Although our app doesn't need any Internet access in this example, it may be required for your project. Also be sure to check what instance type you will need in the  API Specifications .   The FastQC executable supports a variety of file formats (BAM, SAM, FastQ, etc.), and outputs a HTML report and a zip file that contains all the graphs and data. We will use that knowledge to write the input and output parameters for our application. We can also specify other parameters such as the timeout policy, programming language, and instance type. For more information, refer to the  IO and Run Specification  guide.", 
            "title": "Creating the Project"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#integrating-tools-and-packages", 
            "text": "Once we have finished creating the basic FastQC application using  dx-app-wizard , the project structure should look like:  dx-fastqc-example-app/\n\u251c\u2500\u2500 Readme.developer.md\n\u251c\u2500\u2500 Readme.md\n\u251c\u2500\u2500 dxapp.json\n\u251c\u2500\u2500 resources/\n\u251c\u2500\u2500 src/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\u2514\u2500\u2500 test/   Info  Anything in the resources folder is unpacked into the root directory ( / ) of the virtual Linux machine that your application will run on. If we create the directory path  dx-fastqc-example-app/resources/usr/bin/ , anything in the bin folder would be unpacked into  /usr/bin/  on the Linux machine. This is handy because that path is included in the default  $PATH  environment variable.  Your application's executable will use  /home/dnanexus/  as its current working directory.   Though  dx-app-wizard  does not create this, we can create it ourselves.  Paste the following lines into your terminal.  $ mkdir -p dx-fastqc-example-app/resources/usr/bin", 
            "title": "Integrating Tools and Packages"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#packaging-fastqc", 
            "text": "To incorporate FastQC into this project, we need to download the executable binary and package it within the  dx-fastqc-example-app . Download the FastQC v0.11.8 (Win/Linux zip file) and unzip it. After unzipping, move the FastQC folder into the  resources  folder.  $ unzip fastqc_v0.11.8.zip\n$ mv FastQC /path/to/project/dx-fastqc-example-app/resources/  Now, our project will look like this:  dx-fastqc-example-app/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 Readme.developer.md\n\u251c\u2500\u2500 dxapp.json\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 resources/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FastQC/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fastqc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 dx-fastqc-example-app.sh", 
            "title": "Packaging FastQC"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#installing-dependencies", 
            "text": "Tip  If you are importing custom tools, or are using tools that rely on various packages and requirements, they can be specified in the \"runSpec\".  For more information on installing dependencies and available software packages, refer to the  Execution Environment Reference .   Some external package managers that we can leverage when building an app include:      Package Manager  Application      APT  Advanced Packaging Tool for Ubuntu    CPAN  Comprehensive Perl Archive Network    CRAN  Comprehensive R Archive Network    gem  Package Manager for Ruby    pip  PyPI (Python Package Index)      One requirement for FastQC is that it must have a suitable  Java Runtime Environment . To include this in the app, we have to edit the  dxapp.json  file. Open  dxapp.json  and append the following line to  runSpec :     execDepends :   [ \n     { name :   openjdk-7-jre-headless , \n      package_manager :   apt } \n   ]   Be sure to add a comma at the very end of the \"file\" object line to accommodate the new \"execDepends\" lines. Now, the  runSpec  object should look like the following:     ... \n   runSpec :   { \n     timeoutPolicy :   { \n       * :   { \n         hours :   48 \n       } \n     }, \n     interpreter :   bash , \n     release :   14.04 , \n     distribution :   Ubuntu , \n     file :   src/dx-fastqc-example-app.sh , \n     execDepends :   [ \n       { name :   openjdk-7-jre-headless , \n        package_manager :   apt } \n     ] \n   } , \n   ...   When you build and run your application, the virtual environment will now download  openjdk-7  from Ubuntu's APT package manager as a prerequisite. For more information on how to specify packages from Git, R, or Python, refer to the  Software Packages  wiki page.", 
            "title": "Installing Dependencies"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#calling-fastqc", 
            "text": "The last step is to call the FastQC executable from within the app. Open up  src/dx-fastqc-example-app.sh  with a text editor. Inside this Bash script is where we will be working with FastQC and our data. Before we dive in, its a good idea to add a few useful parameters for the script execution.  Right after the Bash shebang ( #!/bin/bash ), add the following line:  set  -e -x  Below is a table describing what each flag does:      Flag  Description      -e  Exit immediately if a command exits with a non-zero status.    -x  Print each command to standard error before execution.      Our first change has to do with how our BAM file is downloaded. Although  dx-app-wizard  automatically generates a line that will download the input file and rename it, we want to keep the original file name because FastQC uses the input file as part of the report name. Remove the  -o bam_file  portion so the line looks like the following:  dx download  $bam_file         # Downloads our input BAM file without renaming   After the application downloads the input file, we need to create the appropriate output directories and run FastQC on our BAM file. Add the following lines to the bash script within the  main  function:  mkdir ~/fastqc-out/                                     # FastQC Output Folder \n/FastQC/fastqc  $bam_file_name  -o ~/fastqc-out         # Runs FastQC on BAM File    Tip  Be sure to use  $bam_file_name  as our input for FastQC. Using  $bam_file  only returns the DNAnexus file-id associated with the input file.  For more information on helper variables, refer to the  Advanced App Tutorial .", 
            "title": "Calling FastQC"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#uploading-files", 
            "text": "After FastQC finishes, the last thing to do is to upload the reports generated by FastQC to our project. These virtual Linux machines are provisioned at-will, meaning that they are only spun up when a job is submitted. When we create an application and run it in the cloud, we submit it as a job to be executed. When a job gets executed, a virtual machine will download all the necessary requirements (tools, packages, data, etc.) and run the job. Any output files on the machine must be uploaded back to the project space after a job finishes executing. Any information and data not uploaded to the project space will be inaccessible and lost.  You will see two lines generated for us by  dx-app-wizard  when we specified the outputs for our application. We need to change these to upload the correct files from our output directory that we specified for FastQC. Otherwise, it assumes they are in the home directory. Before this, we can also (optionally) rename the files to be uploaded. Add the following lines, making sure to replace the two original upload lines.  Lines to remove/overwrite:  # Generated by dx-app-wizard  fastqc_html = $( dx upload fastqc_html --brief )  fastqc_zip = $( dx upload fastqc_zip --brief )   Lines to add:  # (Optional) Renames the FastQC reports \nmv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\nmv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip # Uploads the respective HTML and Zip file (lines to change)  fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief )  fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief )   We are using  $bam_file_prefix  to help name the output report file. These helper variables are provided to help make file naming easy. For more information on helper variables, refer to the  Advanced App Tutorial .  In this step, we are also moving the HTML and Zip file generated by FastQC to the directories which will be uploaded.  After this step,  dx-fastqc-example-app.sh  should look like:  #!/bin/bash  set  -e -x\n\nmain ()   { \n     echo   Value of bam_file:  $bam_file \n\n     # Downloads file from project to virtual machine workspace \n    dx download  $bam_file \n\n     # Creating output directory for FastQC \n    mkdir ~/fastqc-out\n\n     # Runs FastQC on BAM file \n    /FastQC/fastqc  $bam_file_name  -o ~/fastqc-out\n\n     # Renames the FastQC reports to include the BAM file prefix \n    mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\n    mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip\n\n     # Uploads the respective HTML and Zip file \n     fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief ) \n     fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief ) \n\n     # Adds and formats appropriate output variables for your app \n    dx-jobutil-add-output fastqc_html  $fastqc_html  --class = file\n    dx-jobutil-add-output fastqc_zip  $fastqc_zip  --class = file }", 
            "title": "Uploading Files"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#building-your-app", 
            "text": "Before building, ensure that you are in the parent directory of the  local  project folder generated by  dx-app-wizard . To check, if you enter the command  ls , you should see the project folder  dx-fastqc-example-app/  appear in the output.  To build your application, enter the following into your terminal:  $ dx build dx-fastqc-example-app  This command will package the tools and files as an application which can then be run on the DNAnexus Platform. In the screenshot below, you can see the compiled app in our project workspace selected and highlighted in blue.  To verify that the build was completed successfully, you can enter  dx ls . This should show you all the files in your project space in the cloud.  # This will show what files are in your root directory for your project space in the cloud \n$ dx ls  You should see something along the lines of this printed out in your terminal. Note that a compiled copy of our  dx-fastqc-example-app  now lives in the project.  .\n\u251c\u2500\u2500 immediate/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 dx-fastqc-example-app\n\u2514\u2500\u2500 SAMPLE_INFO.txt  You can also  view the project  directly from your browser. You will see a similar result.   Any time you make any changes to the scripts or the application, you will need to rebuild the application. To overwrite a previous version of the app, specify the  -f  command.  You can also inspect and configure the application by clicking on it.", 
            "title": "Building Your App"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#running-your-app", 
            "text": "To run the  dx-fastqc-example-app , enter the following into the terminal:  $ dx run dx-fastqc-example-app -i  bam_file = /path/to/ bam-file .bam  For this example, I am using the PCGP dataset and my run command will look like the following:  $ dx run dx-fastqc-example-app -i  bam_file = /immediate/bam/SJBALL020073_D1.RNA-Seq.bam  The input path will vary depending on how the data looks inside your DNAnexus project, but it might look like the following:  /restricted/bam/ bam-file .bam  You will be prompted to confirm that you wish to run the application with the following JSON input and whether you would like to monitor the job in your terminal.  Using input JSON: { \n     bam_file :  { \n         $dnanexus_link :  { \n             project :  project-FV9XFG0991ZbPVgQ2jx1vZv5 ,\n             id :  file-FV9gzf8991ZXQ1kv7V3BqgjV \n         } \n     }  } \n\nConfirm running the executable with this input  [ Y/n ] : Y\nCalling applet-FVbY8Qj991ZQ1863BGK6x0bk with output destination project-FV9XFG0991ZbPVgQ2jx1vZv5:/\n\nJob ID: job-FVbY8Z0991ZXx5v1Fk3QgJPV\nWatch launched job now?  [ Y/n ]  Y\n\nJob Log\n-------\nWatching job job-FVbY8Z0991ZXx5v1Fk3QgJPV. Press Ctrl+C to stop.  You can also monitor active jobs by going to the project space and selecting the \"Monitor\" tab.", 
            "title": "Running Your App"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#job-completion", 
            "text": "Once the job finishes, you will receive an email from DNAnexus ( notification@dnanexus.com ) about whether the job has completed successfully or failed.  Make sure to check that these emails don't get sent to your spam folder.   Clicking the links in the email should open up a new tab in your browser and take you to the appropriate project. Here, we can see that FastQC has run successfully and that the two files generated by FastQC have been uploaded back into our project space.   Again, if we run the  dx ls  command, we can verify that two new files titled \"fastqc-report.html\" and \"fastqc-report.zip\" are in the root directory of our project.  .\n\u251c\u2500\u2500 immediate/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 dx-fastqc-example-app\n\u251c\u2500\u2500 fastqc-report.html\n\u251c\u2500\u2500 fastqc-report.zip\n\u2514\u2500\u2500 SAMPLE_INFO.txt", 
            "title": "Job Completion"
        }, 
        {
            "location": "/guides/data/creating-a-cloud-app/#conclusion", 
            "text": "If you have made it this far, you have likely wrapped your first genomic analysis tool for use in the cloud. For your reference, we have included the final FastQC application at the  St. Jude App Tutorial Repository .  If you have any questions or suggestions on how we can improve this tutorial, please  file an issue , contact us at  https://stjude.cloud/contact , or email us at  support@stjude.cloud .", 
            "title": "Conclusion"
        }, 
        {
            "location": "/guides/tools/pecan-pie/", 
            "text": "Pecan PIE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nMichael Edmonson, Aman Patel\n\n\n\n\n\n\nPublication\n\n\nZhang et al., NEJM 2015\n, supplementary appendix pp. 7-10\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nPecan PIE (the \nPe\ndiatric \nCan\ncer Variant \nP\nathogenicity\n\nI\nnformation \nE\nxchange) is a cloud-based variant classification\nand interpretation service. It annotates and ranks variants by putative\npathogenicity, then displays them in an interactive web interface for\nformal review and classification following ACMG guidelines. The portal\nalso contains a repository of expert-reviewed germline mutations that\nmay predispose individuals to cancer. It is free for non-commercial use.\n\n\nPecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that\npowers our clinical and research genomics projects. Medal Ceremony\nprovides a 3-level ranking of putative pathogenity - Gold, Silver or\nBronze - for mutations within disease-related genes. Medal assignment is\nbased on matches to 22 mutation databases, mutation type, population\nfrequency, tumor suppressor status and predicted functional impact. The\nevidence used for medal assignment is imported into an interactive\nvariant review page where an analyst can enter additional curated\ninformation such as primary diagnosis, presence of subsequent neoplasm,\nfamily history and related literature. Classification tags can be\nassigned to curated data enabling automated calculation of pathogenicity\nrating based on ACMG/AMP 2015 guidelines.\n\n\nSee \nfile_download\n PowerPoint slides\n\npresented at the ASHG 2017 annual meeting (note that some of this\ninformation is out of date, various improvements have been made since\nthen).\n\n\nGo to \nhttps://pecan.stjude.cloud/pie\n to get started!\n\n\nOverview\n\n\n\n\nAn overview of the Pecan PIE workflow:\n\n\n\n\nLog in and upload a VCF of SNVs and indels.\n\n\nThe portal will process your variants, notifying you upon\n    completion. Variants are annotated with VEP+ (VEP with\n    postprocessing for enhanced splice variant calling) then classified\n    with Medal Ceremony.\n\n\nBrowse results, which include a detailed page for each variation.\n    Variants may be formally classified with an interface based on ACMG\n    guidelines.\n\n\n\n\nGetting started\n\n\nStart by logging into the portal with a DNAnexus account, creating an\naccount if you need one. PIE uses DNAnexus as a secure cloud backend.\nLogging in is required for private storage of your data and so that we\ncan send you e-mail notifications when your analysis jobs are complete.\nPIE is free for non-commercial use. St. Jude pays the (small) cloud\ncomputing costs, your DNAnexus account will not be billed.\n\n\nUploading data\n\n\nPecan PIE takes standard VCF files as input, which may be either\nuncompressed or compressed with \nbgzip\n.\n\n\n\n\nClick the \"Securely upload a VCF file\" button.\n\n\nChoose the genome your variants were mapped to, which may be either\n   GRCh37-lite or GRCh38.\n\n\n\n\nAdvanced options\n\n\nThe \"Advanced option\" panel lets you customize the behavior of the\npipeline:\n\n\n\n\nGene list: Pick a gene list from the pulldown. This filters your\n  variants to genes in the specified list. This option is required and turned\n  on automatically if your uploaded file is 2 megabytes or larger. See the\n  \nfrequently asked questions\n for more\n  information. This option reduces the variant processing burden on PIE by\n  removing variants that will not be assigned a medal in any case because\n  they are not on the cancer predisposition gene list. You can review the\n  genes by clicking on the link that will appear just below the pull down\n  titled \"See gene list\".\n\n\nCustom gene list: Choosing \"custom\" as your gene list will open a\n  window that will let you paste in a list of genes. Any invalid genes will\n  be dropped from your list automatically. You can separate your genes by spaces\n  or new lines.\n\n\nMax Population frequency: PIE by default will not call medals for\n  variants present in the ExAC (ex-TCGA) database at an allele\n  frequency greater than 0.001. This option lets you override the\n  filtering threshold to whatever frequency you prefer. To disable\n  filtering altogether, specify a value of 1.\n\n\n\n\nProgress page\n\n\nAfter uploading is complete you will be taken to a status screen showing\nthe progress of your job through the system. Analysis typically takes\n10-15 minutes depending on file size and system availability.\n\n\nIt isn't necessary to keep your browser open on this page until your\nresults are ready: the system will e-mail you with a link to return to\nyour results. Optional browser notifications are also available.\n\n\nAnalysis of Results\n\n\nResults browser\n\n\nWhen your job is complete you will be taken to an overview page where\nyou can browse your results and examine a detailed results page for each\nvariant.\n\n\n\n\nThe variants in the results can be filtered by:\n\n\n\n\n\n\n\n\nFilter\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nClass\n\n\nPredicted effect of variant on protein coding, e.g. missense, nonsense, etc.\n\n\n\n\n\n\nSomatic medal\n\n\nMedal assigned to the variant by the somatic classifier.\n\n\n\n\n\n\nGermline medal\n\n\nMedal assigned to the variant by the germline classifier.\n\n\n\n\n\n\nCommittee Classification\n\n\nIf the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank.\n\n\n\n\n\n\n\n\nThe \"search\" box lets you filter the results by gene and/or amino acid\nchange. The view is dynamically filtered to matching variants as you\ntype.\n\n\nMedal meaning\n\n\nMedals are only assigned for coding and splice-related variants in\ndisease predisposition genes. Germline medals are only assigned for\nnovel variants or those present in the ExAC (ex-TCGA) database with a\nMAF no greater than 0.1% (0.001 expressed fractionally).\n\n\n\n\n\n\nGold medals\n are assigned to truncations in tumor suppressor genes,\nhotspots derived from the COSMIC database, as well as perfect matches to\nvariants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases.\n\n\nSilver medals\n are assigned to in-frame indels, truncations in non-tumor\nsuppressor genes, variants predicted deleterious by damage-prediction\nalgorithms, variants receiving a gold medal from the somatic classifier,\nand perfect matches to variants in the following databases: ClinVar\n(predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD.\n\n\nBronze medals\n are assigned to variants predicted tolerated by\ndamage-prediction algorithms. Variants having an imperfect match to a\ndatabase (i.e. different variants at the same genomic position or codon)\ntypically receive a lesser medal.\n\n\n\n\nA summary graphic can be found in slide 4 of the ASHG 2017 presentation (\ndownload here\n). \nFor additional details see \nZhang et al., NEJM\n2015\n\n(supplementary appendix pp. 7-10).\n\n\nVariant page\n\n\nEach variant links to a detailed variant page, which integrates data\nfrom a variety of sources. If either you or the St. Jude germline\nvariant review committee have annotated a variant, that information will\nbe pre-populated.\n\n\nSummary information\n\n\nThe top of the page shows a summary of the variant, including its\ngenomic and HGVS annotations, predicted effect on the protein, and\nsomatic and germline medals. A description of the gene from Entrez\nfollows, and a custom description or selection rationale may also be\nentered.\n\n\nMedal call information\n\n\nClicking on one of the medal icons (gold, silver, bronze, unknown) also\non the top of the page will show a summary of information related to the\nmedal call.\n\n\nProteinPaint\n\n\nAn embedded version of ProteinPaint (\nZhou et al., Nat. Genet.\n2016\n) appears next, showing\nthe variant in the context of a number of pediatric datasets including\nPCGP. A link is provided to the main ProteinPaint application which\nprovides visualizations for additional datasets, including COSMIC and\nClinVar.\n\n\nASHG pathogenicity classification\n\n\nFormal variant pathogenicity classification is supported by an interface\nimplementing ACMG guidelines (\nRichards et al., Genet Med.\n2015\n).\nThe analyst reviews a series of curated category tags, assigning\napplicable tags to the variant and optionally supplying additional\ninformation for each such as PubMed IDs and supporting evidence. The\nsystem will then compute an appropriate pathogenicity score based on the\nuser-flagged categories. Additional free-form custom evidence can also\nbe entered. This structured approach both helps eliminate arbitrary\ndecision-making from the pathogenicity classification process and also\nconstructs a concise summary of the logic and evidence supporting the\nfinal call.\n\n\nClinVar and allele frequency\n\n\nMatches of the variant in ClinVar are also provided, along with\npredicted clinical significance and review status.\n\n\nAllele frequencies for the variant in the PCGP (somatic and\ngermline),NHLBI ESP 6500, and ExAC databases are presented both as\nfractional values and on a log10 plot. Detailed allele population\nbreakdowns are provided for ExAC.\n\n\nDamage prediction algorithms\n\n\nPrecomputed damage-prediction algorithm calls for nonsynonymous coding\nSNVs are presented from the dbNSFP database. Available algorithms are\nPolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT.\nThe calls are presented in a circular diagram with entries color-coded\nbased on the predicted severity of the result.\n\n\nMedal ceremony and linkouts\n\n\nAdditional output from medal ceremony classification can also be\nreviewed. This is only loosely structured, additional fields here may\neventually be integrated into Pecan PIE.\n\n\nLinks are provided to relevant dbSNP entries and other information\nsources.\n\n\nFinal classification\n\n\nThe final 5-tier ACMG classification can be selected after which the\ndecision will be marked as reviewed. A checkbox is also available to\nindicate this variant is a potential candidate for functional review.\n\n\nStandalone usage\n\n\nThis section is intended only for users who want to invoke Pecan PIE's\nunderlying analysis pipelines independently on the\n\nDNAnexus\n platform. If you just want to use\nthe Pecan PIE website you can safely ignore this section of the\ndocumentation. We assume familiarity with the DNAnexus platform. If you aren't\nfamiliar with this, DNAnexus' \nquickstart guide\n is a\ngreat place to start.\n\n\n\n\nWarning\n\n\nThis section of the guide is only relevant to power users!\n\n\n\n\nTwo DNAnexus cloud application pipelines were created during the\ndevelopment of Pecan PIE:\n\n\n\n\n\n\n\n\nName\n\n\nCorresponding DNAnexus App\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVEP+\n\n\napp-stjude_vep_plus\n\n\nA cloud installation of \nVEP\n with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format.\n\n\n\n\n\n\nMedal Ceremony\n\n\napp-stjude_medal_ceremony\n\n\nAdditional annotation and automated variant classification. Requires a special input format which is produced by VEP+.\n\n\n\n\n\n\n\n\nPermissions\n\n\nIn order to run the cloud pipelines independently, your DNAnexus account\nneeds to be granted permissions to access them. After your initial login\nto St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single\nlogin is required even if you just want to use the standalone pipelines\nrather than the Pecan PIE portal (\ncontact\nus\n if you encounter problems accessing\nthe pipelines).\n\n\nThere are two methods of running pipelines on DNAnexus:\n\n\n\n\nDNAnexus GUI.\n DNAnexus provides a standardized graphical user interface for\n    configurating, launching, and monitoring jobs on the cloud. Our\n    pipelines can be run like any other DNAnexus pipeline.\n\n\nCommand line.\n Jobs may also be invoked via the \ndx\n command line\n    client. Command-line use allows submitting cloud jobs without\n    interacting with a GUI, and so supports scripting and easier\n    integration with local workflows. See \nthis section\n\n    for information on how to get set up with the \ndx-toolkit\n.\n\n\n\n\n\n\nNote\n\n\nThe following examples demonstrate command-line usage.\n\n\n\n\nUploading files\n\n\nAll input files must be uploaded onto the DNAnexus platform. When\nspecifying files for input you can use either the DNAnexus fie IDs (e.g.\n\nfile-FBgvp680gz1bGQ5p8yZKz69g\n), or the filenames if they are unique. For\nan idea of how to upload files to DNAnexus, see \nthis guide\n.\n\n\nStep 1: Running VEP+\n\n\nTo run the VEP+ DNAnexus app, you can use the following \ndx\n\ncommand with your own inputs in place of the example's:\n\n\ndx run app-stjude_vep_plus -iinput_file\n=\nmy_vcf.vcf -igenome_string\n=\nGRCh37-lite -igermline_reviewable_only\n=\ntrue\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngenome_string\n must be either \nGRCh37-lite\n or \nGRCh38\n. If \nGRCh38\n\nis specified, variants will be lifted over to \nGRCh37-lite\n in output,\ni.e. the output will always be \nGRCh37-lite\n (Medal Ceremony currently only supports \nGRCh37-lite\n).\n\n\nThe input VCF specified by \ninput_file\n may be either\nuncompressed, or compressed with \nbgzip\n \nonly\n (htslib/tabix\npackages).\n\n\nThe \ngermline_reviewable_only\n parameter is optional, but\nstrongly recommended. If specified, only variants in disease-gene\nrelated intervals will be annotated, which is appropriate for\nMedal Ceremony. If this option is not specified all variants will\nbe annotated, which depending on the size of your VCF might take a\nlot longer, and many of the resulting variants won't be usable by\nMedal Ceremony. If you want to do this anyway and have a large\nnumber of variants, consider submitting your job to an instance\nwith more CPU cores (e.g. \nmem1_ssd1_x16\n or \nmem1_ssd1_x32\n) as\nthe code will take advantage of the additional cores. If you are\nusing a custom gene list (below) that takes precedence and this\nparameter is not needed.\n\n\nThe optional parameter \ncustom_genes_file\n specifies a plain\ntext file of HUGO gene symbols to analyze (whitespace separated,\nor one per line). If specified, analysis will be restricted to\nthese genes only.\n\n\nThis pipeline produces two output files, \noutput_file\n contains\nannotations for all variants, while \nmedal_prep_output_file\n is\nthe specially-filtered and formatted file required as input to\nMedal Ceremony below.\n\n\n\n\n\n\nStep 2: Running Medal Ceremony\n\n\nTo run the medal ceremony DNAnexus app, you can use the following \ndx\n\ncommand with your own inputs in place of the example's:\n\n\ndx run app-stjude_medal_ceremony -iinfile\n=\nmedal_prep_output_file\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe optional parameter \ncustom_genes_file\n operates in the same\nway as in the VEP+ pipeline above. For custom gene lists to work\nproperly this parameter must be specified when running both the\nVEP+ and Medal Ceremony pipelines.\n\n\nThe optional parameter \nmax_population_frequency\n may be\nspecified, a fractional value representing the maximum population\nfrequency allowed for a variant in the ExAC (ex-TCGA) database to\nreceive a medal. The default is 0.001, a.k.a. \".1%\".\n\n\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to reach out on\n\nour contact form\n.\n\n\nQ: Which files are supported?\n\n\nPIE works with variants in VCF format:\n\n\n\n\nUploaded files must be compliant with the \nVCF specification\n.\n\n\nVCF files may be either uncompressed, or compressed with \nbgzip\n \nonly\n. \nbgzip\n is part of the htslib/tabix packages (see below).\n\n\n\n\nImproperly formatted VCF files will not work with PIE. Some common\nproblems include:\n\n\n\n\nMissing header line\n\n\nMissing required columns\n\n\nFiles compressed by any method other than \nbgzip\n (\ngzip\n, \nzip\n, or any other program)\n\n\n\n\nTo verify compatibility of your VCF you can try one of these methods:\n\n\n\n\nCompressing your VCF with\n   \nbgzip\n and indexing it with\n   \ntabix\n, both programs from\n   the \nHTSlib\n package (some systems also\n   use the earlier, pre-HTSlib \"tabix\" package). This process will\n   only succeed for compliant VCF files, and can help diagnose\n   failures.\n\n\nRunning \"vcf-validator\" program from the\n   \nvcftools\n package.\n\n\n\n\nWhile the VCF specification also requires that variants be sorted by\nchromosome name and position, PIE is now often able to automatically\ncorrect sorting issues in uploaded files. PIE requires sorted data in\norder to query data for targeted genes.\n\n\nQ: Are there limits on the size of VCF files?\n\n\nUploaded files must not exceed 4 gigabytes. If an uploaded file is larger\nthan 2 megabytes, the cancer predisposition gene list filter will be\nautomatically enabled unless you are using a custom gene list. This reduces\nthe processing burden on the system by removing variants outside of targeted\ngenes.\n\n\nQ: Is there an example/demo VCF I can try with PIE?\n\n\nA. You can use \nthis VCF\n\nfrom the Genome in a Bottle project. This ~133 megabyte\nbgzip-compressed VCF was used during testing of Pecan PIE and is known\nto work. These variants are mapped to GRCh37.\n\n\nQ. What genome versions are supported?\n\n\nA. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38.\nGRCh38 variants are automatically lifted over to 37, as the system\nuses 37 internally; the liftover process is able to compensate for\nstrand and reference/variant allele swaps which can occur. A native\nhg38 version is in development, but is not yet available.\n\n\nPecan PIE only works for human data.\n\n\nQ. What genes are on the curated gene list?\n\n\nA. The list consists of disease-related genes, both cancer and\nnon-cancer, see the \nfile_download\n Excel spreadsheet\n\nfor details. Filtering the source variants to a target list of genes\nreduces the processing burden on the system.\n\n\nWhen browsing the results the view may be filtered to disease\nsub-categories of interest.\n\n\nYou can also specify your own custom list of genes to process when\nsubmitting your VCF file (see the advanced options panel).\n\n\nQ. Why is the classification column blank in my results?\n\n\nQ. This column displays the classification assigned by the St. Jude\nGermline Committee reviewers. If a variant was not classified by this\ncommittee before, this field will be blank.\n\n\nPecan PIE provides classifications from the Medal Ceremony pipeline,\nwhich may assign variants gold, silver, or bronze medals. An \"Unknown\"\nmedal may be assigned for non-disease-predisposition genes, variants\npresent in the ExAC (ex-TCGA) database at an allele frequency \n\n0.1%, or variants without functional annotations (which includes most\nsilent variants).\n\n\nQ. What do the medals mean?\n\n\nA. The medal column is a rough indicator of the likelihood of the variant\nbeing clinically significant as predicted by the medal ceremony\nsoftware. Variants with gold medals are most likely to be significant,\nand those with no medal are least likely. More details can be found in\nthe \nAnalysis of Results \nresults\n\nsection.\n\n\nQ. Why are some of my variants missing?\n\n\nA. Currently only coding and splice-related variants in disease-related\ngenes make it to the medaling process. Intergenic, intronic, and UTR\nvariants are excluded, as are those in non-coding transcripts.\n\n\nQ. Why does the ExAC allele frequency shown differ from the ExAC portal?\n\n\nA. The reported ExAC frequency may differ for several reasons:\n\n\n\n\nPIE uses the TCGA-subtracted distribution of ExAC rather than the\n    main distribution.\n\n\nPIE reports the primary allele frequencies in the ExAC database,\n    specifically the AC, AN, and AF fields from the VCF distribution.\n    The \nExAC portal\n appears to use\n    the \"adjusted\" frequencies which may be different.\n\n\n\n\nQ. Is Pecan PIE free?\n\n\nA. Pecan PIE is free for non-commercial use. St. Jude covers the cost of\nrunning the pipeline and hosting. DNANexus accounts are required to\nkeep track of your jobs in the cloud so that you can retrieve and\nmanage from multiple locations. Accounts also make it possible to\nalert you of job completion via email.", 
            "title": "Pecan PIE"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#pecan-pie", 
            "text": "Authors  Michael Edmonson, Aman Patel    Publication  Zhang et al., NEJM 2015 , supplementary appendix pp. 7-10    Technical Support  Contact Us     Pecan PIE (the  Pe diatric  Can cer Variant  P athogenicity I nformation  E xchange) is a cloud-based variant classification\nand interpretation service. It annotates and ranks variants by putative\npathogenicity, then displays them in an interactive web interface for\nformal review and classification following ACMG guidelines. The portal\nalso contains a repository of expert-reviewed germline mutations that\nmay predispose individuals to cancer. It is free for non-commercial use.  Pecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that\npowers our clinical and research genomics projects. Medal Ceremony\nprovides a 3-level ranking of putative pathogenity - Gold, Silver or\nBronze - for mutations within disease-related genes. Medal assignment is\nbased on matches to 22 mutation databases, mutation type, population\nfrequency, tumor suppressor status and predicted functional impact. The\nevidence used for medal assignment is imported into an interactive\nvariant review page where an analyst can enter additional curated\ninformation such as primary diagnosis, presence of subsequent neoplasm,\nfamily history and related literature. Classification tags can be\nassigned to curated data enabling automated calculation of pathogenicity\nrating based on ACMG/AMP 2015 guidelines.  See  file_download  PowerPoint slides \npresented at the ASHG 2017 annual meeting (note that some of this\ninformation is out of date, various improvements have been made since\nthen).  Go to  https://pecan.stjude.cloud/pie  to get started!", 
            "title": "Pecan PIE"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#overview", 
            "text": "An overview of the Pecan PIE workflow:   Log in and upload a VCF of SNVs and indels.  The portal will process your variants, notifying you upon\n    completion. Variants are annotated with VEP+ (VEP with\n    postprocessing for enhanced splice variant calling) then classified\n    with Medal Ceremony.  Browse results, which include a detailed page for each variation.\n    Variants may be formally classified with an interface based on ACMG\n    guidelines.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#getting-started", 
            "text": "Start by logging into the portal with a DNAnexus account, creating an\naccount if you need one. PIE uses DNAnexus as a secure cloud backend.\nLogging in is required for private storage of your data and so that we\ncan send you e-mail notifications when your analysis jobs are complete.\nPIE is free for non-commercial use. St. Jude pays the (small) cloud\ncomputing costs, your DNAnexus account will not be billed.", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#uploading-data", 
            "text": "Pecan PIE takes standard VCF files as input, which may be either\nuncompressed or compressed with  bgzip .   Click the \"Securely upload a VCF file\" button.  Choose the genome your variants were mapped to, which may be either\n   GRCh37-lite or GRCh38.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#advanced-options", 
            "text": "The \"Advanced option\" panel lets you customize the behavior of the\npipeline:   Gene list: Pick a gene list from the pulldown. This filters your\n  variants to genes in the specified list. This option is required and turned\n  on automatically if your uploaded file is 2 megabytes or larger. See the\n   frequently asked questions  for more\n  information. This option reduces the variant processing burden on PIE by\n  removing variants that will not be assigned a medal in any case because\n  they are not on the cancer predisposition gene list. You can review the\n  genes by clicking on the link that will appear just below the pull down\n  titled \"See gene list\".  Custom gene list: Choosing \"custom\" as your gene list will open a\n  window that will let you paste in a list of genes. Any invalid genes will\n  be dropped from your list automatically. You can separate your genes by spaces\n  or new lines.  Max Population frequency: PIE by default will not call medals for\n  variants present in the ExAC (ex-TCGA) database at an allele\n  frequency greater than 0.001. This option lets you override the\n  filtering threshold to whatever frequency you prefer. To disable\n  filtering altogether, specify a value of 1.", 
            "title": "Advanced options"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#progress-page", 
            "text": "After uploading is complete you will be taken to a status screen showing\nthe progress of your job through the system. Analysis typically takes\n10-15 minutes depending on file size and system availability.  It isn't necessary to keep your browser open on this page until your\nresults are ready: the system will e-mail you with a link to return to\nyour results. Optional browser notifications are also available.", 
            "title": "Progress page"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#analysis-of-results", 
            "text": "", 
            "title": "Analysis of Results"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#results-browser", 
            "text": "When your job is complete you will be taken to an overview page where\nyou can browse your results and examine a detailed results page for each\nvariant.   The variants in the results can be filtered by:     Filter  Meaning      Class  Predicted effect of variant on protein coding, e.g. missense, nonsense, etc.    Somatic medal  Medal assigned to the variant by the somatic classifier.    Germline medal  Medal assigned to the variant by the germline classifier.    Committee Classification  If the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank.     The \"search\" box lets you filter the results by gene and/or amino acid\nchange. The view is dynamically filtered to matching variants as you\ntype.", 
            "title": "Results browser"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#medal-meaning", 
            "text": "Medals are only assigned for coding and splice-related variants in\ndisease predisposition genes. Germline medals are only assigned for\nnovel variants or those present in the ExAC (ex-TCGA) database with a\nMAF no greater than 0.1% (0.001 expressed fractionally).    Gold medals  are assigned to truncations in tumor suppressor genes,\nhotspots derived from the COSMIC database, as well as perfect matches to\nvariants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases.  Silver medals  are assigned to in-frame indels, truncations in non-tumor\nsuppressor genes, variants predicted deleterious by damage-prediction\nalgorithms, variants receiving a gold medal from the somatic classifier,\nand perfect matches to variants in the following databases: ClinVar\n(predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD.  Bronze medals  are assigned to variants predicted tolerated by\ndamage-prediction algorithms. Variants having an imperfect match to a\ndatabase (i.e. different variants at the same genomic position or codon)\ntypically receive a lesser medal.   A summary graphic can be found in slide 4 of the ASHG 2017 presentation ( download here ). \nFor additional details see  Zhang et al., NEJM\n2015 \n(supplementary appendix pp. 7-10).", 
            "title": "Medal meaning"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#variant-page", 
            "text": "Each variant links to a detailed variant page, which integrates data\nfrom a variety of sources. If either you or the St. Jude germline\nvariant review committee have annotated a variant, that information will\nbe pre-populated.  Summary information  The top of the page shows a summary of the variant, including its\ngenomic and HGVS annotations, predicted effect on the protein, and\nsomatic and germline medals. A description of the gene from Entrez\nfollows, and a custom description or selection rationale may also be\nentered.  Medal call information  Clicking on one of the medal icons (gold, silver, bronze, unknown) also\non the top of the page will show a summary of information related to the\nmedal call.  ProteinPaint  An embedded version of ProteinPaint ( Zhou et al., Nat. Genet.\n2016 ) appears next, showing\nthe variant in the context of a number of pediatric datasets including\nPCGP. A link is provided to the main ProteinPaint application which\nprovides visualizations for additional datasets, including COSMIC and\nClinVar.  ASHG pathogenicity classification  Formal variant pathogenicity classification is supported by an interface\nimplementing ACMG guidelines ( Richards et al., Genet Med.\n2015 ).\nThe analyst reviews a series of curated category tags, assigning\napplicable tags to the variant and optionally supplying additional\ninformation for each such as PubMed IDs and supporting evidence. The\nsystem will then compute an appropriate pathogenicity score based on the\nuser-flagged categories. Additional free-form custom evidence can also\nbe entered. This structured approach both helps eliminate arbitrary\ndecision-making from the pathogenicity classification process and also\nconstructs a concise summary of the logic and evidence supporting the\nfinal call.  ClinVar and allele frequency  Matches of the variant in ClinVar are also provided, along with\npredicted clinical significance and review status.  Allele frequencies for the variant in the PCGP (somatic and\ngermline),NHLBI ESP 6500, and ExAC databases are presented both as\nfractional values and on a log10 plot. Detailed allele population\nbreakdowns are provided for ExAC.  Damage prediction algorithms  Precomputed damage-prediction algorithm calls for nonsynonymous coding\nSNVs are presented from the dbNSFP database. Available algorithms are\nPolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT.\nThe calls are presented in a circular diagram with entries color-coded\nbased on the predicted severity of the result.  Medal ceremony and linkouts  Additional output from medal ceremony classification can also be\nreviewed. This is only loosely structured, additional fields here may\neventually be integrated into Pecan PIE.  Links are provided to relevant dbSNP entries and other information\nsources.  Final classification  The final 5-tier ACMG classification can be selected after which the\ndecision will be marked as reviewed. A checkbox is also available to\nindicate this variant is a potential candidate for functional review.", 
            "title": "Variant page"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#standalone-usage", 
            "text": "This section is intended only for users who want to invoke Pecan PIE's\nunderlying analysis pipelines independently on the DNAnexus  platform. If you just want to use\nthe Pecan PIE website you can safely ignore this section of the\ndocumentation. We assume familiarity with the DNAnexus platform. If you aren't\nfamiliar with this, DNAnexus'  quickstart guide  is a\ngreat place to start.   Warning  This section of the guide is only relevant to power users!   Two DNAnexus cloud application pipelines were created during the\ndevelopment of Pecan PIE:     Name  Corresponding DNAnexus App  Description      VEP+  app-stjude_vep_plus  A cloud installation of  VEP  with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format.    Medal Ceremony  app-stjude_medal_ceremony  Additional annotation and automated variant classification. Requires a special input format which is produced by VEP+.", 
            "title": "Standalone usage"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#permissions", 
            "text": "In order to run the cloud pipelines independently, your DNAnexus account\nneeds to be granted permissions to access them. After your initial login\nto St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single\nlogin is required even if you just want to use the standalone pipelines\nrather than the Pecan PIE portal ( contact\nus  if you encounter problems accessing\nthe pipelines).  There are two methods of running pipelines on DNAnexus:   DNAnexus GUI.  DNAnexus provides a standardized graphical user interface for\n    configurating, launching, and monitoring jobs on the cloud. Our\n    pipelines can be run like any other DNAnexus pipeline.  Command line.  Jobs may also be invoked via the  dx  command line\n    client. Command-line use allows submitting cloud jobs without\n    interacting with a GUI, and so supports scripting and easier\n    integration with local workflows. See  this section \n    for information on how to get set up with the  dx-toolkit .    Note  The following examples demonstrate command-line usage.", 
            "title": "Permissions"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#uploading-files", 
            "text": "All input files must be uploaded onto the DNAnexus platform. When\nspecifying files for input you can use either the DNAnexus fie IDs (e.g. file-FBgvp680gz1bGQ5p8yZKz69g ), or the filenames if they are unique. For\nan idea of how to upload files to DNAnexus, see  this guide .", 
            "title": "Uploading files"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#step-1-running-vep", 
            "text": "To run the VEP+ DNAnexus app, you can use the following  dx \ncommand with your own inputs in place of the example's:  dx run app-stjude_vep_plus -iinput_file = my_vcf.vcf -igenome_string = GRCh37-lite -igermline_reviewable_only = true    Tip   genome_string  must be either  GRCh37-lite  or  GRCh38 . If  GRCh38 \nis specified, variants will be lifted over to  GRCh37-lite  in output,\ni.e. the output will always be  GRCh37-lite  (Medal Ceremony currently only supports  GRCh37-lite ).  The input VCF specified by  input_file  may be either\nuncompressed, or compressed with  bgzip   only  (htslib/tabix\npackages).  The  germline_reviewable_only  parameter is optional, but\nstrongly recommended. If specified, only variants in disease-gene\nrelated intervals will be annotated, which is appropriate for\nMedal Ceremony. If this option is not specified all variants will\nbe annotated, which depending on the size of your VCF might take a\nlot longer, and many of the resulting variants won't be usable by\nMedal Ceremony. If you want to do this anyway and have a large\nnumber of variants, consider submitting your job to an instance\nwith more CPU cores (e.g.  mem1_ssd1_x16  or  mem1_ssd1_x32 ) as\nthe code will take advantage of the additional cores. If you are\nusing a custom gene list (below) that takes precedence and this\nparameter is not needed.  The optional parameter  custom_genes_file  specifies a plain\ntext file of HUGO gene symbols to analyze (whitespace separated,\nor one per line). If specified, analysis will be restricted to\nthese genes only.  This pipeline produces two output files,  output_file  contains\nannotations for all variants, while  medal_prep_output_file  is\nthe specially-filtered and formatted file required as input to\nMedal Ceremony below.", 
            "title": "Step 1: Running VEP+"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#step-2-running-medal-ceremony", 
            "text": "To run the medal ceremony DNAnexus app, you can use the following  dx \ncommand with your own inputs in place of the example's:  dx run app-stjude_medal_ceremony -iinfile = medal_prep_output_file   Tip   The optional parameter  custom_genes_file  operates in the same\nway as in the VEP+ pipeline above. For custom gene lists to work\nproperly this parameter must be specified when running both the\nVEP+ and Medal Ceremony pipelines.  The optional parameter  max_population_frequency  may be\nspecified, a fractional value representing the maximum population\nfrequency allowed for a variant in the ExAC (ex-TCGA) database to\nreceive a medal. The default is 0.001, a.k.a. \".1%\".", 
            "title": "Step 2: Running Medal Ceremony"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#frequently-asked-questions", 
            "text": "If you have any questions not covered here, feel free to reach out on our contact form .  Q: Which files are supported?  PIE works with variants in VCF format:   Uploaded files must be compliant with the  VCF specification .  VCF files may be either uncompressed, or compressed with  bgzip   only .  bgzip  is part of the htslib/tabix packages (see below).   Improperly formatted VCF files will not work with PIE. Some common\nproblems include:   Missing header line  Missing required columns  Files compressed by any method other than  bgzip  ( gzip ,  zip , or any other program)   To verify compatibility of your VCF you can try one of these methods:   Compressing your VCF with\n    bgzip  and indexing it with\n    tabix , both programs from\n   the  HTSlib  package (some systems also\n   use the earlier, pre-HTSlib \"tabix\" package). This process will\n   only succeed for compliant VCF files, and can help diagnose\n   failures.  Running \"vcf-validator\" program from the\n    vcftools  package.   While the VCF specification also requires that variants be sorted by\nchromosome name and position, PIE is now often able to automatically\ncorrect sorting issues in uploaded files. PIE requires sorted data in\norder to query data for targeted genes.  Q: Are there limits on the size of VCF files?  Uploaded files must not exceed 4 gigabytes. If an uploaded file is larger\nthan 2 megabytes, the cancer predisposition gene list filter will be\nautomatically enabled unless you are using a custom gene list. This reduces\nthe processing burden on the system by removing variants outside of targeted\ngenes.  Q: Is there an example/demo VCF I can try with PIE?  A. You can use  this VCF \nfrom the Genome in a Bottle project. This ~133 megabyte\nbgzip-compressed VCF was used during testing of Pecan PIE and is known\nto work. These variants are mapped to GRCh37.  Q. What genome versions are supported?  A. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38.\nGRCh38 variants are automatically lifted over to 37, as the system\nuses 37 internally; the liftover process is able to compensate for\nstrand and reference/variant allele swaps which can occur. A native\nhg38 version is in development, but is not yet available.  Pecan PIE only works for human data.  Q. What genes are on the curated gene list?  A. The list consists of disease-related genes, both cancer and\nnon-cancer, see the  file_download  Excel spreadsheet \nfor details. Filtering the source variants to a target list of genes\nreduces the processing burden on the system.  When browsing the results the view may be filtered to disease\nsub-categories of interest.  You can also specify your own custom list of genes to process when\nsubmitting your VCF file (see the advanced options panel).  Q. Why is the classification column blank in my results?  Q. This column displays the classification assigned by the St. Jude\nGermline Committee reviewers. If a variant was not classified by this\ncommittee before, this field will be blank.  Pecan PIE provides classifications from the Medal Ceremony pipeline,\nwhich may assign variants gold, silver, or bronze medals. An \"Unknown\"\nmedal may be assigned for non-disease-predisposition genes, variants\npresent in the ExAC (ex-TCGA) database at an allele frequency  \n0.1%, or variants without functional annotations (which includes most\nsilent variants).  Q. What do the medals mean?  A. The medal column is a rough indicator of the likelihood of the variant\nbeing clinically significant as predicted by the medal ceremony\nsoftware. Variants with gold medals are most likely to be significant,\nand those with no medal are least likely. More details can be found in\nthe  Analysis of Results  results \nsection.  Q. Why are some of my variants missing?  A. Currently only coding and splice-related variants in disease-related\ngenes make it to the medaling process. Intergenic, intronic, and UTR\nvariants are excluded, as are those in non-coding transcripts.  Q. Why does the ExAC allele frequency shown differ from the ExAC portal?  A. The reported ExAC frequency may differ for several reasons:   PIE uses the TCGA-subtracted distribution of ExAC rather than the\n    main distribution.  PIE reports the primary allele frequencies in the ExAC database,\n    specifically the AC, AN, and AF fields from the VCF distribution.\n    The  ExAC portal  appears to use\n    the \"adjusted\" frequencies which may be different.   Q. Is Pecan PIE free?  A. Pecan PIE is free for non-commercial use. St. Jude covers the cost of\nrunning the pipeline and hosting. DNANexus accounts are required to\nkeep track of your jobs in the cloud so that you can retrieve and\nmanage from multiple locations. Accounts also make it possible to\nalert you of job completion via email.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/neoepitope/", 
            "text": "Authors\n\n\nTi-Cheng Chang\n\n\n\n\n\n\nPublication\n\n\nThe Neoepitope Landscape in Pediatric Cancers. Genome Medicine. 2017. 9.1: 78\n.\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nCancers are caused by somatically acquired alterations including single \nnucleotide variations (SNVs), small insertion/deletions (indels),\ntranslocations, and other types of rearrangements. The genes affected by\nthese mutations may produce altered proteins, some of which may lead to\nthe emergence of tumor-specific immunogenic epitopes. We developed an\nanalytical workflow for identification of putative neoepitopes based on\nsomatic missense mutations and gene fusions using whole genome\nsequencing data. The workflow has been used to characterize neoepitope\nlandscape of 23 subtypes of pediatric cancer in the Pediatric Cancer\nGenome Project\n1\n.\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n if using FastQ inputs)\n\n\nInput file\n\n\nGzipped FastQ files generated by experiment.\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\nBAM files (\nrequired\n if using BAM inputs)\n\n\nInput file\n\n\nBAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq).\n\n\nSample.bam\n\n\n\n\n\n\nBAM indices (\nrequired\n if using BAM inputs)\n\n\nInput file\n\n\nCorresponding BAM index of the BAM files above.\n\n\nSample.bam.bai\n\n\n\n\n\n\nMutation file (\nrequired\n)\n\n\nInput file\n\n\nFile describing the mutations present in the sample (special format, see below).\n\n\n*.txt (tab-delimited)\n\n\n\n\n\n\nSNV or fusion\n\n\nParameter\n\n\nSpecify the mutation file contains SNV or gene fusion.\n\n\nSNV\n\n\n\n\n\n\nPeptide size\n\n\nParameter\n\n\nSize of the peptide.\n\n\n9\n\n\n\n\n\n\nAffinity threshold\n\n\nParameter\n\n\nAffinity cutoff for epitope prediction report.\n\n\n500\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEpitope affinity prediction (html)\n\n\nEpitope affinity. The peptide with affinity \n cutoff will be highlighted.\n\n\n\n\n\n\nEpitope affinity prediction (xlsx)\n\n\nExcel tables for the infomation of all epitopes\n\n\n\n\n\n\nAffinity (raw output)\n\n\nEpitope affinity\n\n\n\n\n\n\nPeptide sequence (raw output)\n\n\nPeptide sequences in Fasta format\n\n\n\n\n\n\n\n\nHLA Typing Algorithm\n\n\nThe HLA typing algorithm is used to predict the HLA class I alleles.\nUsers can either provide FastQ (paired or single end reads) or a BAM\nfile as input. When using a BAM file as input, the reads surrounding the\nHLA loci and unmapped reads will be extracted. The reads will be fed\ninto Optitype for HLA typing. The default settings for Optitype are\nused. The output of the HLA type can be combined with the our epitope\ndetection algorithm to perform affinity prediction of neoepitopes.\n\n\nIf you use \nFastQ\n files as input:  \n\n\n\n\nThe input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see \nhttps://github.com/FRED-2/OptiType\n).\n\n\nThe fished FastQs will be used for HLA typing using Opitype.\n\n\n\n\nIf you use \nBAM\n files as input:  \n\n\n\n\nThe reads falling within the HLA loci and their paralogous loci will\n    be extracted.\n\n\nThe reads unmapped to the human genome will be extracted.\n\n\nThe reads from step 1 and 2 will be combined and deduplicated (in\n    FastQ format).\n\n\nThe input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see \nhttps://github.com/FRED-2/OptiType\n).\n\n\nThe fished FastQs will be used for HLA typing using Opitype.\n\n\n\n\nEpitope Prediction Algorithm\n\n\nThe epitope prediction algorithm first extracts peptides covering an array\nof tiling peptides (size defined by users) overlapping each missense\nmutation or gene fusion. Fusion junctions can be identified using RNA-Seq\nby fusion detection tools (Li et. al, unpublished). NetMHCcons\n3\n is subsequently \nused to predict affinities of the peptide array for each HLA receptor in \neach sample. The neoepitope with affinity lower than the threshold will \nbe highlighted in output file (default 500 nM).\n\n\n\n\nCheck the version of the genomic position of the input SNV/fusion\n    file.\n\n\nLift over the genomic coordinations if the reference genomic\n    position is not HG19. Currently, the internal genome annotation was\n    based on HG19 and the genome coordinates of the mutation files will\n    be adjusted to HG19 for peptide extraction.\n\n\nExtract the peptide flanking the mutations.\n\n\nRun NetMHCcons to obtain the affinity prediction of the peptides.\n\n\nProduce the affinity report of each peptide.\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nNeoepitopePred tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nInput file configuration\n\n\nUsers need to provide a mutation file for SNV or gene fusion. The format\nof the mutation file is shown in the following example. The file can be\nprepared in Excel and saved as a tab-delimited text file to use as\ninput.\n\n\nThe HLA alleles for testing will be derived from the HLA typing module\nusing the workflow. The peptide size and affinity cutoff can be modified\nby users. \n\n\nMutation file format\n\n\n\n\n\n\n\n\nGeneName\n\n\nSample\n\n\nChr\n\n\nPostion_hg19\n\n\nClass\n\n\nAAChange\n\n\nmRNA_acc\n\n\nReferenceAllele\n\n\nMutantAllele\n\n\n\n\n\n\n\n\n\n\nGene1\n\n\nSampleA\n\n\nchr10\n\n\n106150600\n\n\nmissense\n\n\nR663H\n\n\nNM_00101\n\n\nA\n\n\nT\n\n\n\n\n\n\nGene2\n\n\nSampleA\n\n\nchr2\n\n\n32330151\n\n\nmissense\n\n\nN329N\n\n\nNM_00102\n\n\nT\n\n\nG\n\n\n\n\n\n\n\n\n\n\nNotes on preparing the above file\n\n\n\n\nThe chromosome requires a 'chr' prefix.\n\n\nThe position requires a suffix of HG19/HG38 to indicate the human genome assembly version.\n\n\nOnly the missense mutations/gene fusion is supported currently and\n    the other types of mutations will not be processed.\n\n\n\n\n\n\nMutation file example\n\n\n\n\nUploading data\n\n\nNeoepitopePred takes the following files as input:\n\n\n\n\nA pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be\n  generated from whole genome sequencing, whole exome sequencing, or RNA-Seq. \n\n\nA file describing the mutations in a sample. \n\n\n\n\nYou can upload these files using the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\n\n\nCaution\n\n\nThis pipeline assumes HG19 coordinates in the mutation file. If the\ncoordinates are based on HG38, the coordinates will lifted over to HG19\nto perform epitope affinity prediction.\n\n\n\n\nOnce you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the \ntool's landing page\n. \nA dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.\n\n\n\n\nSelecting parameters\n\n\nThere are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Neoepitope Prediction\" substep. For a full list of the parameters and their\ndescriptions, see \nthe input section\n (specifically, you are \nlooking at the items in the table labeled \"parameters\").\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ/BAM and mutation files you uploaded in \n\nthe upload data section\n. You can do this by \nclicking on the \nBAM alignment file\n and \nBAM index file\n and \nMutation array\n slots and\nselecting the respective files.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more NeoepitopePred runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n \n\n\nInterpreting results\n\n\nHLA typing\n\n\nThe output of this app contain the prediction of the HLA class I alleles\nfrom OptiType.\n\n\n\n\nA folder stamped with the time will present in the output folder\n  (optitype), which contains the raw output.\n\n\n\n\n\n\n\n\nThe file contains the predicted HLA alleles of the sample.\n\n\n\n\n\n\nNeoepitope prediction\n\n\n\n\nTodo\n\n\nCleanup the formatting of this section.\n\n\n\n\nThe output contains one summary HTML, one folder with raw output and one\nfolder with outputs in Excel formats:\n\n\n\n\nEpitope_affinity_prediction.html:\n\n* This file provides a summary of the epitope prediction that can be\n  visualized directly from web browser.\n* The peptides with affinity lower than user-defined cutoff will be\n  highlighted in green in the webpage.\n\n\n\n\nRaw_output: this folder contains the raw output of the affinity\nprediction.\n\n\n\n\nThere will two major types files present here:\n\n\n\n\naffinity.out: these files are the prediction results from the\n    netMHCcons for each peptide.\n\n\n\n\nThe following columns will be shown in the output:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGene name\n\n\nthe name of the genes\n\n\n\n\n\n\nSample\n\n\nthe name of the samples\n\n\n\n\n\n\nChromosome (chr)\n\n\nthe chromosome location of the variation\n\n\n\n\n\n\nPosition\n\n\nthe chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19.\n\n\n\n\n\n\nClass\n\n\nclass of the varitaion\n\n\n\n\n\n\nReference allele\n\n\nreference allele at the position\n\n\n\n\n\n\nMutant allele\n\n\nmutated allele at the position\n\n\n\n\n\n\nmRNA_acc\n\n\nNCBI accession number of the mRNA\n\n\n\n\n\n\nAllele\n\n\nHLA allele tested\n\n\n\n\n\n\nPeptide\n\n\nthe neoepitope sequences tested\n\n\n\n\n\n\nGene_variant\n\n\nthe gene and variant residues\n\n\n\n\n\n\n1-log50k\n\n\nPrediction score from netMHCcons\n\n\n\n\n\n\nnM\n\n\nAffinity as IC50 values in nM\n\n\n\n\n\n\n%Rank\n\n\n% Rank of prediction score to a set of 200.000 random natural 9mer peptides\n\n\n\n\n\n\nHLAtype\n\n\nAll of the hla alleles predicted in the specific sample\n\n\n\n\n\n\n\n\nflanking.seq: these files contain the sequences used for the prediction.\n\n\n\n\nXLSX: this folder contains the raw output of the affinity prediction as\ndescribed above in Excel files. The files can be downloaded and opened\nwith Excel for downstream filtering and analyses\n\n\n\n\nFrequently asked questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact form\n.\n\n\n\n\n\n\n\n\n\n\nDowning JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome\nProject. Nature genetics. 2012;44(6):619-622.\n\n\n\n\n\n\nSzolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O:\nOptiType: precision HLA typing from next-generation sequencing\ndata. Bioinformatics 2014, 30:3310-3316.\n\n\n\n\n\n\nKarosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a\nconsensus method for the major histocompatibility complex class I\npredictions. Immunogenetics 2012, 64:177-186.", 
            "title": "NeoepitopePred"
        }, 
        {
            "location": "/guides/tools/neoepitope/#overview", 
            "text": "Inputs     Name  Type  Description  Example      FastQ files ( required  if using FastQ inputs)  Input file  Gzipped FastQ files generated by experiment.  Sample_R1.fastq.gz and Sample_R2.fastq.gz    BAM files ( required  if using BAM inputs)  Input file  BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq).  Sample.bam    BAM indices ( required  if using BAM inputs)  Input file  Corresponding BAM index of the BAM files above.  Sample.bam.bai    Mutation file ( required )  Input file  File describing the mutations present in the sample (special format, see below).  *.txt (tab-delimited)    SNV or fusion  Parameter  Specify the mutation file contains SNV or gene fusion.  SNV    Peptide size  Parameter  Size of the peptide.  9    Affinity threshold  Parameter  Affinity cutoff for epitope prediction report.  500     Outputs     Name  Description      Epitope affinity prediction (html)  Epitope affinity. The peptide with affinity   cutoff will be highlighted.    Epitope affinity prediction (xlsx)  Excel tables for the infomation of all epitopes    Affinity (raw output)  Epitope affinity    Peptide sequence (raw output)  Peptide sequences in Fasta format     HLA Typing Algorithm  The HLA typing algorithm is used to predict the HLA class I alleles.\nUsers can either provide FastQ (paired or single end reads) or a BAM\nfile as input. When using a BAM file as input, the reads surrounding the\nHLA loci and unmapped reads will be extracted. The reads will be fed\ninto Optitype for HLA typing. The default settings for Optitype are\nused. The output of the HLA type can be combined with the our epitope\ndetection algorithm to perform affinity prediction of neoepitopes.  If you use  FastQ  files as input:     The input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see  https://github.com/FRED-2/OptiType ).  The fished FastQs will be used for HLA typing using Opitype.   If you use  BAM  files as input:     The reads falling within the HLA loci and their paralogous loci will\n    be extracted.  The reads unmapped to the human genome will be extracted.  The reads from step 1 and 2 will be combined and deduplicated (in\n    FastQ format).  The input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see  https://github.com/FRED-2/OptiType ).  The fished FastQs will be used for HLA typing using Opitype.   Epitope Prediction Algorithm  The epitope prediction algorithm first extracts peptides covering an array\nof tiling peptides (size defined by users) overlapping each missense\nmutation or gene fusion. Fusion junctions can be identified using RNA-Seq\nby fusion detection tools (Li et. al, unpublished). NetMHCcons 3  is subsequently \nused to predict affinities of the peptide array for each HLA receptor in \neach sample. The neoepitope with affinity lower than the threshold will \nbe highlighted in output file (default 500 nM).   Check the version of the genomic position of the input SNV/fusion\n    file.  Lift over the genomic coordinations if the reference genomic\n    position is not HG19. Currently, the internal genome annotation was\n    based on HG19 and the genome coordinates of the mutation files will\n    be adjusted to HG19 for peptide extraction.  Extract the peptide flanking the mutations.  Run NetMHCcons to obtain the affinity prediction of the peptides.  Produce the affinity report of each peptide.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/neoepitope/#getting-started", 
            "text": "To get started, you need to navigate to the  NeoepitopePred tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/neoepitope/#input-file-configuration", 
            "text": "Users need to provide a mutation file for SNV or gene fusion. The format\nof the mutation file is shown in the following example. The file can be\nprepared in Excel and saved as a tab-delimited text file to use as\ninput.  The HLA alleles for testing will be derived from the HLA typing module\nusing the workflow. The peptide size and affinity cutoff can be modified\nby users.   Mutation file format     GeneName  Sample  Chr  Postion_hg19  Class  AAChange  mRNA_acc  ReferenceAllele  MutantAllele      Gene1  SampleA  chr10  106150600  missense  R663H  NM_00101  A  T    Gene2  SampleA  chr2  32330151  missense  N329N  NM_00102  T  G      Notes on preparing the above file   The chromosome requires a 'chr' prefix.  The position requires a suffix of HG19/HG38 to indicate the human genome assembly version.  Only the missense mutations/gene fusion is supported currently and\n    the other types of mutations will not be processed.    Mutation file example", 
            "title": "Input file configuration"
        }, 
        {
            "location": "/guides/tools/neoepitope/#uploading-data", 
            "text": "NeoepitopePred takes the following files as input:   A pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be\n  generated from whole genome sequencing, whole exome sequencing, or RNA-Seq.   A file describing the mutations in a sample.    You can upload these files using the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/neoepitope/#running-the-tool", 
            "text": "Caution  This pipeline assumes HG19 coordinates in the mutation file. If the\ncoordinates are based on HG38, the coordinates will lifted over to HG19\nto perform epitope affinity prediction.   Once you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the  tool's landing page . \nA dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/neoepitope/#selecting-parameters", 
            "text": "There are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Neoepitope Prediction\" substep. For a full list of the parameters and their\ndescriptions, see  the input section  (specifically, you are \nlooking at the items in the table labeled \"parameters\").", 
            "title": "Selecting parameters"
        }, 
        {
            "location": "/guides/tools/neoepitope/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ/BAM and mutation files you uploaded in  the upload data section . You can do this by \nclicking on the  BAM alignment file  and  BAM index file  and  Mutation array  slots and\nselecting the respective files.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/neoepitope/#starting-the-workflow", 
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/neoepitope/#monitoring-run-progress", 
            "text": "Once you have started one or more NeoepitopePred runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.      Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/neoepitope/#analysis-of-results", 
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/neoepitope/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/neoepitope/#interpreting-results", 
            "text": "HLA typing  The output of this app contain the prediction of the HLA class I alleles\nfrom OptiType.   A folder stamped with the time will present in the output folder\n  (optitype), which contains the raw output.     The file contains the predicted HLA alleles of the sample.    Neoepitope prediction   Todo  Cleanup the formatting of this section.   The output contains one summary HTML, one folder with raw output and one\nfolder with outputs in Excel formats:   Epitope_affinity_prediction.html: \n* This file provides a summary of the epitope prediction that can be\n  visualized directly from web browser.\n* The peptides with affinity lower than user-defined cutoff will be\n  highlighted in green in the webpage.   Raw_output: this folder contains the raw output of the affinity\nprediction.   There will two major types files present here:   affinity.out: these files are the prediction results from the\n    netMHCcons for each peptide.   The following columns will be shown in the output:     Column  Description      Gene name  the name of the genes    Sample  the name of the samples    Chromosome (chr)  the chromosome location of the variation    Position  the chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19.    Class  class of the varitaion    Reference allele  reference allele at the position    Mutant allele  mutated allele at the position    mRNA_acc  NCBI accession number of the mRNA    Allele  HLA allele tested    Peptide  the neoepitope sequences tested    Gene_variant  the gene and variant residues    1-log50k  Prediction score from netMHCcons    nM  Affinity as IC50 values in nM    %Rank  % Rank of prediction score to a set of 200.000 random natural 9mer peptides    HLAtype  All of the hla alleles predicted in the specific sample     flanking.seq: these files contain the sequences used for the prediction.   XLSX: this folder contains the raw output of the affinity prediction as\ndescribed above in Excel files. The files can be downloaded and opened\nwith Excel for downstream filtering and analyses", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/neoepitope/#frequently-asked-questions", 
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact form .      Downing JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome\nProject. Nature genetics. 2012;44(6):619-622.    Szolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O:\nOptiType: precision HLA typing from next-generation sequencing\ndata. Bioinformatics 2014, 30:3310-3316.    Karosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a\nconsensus method for the major histocompatibility complex class I\npredictions. Immunogenetics 2012, 64:177-186.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/chipseq/", 
            "text": "Authors\n\n\nXing Tang, Yong Cheng\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nThe ChIP-Seq Peak Calling workflow follows ENCODE best practices to call \nbroad or narrow peaks on Illumina-generated ChIP-Seq data. \nHere, a Gzipped FastQ file from an Immunoprecipitation (IP) experiment \nis considered the \"case sample file\" and a Gzipped FastQ file from a control \nexperiment is considered the \"control sample file\". The pipeline can run on\nmatched case/control samples (recommended for better results) or just a \ncase sample.\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n if using FastQ inputs)\n\n\nInput file\n\n\nGzipped FastQ files generated by experiment.\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nFormat\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBED file\n\n\n.bed\n\n\nPeak calls\n\n\n\n\n\n\nBinary file\n\n\n.bb\n\n\nBinary format for BED file\n\n\n\n\n\n\nBigWig file\n\n\n.bw\n\n\nShows read coverage\n\n\n\n\n\n\nMetrics file\n\n\n.txt\n\n\nShows mapping and duplication rate\n\n\n\n\n\n\nCross correlation plot\n\n\n.pdf\n\n\nQuality plot showing if the forward and reverse reads tend to be centered around binding sites.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nThe reads of the FastQ file(s) are aligned to the specified reference genome. \n\n\nThe aligned reads are then post-processed based on best-practice QC techniques\n(removing multiple mapped reads, removing duplicated reads, etc). \n\n\nPeaks are called by SICER (broad peak analysis) or MACS2 (narrow peak\nanalysis). \n\n\nQualified peaks will be output as BED (.bed) and big BED (.bb)\nfiles. \n\n\nThe coverage information will be output as a bigWig (.bw)\nfile. \n\n\nA cross correlation plot and general metrics file are generated to help check\nthe quality of experiment.\n\n\n\n\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nChIP-Seq tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an\nIP experiment as input. You can upload your input FastQ files by\nusing the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\nOnce you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the \ntool's landing page\n. \nA dropdown will present the different presets for running the ChIP-Seq workflow.\nYou'll need to decide \n(1)\n whether you'd like to run broad/narrow peak\ncalling and \n(2)\n whether you have a case sample and a control sample (preferred)\nor just a case sample. This will determine which preset you should\nclick in this dropdown. There are various other parameters that you can \nset, but they are covered in further sections of this guide.\n\n\n\n\nBroad vs. narrow peak calling\n\n\nChoosing between broad and narrow peak calling depends on the experiment\ndesign. The following are good rules of thumb for choosing between the\ntwo configurations. If you are not sure which configuration to use,\nplease consult with an expert at your institution or \ncontact us\n.\n\n\nNarrow Peak Calling\n\n\nIf your target protein is a transcription factor, you should probably\nchoose narrow peak calling. You can also try the narrow peak calling\nworkflows for the following histone marks:\n\n\n\n\nH3K4me3\n\n\nH3K4me2\n\n\nH3K9-14ac\n\n\nH3K27ac\n\n\nH2A.Z\n\n\n\n\nBroad Peak Calling\n\n\nYou should try the broad peak calling workflows for the following\nhistone marks:\n\n\n\n\nH3K36me3\n\n\nH3K79me2\n\n\nH3K27me3\n\n\nH3K9me3\n\n\nH3K9me1\n\n\n\n\nSpecial Cases\n\n\nIn some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between\nnarrow and broad shape, you might need to look into each peak region and\nconsult experts.\n\n\n\n\nWarning\n\n\nIf your fragment size is less than 50 base pairs, please refer to the\n\nfrequently asked questions\n.\n\n\n\n\nSelecting parameters\n\n\nThere are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Parameter Wrapper\" substep.\n\n\n\n\nThe following are the parameters that can be set, a short\ndescription of each parameter, and an example value. If you\nhave questions, please \ncontact us\n.\n\n\n\n\n\n\n\n\nParameter Name\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nOutput prefix (\nrequired\n)\n\n\nA name used a prefix for all outputs in the run\n\n\nSAMPLE1\n\n\n\n\n\n\nReference genome (\nrequired\n)\n\n\nSupported reference genome from one of hg19, GRCh38, mm9, mm10, dm3\n\n\nGRCh38\n\n\n\n\n\n\nOutput bigWig\n\n\nWhether or not to include a bigwig file in the output\n\n\nTrue\n\n\n\n\n\n\nRemove blacklist peaks\n\n\nWhether or not to remove known problem areas\n\n\nTrue\n\n\n\n\n\n\nFragment length\n\n\nHardcoded fragment length of your reads. 'NA' for auto-detect.\n\n\nNA\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nPlease be aware of the following stumbling points when setting parameters:\n\n\n\n\nDo not use spaces anywhere in your input file names, your output\n  prefix, or any of the other parameters. This is generally bad\n  practice and doesn't play well with the pipeline (consider using\n  \"_\" instead).\n\n\nDo not change the output directory when you run the pipeline. At\n  the top of parameter input page, there is a text box that allows\n  you to change the output folder. \nPlease ignore that setting\n. You\n  only need to specify an output prefix as described above. All of\n  the results will be put under \n/Results/[OUTPUT_PREFIX]\n.\n\n\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files you uploaded in \n\nthe upload data section\n. You can do this by \nclicking on the \nChIP Reads\n and \nControl Reads\n slots and\nselecting the respective files. If you are not doing a case/control\nrun, you only need to hook up the case sample.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that:\n\n\n\n\nall of the inputs are correctly hooked up (see \nhooking up inputs\n), and \n\n\nall of the required parameters are set (see \nsetting parameters\n).\n\n\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more ChIP-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand the job logs can be accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nInteractive visualizations\n\n\nToday, the ChIP-Seq pipeline does not produce an interactive visualization. We are\nworking on adding this! In the meantime, you can view the cross-correlation plot(s)\nas outlined in the sections below.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view of your cloud workspace. This is similar to the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files. To access ChIP-Seq results, you should click on the \n\nResults\n folder, then select the output folder name you gave in the \nselecting parameters\n part of the guide.\n\n\n\n\nInterpreting results\n\n\nFor the ChIP-Seq pipeline, every pipeline run outputs a \nREADME.doc\n file\nwhich contains the latest information on which results are included.\nYou can refer to that file for the most up to date information on raw outputs.\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to \n\ncontact us\n.\n\n\nQ: Should I choose narrow peak calling pipeline or broad peak calling pipeline?\n\n\nA. We built two workflows: one for narrow peak calling and another broad\npeak calling. If your target protein is a transcription factor, please\nuse narrow peak calling workflow. For histone marks H3K4me3, H3K4me2,\nH3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling\nworkflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and\nH3K9me1, you could try broad peak calling workflow. In some scenario,\nH3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad\nshape, you might need to look into each peak region and consult\nexperts.\n\n\nQ. What to do if your fragment size is less than 50 base pairs?\n\n\nA. We estimate fragment size from the data based on the cross correlation\nplot. Usually the fragment size is above 50bp. If the estimated\nfragment size lower than 50bp, the workflow will stop at the peak\ncalling stage (MACS2/SICER) after BWA mapping finishes. You can rerun\nthe analysis with a specified fragment length.", 
            "title": "ChIP-Seq Peak Calling"
        }, 
        {
            "location": "/guides/tools/chipseq/#overview", 
            "text": "Inputs     Name  Type  Description  Example      FastQ files ( required  if using FastQ inputs)  Input file  Gzipped FastQ files generated by experiment.  Sample_R1.fastq.gz and Sample_R2.fastq.gz     Outputs     Name  Format  Description      BED file  .bed  Peak calls    Binary file  .bb  Binary format for BED file    BigWig file  .bw  Shows read coverage    Metrics file  .txt  Shows mapping and duplication rate    Cross correlation plot  .pdf  Quality plot showing if the forward and reverse reads tend to be centered around binding sites.     Process   The reads of the FastQ file(s) are aligned to the specified reference genome.   The aligned reads are then post-processed based on best-practice QC techniques\n(removing multiple mapped reads, removing duplicated reads, etc).   Peaks are called by SICER (broad peak analysis) or MACS2 (narrow peak\nanalysis).   Qualified peaks will be output as BED (.bed) and big BED (.bb)\nfiles.   The coverage information will be output as a bigWig (.bw)\nfile.   A cross correlation plot and general metrics file are generated to help check\nthe quality of experiment.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/chipseq/#getting-started", 
            "text": "To get started, you need to navigate to the  ChIP-Seq tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/chipseq/#uploading-data", 
            "text": "The ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an\nIP experiment as input. You can upload your input FastQ files by\nusing the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/chipseq/#running-the-tool", 
            "text": "Once you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the  tool's landing page . \nA dropdown will present the different presets for running the ChIP-Seq workflow.\nYou'll need to decide  (1)  whether you'd like to run broad/narrow peak\ncalling and  (2)  whether you have a case sample and a control sample (preferred)\nor just a case sample. This will determine which preset you should\nclick in this dropdown. There are various other parameters that you can \nset, but they are covered in further sections of this guide.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/chipseq/#broad-vs-narrow-peak-calling", 
            "text": "Choosing between broad and narrow peak calling depends on the experiment\ndesign. The following are good rules of thumb for choosing between the\ntwo configurations. If you are not sure which configuration to use,\nplease consult with an expert at your institution or  contact us .  Narrow Peak Calling  If your target protein is a transcription factor, you should probably\nchoose narrow peak calling. You can also try the narrow peak calling\nworkflows for the following histone marks:   H3K4me3  H3K4me2  H3K9-14ac  H3K27ac  H2A.Z   Broad Peak Calling  You should try the broad peak calling workflows for the following\nhistone marks:   H3K36me3  H3K79me2  H3K27me3  H3K9me3  H3K9me1   Special Cases  In some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between\nnarrow and broad shape, you might need to look into each peak region and\nconsult experts.   Warning  If your fragment size is less than 50 base pairs, please refer to the frequently asked questions .", 
            "title": "Broad vs. narrow peak calling"
        }, 
        {
            "location": "/guides/tools/chipseq/#selecting-parameters", 
            "text": "There are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Parameter Wrapper\" substep.   The following are the parameters that can be set, a short\ndescription of each parameter, and an example value. If you\nhave questions, please  contact us .     Parameter Name  Description  Example      Output prefix ( required )  A name used a prefix for all outputs in the run  SAMPLE1    Reference genome ( required )  Supported reference genome from one of hg19, GRCh38, mm9, mm10, dm3  GRCh38    Output bigWig  Whether or not to include a bigwig file in the output  True    Remove blacklist peaks  Whether or not to remove known problem areas  True    Fragment length  Hardcoded fragment length of your reads. 'NA' for auto-detect.  NA      Caution  Please be aware of the following stumbling points when setting parameters:   Do not use spaces anywhere in your input file names, your output\n  prefix, or any of the other parameters. This is generally bad\n  practice and doesn't play well with the pipeline (consider using\n  \"_\" instead).  Do not change the output directory when you run the pipeline. At\n  the top of parameter input page, there is a text box that allows\n  you to change the output folder.  Please ignore that setting . You\n  only need to specify an output prefix as described above. All of\n  the results will be put under  /Results/[OUTPUT_PREFIX] .", 
            "title": "Selecting parameters"
        }, 
        {
            "location": "/guides/tools/chipseq/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ files you uploaded in  the upload data section . You can do this by \nclicking on the  ChIP Reads  and  Control Reads  slots and\nselecting the respective files. If you are not doing a case/control\nrun, you only need to hook up the case sample.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/chipseq/#starting-the-workflow", 
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that:   all of the inputs are correctly hooked up (see  hooking up inputs ), and   all of the required parameters are set (see  setting parameters ).   If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/chipseq/#monitoring-run-progress", 
            "text": "Once you have started one or more ChIP-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand the job logs can be accessed by clicking around the sub-items.      Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/chipseq/#interactive-visualizations", 
            "text": "Today, the ChIP-Seq pipeline does not produce an interactive visualization. We are\nworking on adding this! In the meantime, you can view the cross-correlation plot(s)\nas outlined in the sections below.", 
            "title": "Interactive visualizations"
        }, 
        {
            "location": "/guides/tools/chipseq/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view of your cloud workspace. This is similar to the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files. To access ChIP-Seq results, you should click on the  Results  folder, then select the output folder name you gave in the  selecting parameters  part of the guide.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/chipseq/#interpreting-results", 
            "text": "For the ChIP-Seq pipeline, every pipeline run outputs a  README.doc  file\nwhich contains the latest information on which results are included.\nYou can refer to that file for the most up to date information on raw outputs.", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/chipseq/#frequently-asked-questions", 
            "text": "If you have any questions not covered here, feel free to  contact us .  Q: Should I choose narrow peak calling pipeline or broad peak calling pipeline?  A. We built two workflows: one for narrow peak calling and another broad\npeak calling. If your target protein is a transcription factor, please\nuse narrow peak calling workflow. For histone marks H3K4me3, H3K4me2,\nH3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling\nworkflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and\nH3K9me1, you could try broad peak calling workflow. In some scenario,\nH3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad\nshape, you might need to look into each peak region and consult\nexperts.  Q. What to do if your fragment size is less than 50 base pairs?  A. We estimate fragment size from the data based on the cross correlation\nplot. Usually the fragment size is above 50bp. If the estimated\nfragment size lower than 50bp, the workflow will stop at the peak\ncalling stage (MACS2/SICER) after BWA mapping finishes. You can rerun\nthe analysis with a specified fragment length.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/", 
            "text": "Authors\n\n\nScott Newman, Clay McLeod, Yongjin Li\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nFusion genes are important for cancer diagnosis, subtype definition and\ntargeted therapy. RNASeq is useful for detecting fusion transcripts.\nComputational methods face challenges to identify fusion transcripts\narising from internal tandem duplication (ITD), multiple genes, low\nexpression or non-templated insertions. Here we present an end-to-end\nclinically validated pipeline \"Rapid RNA-Seq\" that detects gene fusions\nand ITDs from human RNA-Seq.\n\n\nOverview\n\n\nInputs\n\n\nThe input can be either of the two entries below based on whether you want to start\nwith FastQ files or a BAM file.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPaired FastQ files\n\n\nGzipped FastQ files generated by human RNA-Seq\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\nBAM file\n\n\nAligned reads file from human RNA-Seq\n\n\nSample.bam\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nIf you provide a BAM file to the pipeline, it \nmust\n be aligned to GRCh37-lite.\nRunning a BAM aligned to any other reference genome is not supported. Maybe more\nimportantly, we do not check the genome build of the BAM, so errors in computation\nor the results can occur. If your BAM is \nnot\n aligned to this genome build, we \nrecommend converting the BAM back to FastQ files using \n\nPicard's SamToFastq\n\nfunctionality and using the FastQ version of the pipeline.\n\n\n\n\nOutputs\n\n\nThe Rapid RNA-Seq pipeline produces the following outputs:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPredicted gene fusions (.txt)\n\n\nFile containing putative gene fusions.\n\n\n\n\n\n\nCoverage file (.bw)\n\n\nbigWig file containing coverage information.\n\n\n\n\n\n\nSplice junction read counts (.txt)\n\n\nRead counts for the splice junction detected.\n\n\n\n\n\n\nInteractive fusion visualization\n\n\nFusion visualization produced by ProteinPaint.\n\n\n\n\n\n\nInteractive coverage visualization\n\n\nCoverage visualization produced by ProteinPaint.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nThe raw sequence data is aligned to GRCh37-lite using standard STAR\n   mapping.\n\n\nA coverage bigWig (.bw) file is produced to allow the user to assess\n   sample quality across the genome.\n\n\nTwo gene fusion detection algorithms are run in parallel.\n\n\nThe \nFuzzion\n (Rice et al. unpublished data) fusion detection\n  algorithm is run to provide high sensitivity for recurrent gene\n  fusions.\n\n\nThe \nRNAPEG\n (Edmonson et al. unpublished data) splice\n  junction read counting algorithm is run to quantify read counts\n  for splice junctions. These splice junction read counts are then\n  used by \nCicero\n (Li et al. unpublished data) to detect\n  putative gene fusions.\n\n\n\n\n\n\nCustom visualizations for putative gene fusions and genome coverage\n   are produced by ProteinPaint.\n\n\n\n\nMapping\n\n\nWe use the \nSTAR aligner\n \nto rapidly map reads to the GRCh37 human reference genome. This step generally \ntakes around one hour to complete assuming approximately 55-75 million paired reads\nare supplied.\n\n\nCoverage\n\n\nInternally developed scripts calculate the coverage of mapped reads\ngenome wide. The resulting bigWig file can be viewed in ProteinPaint or\nused for quality control.\n\n\nSplice junction read quantification\n\n\nWe use our RNAPEG software to quantify reads spanning known and novel\nsplice junctions. RNAPEG also corrects improper mappings at splice\njunction boundaries for more accurate definition of novel splice\njunctions. The resulting junctions file can be viewed along with the\ncoverage bigWig file to gain insights into gene expression and splicing\npatterns\n\n\nGenome-wide fusion prediction\n\n\nWe developed an assembly-based algorithm CICERO (Clipped-reads Extended\nfor RNA Optimization) that is able to extend the read-length spanning\nfusion junctions for detecting complex fusions. CICERO finds clipped\nreads and junction spanning reads, assembles them into a contig and maps\nthe contig back to the reference genome. Mapped contigs are then\nannotated and filtered. Those with potential genic effects including\ngene fusion, ITD, readthrough or circular RNA are reported in the\n\nfinal_fusions.txt\n file. An interactive version of this file with\npredictions sorted by quality can be inspected with the ProteinPaint\ninteractive fusion viewer.\n\n\nAn abstract describing CICERO was presented at ASHG, 2014:\n\nhttp://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm\n\n\nLow stringency fusion gene \"Hotpot\" search\n\n\nWe have observed that certain fusions such as KIAA1549-BRAF in low-grade\nglioma have apparently limited read support in the bam file \u2014 either due\nto low expression or low tumor purity. In these cases, we use a\nsecondary tool, FUZZION, that performs fuzzy matching for known fusion\ngene junctions for every read in the bam file (both mapped and\nunmapped). FUZZION can recover even a single low quality read\npotentially supporting a known fusion gene junction. The FUZZION output\nis a simple text file with read IDs and sequences supporting a\nparticular gene fusion. The fusion point is indicated with square\nbrackets \n[]\n.\n\n\nGetting started\n\n\n\n\nCaution\n\n\nThis pipeline assumes GRCh37-lite coordinates. If your BAM is \n\nnot\n aligned to this genome build, we recommend converting the BAM \nback to FastQ files using \nPicard's SamToFastq\n\nfunctionality.\n\n\n\n\nTo get started, you need to navigate to the \nRapid RNA-Seq tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe Rapid RNA-Seq pipeline takes either a paired set of Gzipped FastQ files or \na GRCh37-lite aligned BAM from human RNA-Seq. You can upload your input file(s)\nusing the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\nOnce you've uploaded data to your cloud workspace, click \"Launch Tool\" on the \ntool's landing page\n. A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files you uploaded in \n\nthe upload data section\n. In this example,\nwe are using the FastQ version of the pipeline, so you can \nhook up the inputs by clicking on the \nFastq/R1\n and \nFastq/R2\n\nslots and selecting the respective files. If you are using \nthe BAM-based workflow, the process is similar.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more Rapid RNA-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n \n\n\nInterpreting results\n\n\nThe complete output file specification is listed in the \noverview section\n\nof this guide. Here, we will discuss each of the different output files in more detail.\n\n\n\n\nPredicted gene fusions\n: The putative gene fusions will be\n  contained in the file \n[SAMPLE].final_fusions.txt\n. This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsample\n\n\nSample name\n\n\n\n\n\n\ngene*\n\n\nGene name\n\n\n\n\n\n\nchr*\n\n\nChromosome name\n\n\n\n\n\n\npos*\n\n\nGenomic Location\n\n\n\n\n\n\nort*\n\n\nStrand\n\n\n\n\n\n\nreads*\n\n\nSupporting reads\n\n\n\n\n\n\nmedal\n\n\nEstimated pathogenicity assessment using St. Jude Medal Ceremony\n\n\n\n\n\n\n\n\n\n\nCoverage file\n: A standard\n    \nbigWig\n file\n    used to describe genomic read coverage.\n\n\nSplice junction read counts\n: A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njunction\n\n\nSplice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in \n.bed\n output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.\n\n\n\n\n\n\ncount\n\n\nRaw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.\n\n\n\n\n\n\ntype\n\n\nEither \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).\n\n\n\n\n\n\ngenes\n\n\nGene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.\n\n\n\n\n\n\ntranscripts\n\n\nList of known transcript IDs matching the junction.\n\n\n\n\n\n\nqc_flanking\n\n\nCount of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).\n\n\n\n\n\n\nqc_plus\n\n\nCount of supporting reads aligned to the + strand.\n\n\n\n\n\n\nqc_minus\n\n\nCount of supporting reads aligned to the - strand.\n\n\n\n\n\n\nqc_perfect_reads\n\n\nCount of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).\n\n\n\n\n\n\nqc_clean_reads\n\n\nCount of supporting reads whose alignments are not perfect but which have a ratio of \n= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.\n\n\n\n\n\n\n\n\nKnown issues\n\n\n\n\nAdapter contamination\n\n\nThis pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.\n\n\n\n\n\n\nHigh coverage regions\n\n\nCertain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.\n\n\n\n\n\n\nInteractive Visualizations Exon vs Intron Nomenclature\n\n\nWhen a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.\n\n\n\n\nFrequently asked questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact form\n.", 
            "title": "Rapid RNA-Seq Fusion Detection"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#overview", 
            "text": "Inputs  The input can be either of the two entries below based on whether you want to start\nwith FastQ files or a BAM file.     Name  Description  Example      Paired FastQ files  Gzipped FastQ files generated by human RNA-Seq  Sample_R1.fastq.gz and Sample_R2.fastq.gz    BAM file  Aligned reads file from human RNA-Seq  Sample.bam      Caution  If you provide a BAM file to the pipeline, it  must  be aligned to GRCh37-lite.\nRunning a BAM aligned to any other reference genome is not supported. Maybe more\nimportantly, we do not check the genome build of the BAM, so errors in computation\nor the results can occur. If your BAM is  not  aligned to this genome build, we \nrecommend converting the BAM back to FastQ files using  Picard's SamToFastq \nfunctionality and using the FastQ version of the pipeline.   Outputs  The Rapid RNA-Seq pipeline produces the following outputs:     Name  Description      Predicted gene fusions (.txt)  File containing putative gene fusions.    Coverage file (.bw)  bigWig file containing coverage information.    Splice junction read counts (.txt)  Read counts for the splice junction detected.    Interactive fusion visualization  Fusion visualization produced by ProteinPaint.    Interactive coverage visualization  Coverage visualization produced by ProteinPaint.     Process   The raw sequence data is aligned to GRCh37-lite using standard STAR\n   mapping.  A coverage bigWig (.bw) file is produced to allow the user to assess\n   sample quality across the genome.  Two gene fusion detection algorithms are run in parallel.  The  Fuzzion  (Rice et al. unpublished data) fusion detection\n  algorithm is run to provide high sensitivity for recurrent gene\n  fusions.  The  RNAPEG  (Edmonson et al. unpublished data) splice\n  junction read counting algorithm is run to quantify read counts\n  for splice junctions. These splice junction read counts are then\n  used by  Cicero  (Li et al. unpublished data) to detect\n  putative gene fusions.    Custom visualizations for putative gene fusions and genome coverage\n   are produced by ProteinPaint.   Mapping  We use the  STAR aligner  \nto rapidly map reads to the GRCh37 human reference genome. This step generally \ntakes around one hour to complete assuming approximately 55-75 million paired reads\nare supplied.  Coverage  Internally developed scripts calculate the coverage of mapped reads\ngenome wide. The resulting bigWig file can be viewed in ProteinPaint or\nused for quality control.  Splice junction read quantification  We use our RNAPEG software to quantify reads spanning known and novel\nsplice junctions. RNAPEG also corrects improper mappings at splice\njunction boundaries for more accurate definition of novel splice\njunctions. The resulting junctions file can be viewed along with the\ncoverage bigWig file to gain insights into gene expression and splicing\npatterns  Genome-wide fusion prediction  We developed an assembly-based algorithm CICERO (Clipped-reads Extended\nfor RNA Optimization) that is able to extend the read-length spanning\nfusion junctions for detecting complex fusions. CICERO finds clipped\nreads and junction spanning reads, assembles them into a contig and maps\nthe contig back to the reference genome. Mapped contigs are then\nannotated and filtered. Those with potential genic effects including\ngene fusion, ITD, readthrough or circular RNA are reported in the final_fusions.txt  file. An interactive version of this file with\npredictions sorted by quality can be inspected with the ProteinPaint\ninteractive fusion viewer.  An abstract describing CICERO was presented at ASHG, 2014: http://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm  Low stringency fusion gene \"Hotpot\" search  We have observed that certain fusions such as KIAA1549-BRAF in low-grade\nglioma have apparently limited read support in the bam file \u2014 either due\nto low expression or low tumor purity. In these cases, we use a\nsecondary tool, FUZZION, that performs fuzzy matching for known fusion\ngene junctions for every read in the bam file (both mapped and\nunmapped). FUZZION can recover even a single low quality read\npotentially supporting a known fusion gene junction. The FUZZION output\nis a simple text file with read IDs and sequences supporting a\nparticular gene fusion. The fusion point is indicated with square\nbrackets  [] .", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#getting-started", 
            "text": "Caution  This pipeline assumes GRCh37-lite coordinates. If your BAM is  not  aligned to this genome build, we recommend converting the BAM \nback to FastQ files using  Picard's SamToFastq \nfunctionality.   To get started, you need to navigate to the  Rapid RNA-Seq tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#uploading-data", 
            "text": "The Rapid RNA-Seq pipeline takes either a paired set of Gzipped FastQ files or \na GRCh37-lite aligned BAM from human RNA-Seq. You can upload your input file(s)\nusing the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#running-the-tool", 
            "text": "Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the  tool's landing page . A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ files you uploaded in  the upload data section . In this example,\nwe are using the FastQ version of the pipeline, so you can \nhook up the inputs by clicking on the  Fastq/R1  and  Fastq/R2 \nslots and selecting the respective files. If you are using \nthe BAM-based workflow, the process is similar.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#starting-the-workflow", 
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#monitoring-run-progress", 
            "text": "Once you have started one or more Rapid RNA-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.      Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#analysis-of-results", 
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#interpreting-results", 
            "text": "The complete output file specification is listed in the  overview section \nof this guide. Here, we will discuss each of the different output files in more detail.   Predicted gene fusions : The putative gene fusions will be\n  contained in the file  [SAMPLE].final_fusions.txt . This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.      Field Name  Description      sample  Sample name    gene*  Gene name    chr*  Chromosome name    pos*  Genomic Location    ort*  Strand    reads*  Supporting reads    medal  Estimated pathogenicity assessment using St. Jude Medal Ceremony      Coverage file : A standard\n     bigWig  file\n    used to describe genomic read coverage.  Splice junction read counts : A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.      Field Name  Description      junction  Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in  .bed  output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.    count  Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.    type  Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).    genes  Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.    transcripts  List of known transcript IDs matching the junction.    qc_flanking  Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).    qc_plus  Count of supporting reads aligned to the + strand.    qc_minus  Count of supporting reads aligned to the - strand.    qc_perfect_reads  Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).    qc_clean_reads  Count of supporting reads whose alignments are not perfect but which have a ratio of  = 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#known-issues", 
            "text": "Adapter contamination  This pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.    High coverage regions  Certain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.    Interactive Visualizations Exon vs Intron Nomenclature  When a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.", 
            "title": "Known issues"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#frequently-asked-questions", 
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact form .", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/warden/", 
            "text": "Authors\n\n\nLance Palmer\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nThe WARDEN (\nW\norkflow for the \nA\nnalysis of \nR\nNA-Seq \nD\nifferential \nE\nxpressio\nN\n)\nsoftware uses RNA-Seq sequence files to perform alignment, coverage\nanalysis, gene counts and differential expression analysis.\n\n\nOverview\n\n\nInputs\n\n\nThe WARDEN workflow requires 2 types of input files and along with two\nrequired parameters to be set. All other parameters are preset with sane defaults.\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n)\n\n\nInput file(s)\n\n\nGzipped FastQ files generated by experiment\n\n\nSample1.fastq.gz, Sample2.fastq.gz\n\n\n\n\n\n\nSample sheet (\nrequired\n)\n\n\nInput file\n\n\nSample sheet generated and uploaded by the user\n\n\n*.txt\n\n\n\n\n\n\n\n\n\n\nTodo\n\n\nDelete these if they are in the parameters below.\n\n\n\n\n| Reference genome | Parameter | Supported reference genome (HG19, HG38, mm9, mm10, dm3, dm6) | HG38 |\n| Sequencing method | Parameter | Sequencing method of experiment (forward, reverse, unstranded) | forward |\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFastQC Report\n\n\nQuality control analysis by FastQC.\n\n\n\n\n\n\nAligned BAM\n\n\nAligned BAM files from STAR mapping.\n\n\n\n\n\n\nSplice junctions\n\n\nSplice junction information from STAR mapping.\n\n\n\n\n\n\nCoverage files\n\n\nbigWig (\n.bw\n) and BED (\n.bed\n) files detailing coverage.\n\n\n\n\n\n\nGene counts\n\n\nGene counts generated by HT-Seq count.\n\n\n\n\n\n\nVOOM/LIMMA results\n\n\nPairwise comparisons of expression data. Requires at least 3 samples vs 3 samples.\n\n\n\n\n\n\nSimple DE analysis\n\n\nNo statistical analysis, requires only a 1 samples vs 1 sample comparison.\n\n\n\n\n\n\nMA/Volcano plots\n\n\nBoth of the above produce tabular outputs, MA plots and volcano plots.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nFastQ files generated by RNA-Seq are mapped to a reference genome using the STAR.\n\n\nHT-Seq count is used to assign mapped reads to genes. \n\n\nDifferential expression analysis is performed using VOOM normalization of counts and\nLIMMA analysis. \n\n\nCoverage plots of mapped reads are generated as interactive visualizations.\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nWARDEN tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe WARDEN Differential Expression analysis pipeline takes Gzipped FastQ\nfiles generated by an RNA-Seq experiment as input. You can upload your input \nfile(s) using the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nSample sheet\n\n\nOnce your data is uploaded, you'll need to create a sample sheet which\ndescribes the relationship between case and control samples,\nphenotype/condition information, and the comparisons you would like to\nperform. The sample sheet is a tab-delimited text document that can be\ncreated in Microsoft Excel (recommended) or a text editor.\n\n\n\n\nNote\n\n\nYou will need to upload your sample sheet in a similar manner as your\nFastQ files, so you can follow the \nsame uploading instructions\n \nto achieve this.\n\n\n\n\nPrepare using Microsoft Excel\n\n\n\n\nTip\n\n\nDownload the \nfile_download\n sample excel spreadsheet\n as a starting\npoint!\n\n\n\n\nThe final product for the excel spreadsheet will look like the\nscreenshot below. If you create the sample sheet from scratch, please\nensure the the columns are \nexactly\n in this order.\n\n\n\n\n\n\n\n\nSample rows\n\n\nEach row in the spreadsheet (except for the last row, which we will talk \nabout in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below:\n\n\n\n\nGuidelines\n\n\n\n\nThe sample name should be unique and should only contain letters,\nnumbers and underscores.\n\n\nThe condition/phenotype column associates similar samples together.\nThe values should contain only letters, numbers and underscores.\n\n\nReadFile1 should contain forward reads (e.g. \n*.R1.fastq.gz\n or \n*_1.fastq.gz\n).\n\n\nReadFile2 will contain reads in reverse orientation to ReadFile2\n(e.g. \n*.R2.fastq.gz\n or \n*_2.fastq.gz\n).\n\n\nFor single end reads a single dash ('-') should be entered in the ReadFile2 column.\n\n\n\n\n\n\nComparison row\n\n\nThe last line in the sample sheet is called the \"comparison row\". This\nline specifies the comparisons to be done between conditions/phenotypes.\nAll pairwise combinations of the values in the \"Phenotype\" column can be \nanalyzed. To specify the comparisons, on a separate line, include \n#comparisons=\n followed be a comma delimited list of two conditions separated by a dash. \n\n\n\n\nExample\n\n\nThe following lines are all valid examples.\n\n\n\n\n#comparisons=KO-WT\n\n\n#comparisons=Condition1-Control,Condition2-Control\n\n\n#comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1\n\n\n\n\n\n\n\n\nNote\n\n\nIf a comparison has at least 3 samples for each condition/phenotype,\nVOOM/LIMMA will be run. A simple differential comparison will be run on\nall samples.\n\n\n\n\nFinalizing the sample sheet\n\n\nTo finalize the sample sheet, save the Microsoft Excel file with\nwhatever name you like. Save the file as an Excel Workbook with the\n.xlsx extension.\n\n\nPrepare using a text editor\n\n\n\n\nTip\n\n\nDownload the \nfile_download\n sample text file\n as a starting\npoint!\n\n\n\n\nCreating a sample sheet with a text editor is an option for advanced\nusers. The process of creating a sample sheet with a text editor is the same as\ncreating one with Microsoft Excel, with the small difference that you\nmust manually create your columns using the tab character. Save the file\nwith a .txt extension.\n\n\nRunning the tool\n\n\n\n\nNote\n\n\nThe WARDEN tool operation is slightly different than the other pipelines\nbecause it accepts a variable number of samples. \nFirst\n, you will run\na \"bootstrapping\" step that creates a custom executable for your\nanalysis. \nSecond\n, you will need to manually execute the generated\nworkflow from the first step. This allows us to take advantage of many\nnice features, like check-pointing and cost reduction. Don't worry, \nwe'll show you how to do this step by step below.\n\n\n\n\nOnce you've uploaded data to your cloud workspace, click \"Launch Tool\" on the \ntool's landing page\n. You will be redirected to the virtual\ncloud workspace with the workflow screen opened for you.\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files and sample sheet\n you uploaded in \nthe upload data section\n. \nClick the \nFASTQ_FILES\n input field and select \nall\n FastQ files.\nNext, click the \nsampleList\n input field and select the corresponding\nsamplesheet.\n\n\n\n\nSelecting parameters\n\n\nWe now need to configure the parameters for the pipeline, such as reference\ngenome and sequencing method. You can access all of the available parameters \nby clicking on the \nWARDEN WORKFLOW GENERATOR\n substep.\n\n\n\n\nParameter setup steps\n\n\n\n\nIn the \nOutput Folder\n field, select a folder to output to. You can\nstructure your experiments however you like (e.g. \n/My_Outputs\n)\n\n\nIn the \nanalysisName\n field, enter a prefix for all of the output files. This\ncan be any value you want to use to remember this run. \nBe sure to use underscores\ninstead of spaces here!\n\n\nSelect the \nsequenceStandedness\n from the drop down menu. \nThis information can be determined from the sequencing or source \nof the data. If you don't know what to put here, select \"no\".\n\n\nSelect the \nGenome\n pulldown menu. Choose the appropriate box.\n\n\nThe LIMMA parameters can be left alone for most analyses. If you are\nan advanced LIMMA user, you can change the various settings exposed\nbelow the required parameters.\n\n\nWhen all parameters have been set, press the save button.\n\n\n\n\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up and your parameters are set, \nyou should be able to start the workflow by clicking the \"Run as Analysis...\" \nbutton in the top right hand corner of the workflow dialog.\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nThe tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\".\n\n\n\n\nWhen the custom workflow has finished generating, the word 'Done' will\nappear in green in the status column. This indicates that the\nbootstrapping step has completed successfully. \n\n\n\n\nCustom Workflow Process\n\n\n\n\nWait for the workflow generator to finish.\n\n\n\n\nClick on the WARDEN name in the name column.\n\n\n\n\n\n\nYou will now be on a page specific to the running of the workflow.\n    On the left side, you will see the inputs you selected for the\n    workflow generator. On the right side are the output files\n    (including the generated workflow). Select the generated workflow as\n    shown in the picture below.\n\n\n\n\n\n\n\n\nYou will now be within the output folder you specified earlier.\n    Select the file that begins with 'WARDEN WORKFLOW:'\n\n\n\n\n\n\n\n\nA workflow generated for your data will be presented to you. Select\n    'Run as analysis' in the upper right.\n\n\n\n\n\n\n\n\nThe workflow will initiate, and you will be brought to the 'Monitor'\n    page. (Note to get back to this page, you can select 'Monitor' on\n    one of the menu bars near the top ) Expand the the workflow progress\n    be selecting the '+' sign next to 'In Progress'\n\n\n\n\n\n\n\n\nAs parts of the pipeline are run, you will see different tasks in\n    different colors. Green means done, blue is running, orange is\n    waiting, and red means error.\n\n\n\n\n\n\n\n\nWhen done the status will be shown as 'Done'. Select the Workflow\n    name under Status.\n\n\n\n\n\n\n\n\nYou will be brought to a page that show more information about the\n    workflow analysis. Click on the output folder to go to the output.\n\n\n\n\n\n\n\n\nThe output folders will now be shown.\n\n\n\n\n\n\n\n\nFor a description of the output, please refer to \nNavigating Results\n.\n\n\nMonitoring run progress\n\n\nOnce you have started one or more WARDEN runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kick off\ngets one row in the Monitor section.\n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n\n\nNavigating Results\n\n\n\n\nNote\n\n\nNavigating to the raw results of your runs is the same for all\npipelines. This guide will feature the \nrapid-rnaseq\n pipeline, but you can follow along for\nany tool.\n\n\n\n\nRaw result files\n\n\nNavigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is\n\nhere\n). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"View Results Files\".\n\n\n\n\nYou should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.\n\n\n\n\nCustom visualization results\n\n\nNavigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is\n\nhere\n). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"Visualize Results\".\n\n\n\n\nYou should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.\n\n\n\n\nInterpreting results\n\n\nThe complete output file specification is listed in the \noverview section\n\nof this guide. Here, we will discuss each of the different output files in more detail.\n\n\n\n\nPredicted gene fusions\n: The putative gene fusions will be\n  contained in the file \n[SAMPLE].final_fusions.txt\n. This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsample\n\n\nSample name\n\n\n\n\n\n\ngene*\n\n\nGene name\n\n\n\n\n\n\nchr*\n\n\nChromosome name\n\n\n\n\n\n\npos*\n\n\nGenomic Location\n\n\n\n\n\n\nort*\n\n\nStrand\n\n\n\n\n\n\nreads*\n\n\nSupporting reads\n\n\n\n\n\n\nmedal\n\n\nEstimated pathogenicity assessment using St. Jude Medal Ceremony\n\n\n\n\n\n\n\n\n\n\nCoverage file\n: A standard\n    \nbigWig\n file\n    used to describe genomic read coverage.\n\n\nSplice junction read counts\n: A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njunction\n\n\nSplice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in \n.bed\n output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.\n\n\n\n\n\n\ncount\n\n\nRaw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.\n\n\n\n\n\n\ntype\n\n\nEither \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).\n\n\n\n\n\n\ngenes\n\n\nGene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.\n\n\n\n\n\n\ntranscripts\n\n\nList of known transcript IDs matching the junction.\n\n\n\n\n\n\nqc_flanking\n\n\nCount of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).\n\n\n\n\n\n\nqc_plus\n\n\nCount of supporting reads aligned to the + strand.\n\n\n\n\n\n\nqc_minus\n\n\nCount of supporting reads aligned to the - strand.\n\n\n\n\n\n\nqc_perfect_reads\n\n\nCount of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).\n\n\n\n\n\n\nqc_clean_reads\n\n\nCount of supporting reads whose alignments are not perfect but which have a ratio of \n= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.\n\n\n\n\n\n\n\n\nPrimary Results\n\n\nAlignment statistics\n\n\nSeveral files should be examined initially to determine the quality of\nthe results. \nalignmentStatistics.txt\n shows alignment statistics for\nall samples. This file is a plain text tab-delimited file that can be\nopened in Excel or a text editor such as Notepad++. This file contains\ninformation on the total reads per sample, the percantage of duplicate\nreads and the percentage of mapped reads. An example of this file is\nbelow. (Within the DNAnexus output directory structure, these files will\nbe in the COMBINED_FLAGSTAT directory.)\n\n\n\n\n\n\n\n\nMultidimensional scaling (MDS) Plot\n\n\nThe second set of files to look at are the Multidimensional scaling\n(MDS) plots (\nhttps://en.wikipedia.org/wiki/Multidimensional_scaling\n)\nusing the plotMDS function within LIMMA. Similar to PCA, these graphs\nwill show how similar samples are to each other. There are different\nsets of MDS plots. For comparisons where there are 3 or more samples per\ncondition, an MDS plot using Voom (Limma) normalized values are\ngenerated. An example can be seen below. These files will be labeled\n\nmdsPlot.png\n. For all comparisons, regardless of sample size, and MDS\nplot will also be generated with Counts per million (CPM) normalized\ngene counts. These files will be labeled \nmdsPlot.normCPM.png\n.\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)\n\n\n\n\nMDS plot from just CPM normalized data.\n\n\n\n\nProteinPaint Visualizations\n\n\nSeveral files on DNAnexus allow the data to be viewed in the Protein\nPaint viewer. (Note: We plan to have links downloaded in the future to\nallow the viewing of these files off of DNAnexus.)\n\n\nLIMMA differential expression viewer\n\n\nWithin LIMMA/VIEWERS direcory (note if no comparisons meet the 3 sample\ncondition, the LIMMA folder will not exist), there will be a viewer file\nfor each valid comparison ( *\nresults.\n.txt.viewer**). Simply select\nthe file and press 'Launch viewer' in the lower right. A viewer will pop\nup showing both the MA Plot and Volcano plot. By moving the mouse over a\ncircle, the circle will hilight and the corresponding gene on the other\ngraph will also hilight. Additional information about the gene and its\nexpression values will also be shown. One can also type in multiple gene\nsymbols in the provided text box. By pressing 'Show gene labels' all\nthese genes will show up on the plots.\n\n\n\n\nSimple differential expression viewer\n\n\nThere will also be a viewer for the simple differential expresssion\nanalysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all\nbeen set to 1, so the volcano plot will not be relevant.\n\n\nbigWig viewer\n\n\nIn the BIGWIG_VIEWER directory there will be a bigwigViewer file.\nSelect this file and then 'Launch viewer'. A graph of coverage for the\ngenome should be visible.\n\n\nSecondary Results\n\n\n\n\nInteractive MA/Volcano Plots\n\n\nIn addition to viewing the MA and volcano plots through the visualization tool\n\n\n\n\nDifferential expression results\n \n\n\nOther useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition, \nresults.*.txt\n will be produced.\n\n\n\n\nGSEA.input.txt\n and \nGSEA.tStat.txt\n \n\n\nInput files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram.\n- (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)\n\n\n\n\n\n\nFor plain text results from the simple differential expresison analysis, the files will be named \nsimpleDE.*.txt\n. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)\n\n\n\n\n\n\nPrelabelled MA and volcano plots are provided for the analysis. These files are labeled \nmaPlot.*.png\n and \nvolcanoPlot.*.png\n where \n*\n is the comparison (e.g. ko_vs_wt)\n\n\n\n\n\n\nThe MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An exmaple MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)\n\n\n\n\n\n\n\n\nThe volcano plot shows the Log2Fold change between the conditions on the\nX-axis, and the -Log10 of the multiple testing corrected P-value on the\nY-axis.\n\n\n\n\nAn MA plot is generated for all comparisons regardless of number of\nsamples. This is the \nsimpleDEPlot.*.png\n no statistics are shown\nand genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)\n\n\n\n\nDifferential analysis input\n\n\nInputs and commands are provided for rerunning differential expression\nanalysis on ones own computer. The R commands used for the analysis are\nfound in \nvoomLimma.R\n. An experienced R user can rerun the analysis\nwith any desired changes. This analysis requires the input\n\ncountFile.txt\n which contains counts per genes, the\n\nRparameters.txt\n file containing input parameters, and a processed\nsample list file \nsampleList.txt\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)\n\n\nThe input for the simple differential analysis expression will be\n\nRparameters_simple.txt\n, \nsimpleDE.R\n, \ncountFile.txt\n and\n\nsampleList.txt\n. \ncountFile.txt\n and \nsampleList.txt\n are the\nsame files used by the LIMMA analysis.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe SIMPLE_DIFEX directory.)\n\n\n\n\nCoverage results\n\n\nbigWig files will be generated for use in genome browsers (such as IGV\n\nhttp://software.broadinstitute.org/software/igv/\n). For each smaple,\nmultiple bigWig files will be found. For all types of sequencing\nstrandedness, there will be bigWig files labelled,\n\n*.sortedCoverageFile.bed.bw\n where '\n' is the sample name. For\nstranded data there will also be*.sortedPosCoverageFile.bed.bw\n* and\n\n*.sortedNegCoverageFile.bed.bw\n which contains coverage information\nfor the positive and negative strand of the genome.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe BIGWIG directory.)\n\n\n\n\nQuality Control Results (FastQC)\n\n\nWithin the FastQC directory, foreach sample and read direction there\nwill be an html file and a zip file (\n*.FastQc.html\n\n\n*.FastQc.zip\n where '*' is the base FastQ name), containing results\nfrom FastQTC. For the average user the html file is sufficient. This\nfile can give some basic statistics on the quality of the data.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe FastQC directory.)\n\n\n\n\nBAM alignment files\n\n\nThere are two BAM files generated per sample that contain mapping\ninformation for all reads. The first is labeled\n\n*.Aligned.sortedByCoord.dup.bam\n where '\n' is the sample name. The\nBAM file is sorted by coordinates and has duplicates marked. The second\nfile is*.Aligned.toTranscriptome.out.bam\n* and contains reads mapped\nto transcripts.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)\n\n\n\n\nChimeric reads and junction files\n\n\nAdditional files created by STAR are provided. More information on these\nfiles can be found at\n\nhttp://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.posix/doc/STARmanual.pdf\n.\n\n*.SJ.out.tab\n contain splice junction information. Fusion detectino\nfiles are labelled \n*.Chimeric.out.bam\n and\n\n*.Chimeric.out.junction\n.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)\n\n\n\n\nFPKM and count files (per sample)\n\n\nPer sample files containing FPKM and raw count values for each gene can\nbe found in \n*.fpkm.txt\n and \n*.htseq_counts.txt\n where '*' is\nthe sample name.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe \nNOTE FIND OUT WHICH DIRECTORY\n directory.)\n\n\n\n\nMethods Files\n\n\nA more human readable explanation is found in \nmethods.docx\n. Detailed\ndocumentation can be found in \nmethods.txt\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe METHODS directory.)\n\n\nAuxilary Files\n\n\nThis section describes the files that exist within the DNAnexus output\nfolder. Most of these files will not be of interest to the average user.\nHowever, interactive viewers are describe in \nLIMMA differential\nexpression viewer\n and \nSimple\ndifferential expression viewer\n.\n\n\nThe output will be divided into multiple folders. The results being the\nmost useful will be the differential expression analysis results in the\nLIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage\nwill be in the BIGWIG folder. Other folder contain different types of\ndata and are explained in further detail below.\n\n\n\n\n\n\n\n\nThe following description of files is sorted by their output directory.\n\n\n\n\nALIGN\n\n\nThis directory contains the BAM files described in \nBAM alignment\nfiles\n and the chimeric and junction files are\ndescribed in \nChimeric reads and junction\nfiles\n. In adition there are 2 log\nfiles. \n*Log.final.out\n has relevant statistics for the alignemnt.\nThe \n*.Log.out\n file just contains a log of the analysis run,\nincluding input parameters. Per sample FLAGSTAT results are found in\n\n*.flagStatOut.txt\n. These flagstat files are combined into the file\n\nalignmentStatistics.txt\n described in \nInitial analysis of\nresults\n. Finally the ALIGN directory has\nmultiple \n.starAlign.methods.txt files. These files can be ignored as\nthey are summarized in the finalmethods.docx\n* and \nmethods.txt\n\nfiles described in \nMethods Files\n.\n\n\n\n\nBIGWIG\n\n\nAll of the files here are described in section \nCoverage\nresults\n. The \nbgToBw.methods.txt\n files can be\nignored as they are summarized in the files described in \nMethods\nFiles\n.\n\n\n\n\nBIGWIG_VIEWER\n\n\nSee \nbigWig viewer\n\n\n\n\nCOMBINED_FLAGSTAT\n\n\n\n\nTodo\n\n\nDescribe combined flagstat.\n\n\n\n\n\n\nCOMBINED_HTSEQ\n\n\nUsed for input in differential expression analysis. The\ncombineCountFile.txt is the same as countFile.txt described in\n\nDifferential analysis input\n\n\n\n\nCOVERAGE\n\n\nBED graph files used to generate bigWig files are here.\n\n\n\n\nFastQC\n\n\nSee \nQuality Control Results (FastQC)\n\n\n\n\nHTSEQ\n\n\nPer-sample HTSEQ-count results (\n*.htseq_counts.txt\n) and FPKM\nresults (\n*.fpkm.txt\n). Temporary methods files are found as\n*.htseq-count.methods.txt\n\n\n\n\nLIMMA\n\n\nmdsPlot.png\n, \nmaPlot.\n.png,volcanoPlot.\n.png\n are described in\n\nInitial analysis of results\n\n\nresults.\n.txt,GSEA.input.\n.txt\n and *\nGSEA.tStat.\n.txt** are\ndescribe in \nDifferential expression\nresults\n\n\nvoomLimma.R\n, \ncountFile.txt\n, \nRparameters.txt\n, and\n\nsampleList.txt\n are described in \nDifferential analysis\ninput\n\n\nSee \nLIMMA differential expression\nviewer\n for a description of the\nVIEWERS directory.\n\n\nOther files in the LIMMA directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nlimmaMethods.txt is an intermediate file describing methods. Out.tar.gz\nis used for testing purposes. The sessionInfo.txt file describe the R\nsession working parameters and modules loaded. meanVariance.png is a\nplot for assessing quality of count data\n(\nhttps://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29\n)\n\n\n\n\nMETHODS\n\n\nThe files here are described in \nMethods Files\n.\n\n\n\n\nSAMPLELIST\n\n\nThese files are used internally by the pipeline.\n\n\n\n\nSIMPLE_DIFEX\n\n\nmdsPlot.normCPM.png\n and *\nsimpleDEPlot.\n.png** are described in\n\nInitial analysis of results\n\n\n*\nsimpleDE.\n.txt** are describe in \nDifferential expression\nresults\n\n\nsimpleDE.R\n, \ncountFile.txt\n, \nRparameters_simple.txt\n, and\n\nsampleList.txt\n are described in \nDifferential analysis\ninput\n\n\nSee \nSimple differential expression\nviewer\n for a description of the\nVIEWERS directory.\n\n\nOther files in the SIMPLE_DIFEX directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nsimpleDifEx.methods.txt is an intermediate file describing methods.\nOut.tar.gz is used for testing purposes. The sessionInfo.txt file\ndescribe the R session working parameters and modules loaded.\n\n\nKnown Issues\n\n\n\n\nAdapter contamination\n\n\nThis pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.\n\n\n\n\n\n\nHigh coverage regions\n\n\nCertain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.\n\n\n\n\n\n\nInteractive Visualizations Exon vs Intron Nomenclature\n\n\nWhen a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.\n\n\n\n\nFrequently Asked Questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact\nform\n.", 
            "title": "WARDEN Differential Expression Analysis"
        }, 
        {
            "location": "/guides/tools/warden/#overview", 
            "text": "Inputs  The WARDEN workflow requires 2 types of input files and along with two\nrequired parameters to be set. All other parameters are preset with sane defaults.     Name  Type  Description  Example      FastQ files ( required )  Input file(s)  Gzipped FastQ files generated by experiment  Sample1.fastq.gz, Sample2.fastq.gz    Sample sheet ( required )  Input file  Sample sheet generated and uploaded by the user  *.txt      Todo  Delete these if they are in the parameters below.   | Reference genome | Parameter | Supported reference genome (HG19, HG38, mm9, mm10, dm3, dm6) | HG38 |\n| Sequencing method | Parameter | Sequencing method of experiment (forward, reverse, unstranded) | forward |  Outputs     Name  Description      FastQC Report  Quality control analysis by FastQC.    Aligned BAM  Aligned BAM files from STAR mapping.    Splice junctions  Splice junction information from STAR mapping.    Coverage files  bigWig ( .bw ) and BED ( .bed ) files detailing coverage.    Gene counts  Gene counts generated by HT-Seq count.    VOOM/LIMMA results  Pairwise comparisons of expression data. Requires at least 3 samples vs 3 samples.    Simple DE analysis  No statistical analysis, requires only a 1 samples vs 1 sample comparison.    MA/Volcano plots  Both of the above produce tabular outputs, MA plots and volcano plots.     Process   FastQ files generated by RNA-Seq are mapped to a reference genome using the STAR.  HT-Seq count is used to assign mapped reads to genes.   Differential expression analysis is performed using VOOM normalization of counts and\nLIMMA analysis.   Coverage plots of mapped reads are generated as interactive visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/warden/#getting-started", 
            "text": "To get started, you need to navigate to the  WARDEN tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/warden/#uploading-data", 
            "text": "The WARDEN Differential Expression analysis pipeline takes Gzipped FastQ\nfiles generated by an RNA-Seq experiment as input. You can upload your input \nfile(s) using the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/warden/#sample-sheet", 
            "text": "Once your data is uploaded, you'll need to create a sample sheet which\ndescribes the relationship between case and control samples,\nphenotype/condition information, and the comparisons you would like to\nperform. The sample sheet is a tab-delimited text document that can be\ncreated in Microsoft Excel (recommended) or a text editor.   Note  You will need to upload your sample sheet in a similar manner as your\nFastQ files, so you can follow the  same uploading instructions  \nto achieve this.", 
            "title": "Sample sheet"
        }, 
        {
            "location": "/guides/tools/warden/#prepare-using-microsoft-excel", 
            "text": "Tip  Download the  file_download  sample excel spreadsheet  as a starting\npoint!   The final product for the excel spreadsheet will look like the\nscreenshot below. If you create the sample sheet from scratch, please\nensure the the columns are  exactly  in this order.     Sample rows  Each row in the spreadsheet (except for the last row, which we will talk \nabout in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below:   Guidelines   The sample name should be unique and should only contain letters,\nnumbers and underscores.  The condition/phenotype column associates similar samples together.\nThe values should contain only letters, numbers and underscores.  ReadFile1 should contain forward reads (e.g.  *.R1.fastq.gz  or  *_1.fastq.gz ).  ReadFile2 will contain reads in reverse orientation to ReadFile2\n(e.g.  *.R2.fastq.gz  or  *_2.fastq.gz ).  For single end reads a single dash ('-') should be entered in the ReadFile2 column.    Comparison row  The last line in the sample sheet is called the \"comparison row\". This\nline specifies the comparisons to be done between conditions/phenotypes.\nAll pairwise combinations of the values in the \"Phenotype\" column can be \nanalyzed. To specify the comparisons, on a separate line, include  #comparisons=  followed be a comma delimited list of two conditions separated by a dash.    Example  The following lines are all valid examples.   #comparisons=KO-WT  #comparisons=Condition1-Control,Condition2-Control  #comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1     Note  If a comparison has at least 3 samples for each condition/phenotype,\nVOOM/LIMMA will be run. A simple differential comparison will be run on\nall samples.   Finalizing the sample sheet  To finalize the sample sheet, save the Microsoft Excel file with\nwhatever name you like. Save the file as an Excel Workbook with the\n.xlsx extension.", 
            "title": "Prepare using Microsoft Excel"
        }, 
        {
            "location": "/guides/tools/warden/#prepare-using-a-text-editor", 
            "text": "Tip  Download the  file_download  sample text file  as a starting\npoint!   Creating a sample sheet with a text editor is an option for advanced\nusers. The process of creating a sample sheet with a text editor is the same as\ncreating one with Microsoft Excel, with the small difference that you\nmust manually create your columns using the tab character. Save the file\nwith a .txt extension.", 
            "title": "Prepare using a text editor"
        }, 
        {
            "location": "/guides/tools/warden/#running-the-tool", 
            "text": "Note  The WARDEN tool operation is slightly different than the other pipelines\nbecause it accepts a variable number of samples.  First , you will run\na \"bootstrapping\" step that creates a custom executable for your\nanalysis.  Second , you will need to manually execute the generated\nworkflow from the first step. This allows us to take advantage of many\nnice features, like check-pointing and cost reduction. Don't worry, \nwe'll show you how to do this step by step below.   Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the  tool's landing page . You will be redirected to the virtual\ncloud workspace with the workflow screen opened for you.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/warden/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ files and sample sheet\n you uploaded in  the upload data section . \nClick the  FASTQ_FILES  input field and select  all  FastQ files.\nNext, click the  sampleList  input field and select the corresponding\nsamplesheet.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/warden/#selecting-parameters", 
            "text": "We now need to configure the parameters for the pipeline, such as reference\ngenome and sequencing method. You can access all of the available parameters \nby clicking on the  WARDEN WORKFLOW GENERATOR  substep.   Parameter setup steps   In the  Output Folder  field, select a folder to output to. You can\nstructure your experiments however you like (e.g.  /My_Outputs )  In the  analysisName  field, enter a prefix for all of the output files. This\ncan be any value you want to use to remember this run.  Be sure to use underscores\ninstead of spaces here!  Select the  sequenceStandedness  from the drop down menu. \nThis information can be determined from the sequencing or source \nof the data. If you don't know what to put here, select \"no\".  Select the  Genome  pulldown menu. Choose the appropriate box.  The LIMMA parameters can be left alone for most analyses. If you are\nan advanced LIMMA user, you can change the various settings exposed\nbelow the required parameters.  When all parameters have been set, press the save button.", 
            "title": "Selecting parameters"
        }, 
        {
            "location": "/guides/tools/warden/#starting-the-workflow", 
            "text": "Once your input files are hooked up and your parameters are set, \nyou should be able to start the workflow by clicking the \"Run as Analysis...\" \nbutton in the top right hand corner of the workflow dialog.   Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.   The tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\".   When the custom workflow has finished generating, the word 'Done' will\nappear in green in the status column. This indicates that the\nbootstrapping step has completed successfully.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/warden/#custom-workflow-process", 
            "text": "Wait for the workflow generator to finish.   Click on the WARDEN name in the name column.    You will now be on a page specific to the running of the workflow.\n    On the left side, you will see the inputs you selected for the\n    workflow generator. On the right side are the output files\n    (including the generated workflow). Select the generated workflow as\n    shown in the picture below.     You will now be within the output folder you specified earlier.\n    Select the file that begins with 'WARDEN WORKFLOW:'     A workflow generated for your data will be presented to you. Select\n    'Run as analysis' in the upper right.     The workflow will initiate, and you will be brought to the 'Monitor'\n    page. (Note to get back to this page, you can select 'Monitor' on\n    one of the menu bars near the top ) Expand the the workflow progress\n    be selecting the '+' sign next to 'In Progress'     As parts of the pipeline are run, you will see different tasks in\n    different colors. Green means done, blue is running, orange is\n    waiting, and red means error.     When done the status will be shown as 'Done'. Select the Workflow\n    name under Status.     You will be brought to a page that show more information about the\n    workflow analysis. Click on the output folder to go to the output.     The output folders will now be shown.     For a description of the output, please refer to  Navigating Results .", 
            "title": "Custom Workflow Process"
        }, 
        {
            "location": "/guides/tools/warden/#monitoring-run-progress", 
            "text": "Once you have started one or more WARDEN runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kick off\ngets one row in the Monitor section.  You can click the \"+\" on any of the runs to check \nthe status of individual steps of the pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.   Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/warden/#analysis-of-results", 
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/warden/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/warden/#navigating-results", 
            "text": "Note  Navigating to the raw results of your runs is the same for all\npipelines. This guide will feature the  rapid-rnaseq  pipeline, but you can follow along for\nany tool.", 
            "title": "Navigating Results"
        }, 
        {
            "location": "/guides/tools/warden/#raw-result-files", 
            "text": "Navigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is here ). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"View Results Files\".   You should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.", 
            "title": "Raw result files"
        }, 
        {
            "location": "/guides/tools/warden/#custom-visualization-results", 
            "text": "Navigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is here ). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"Visualize Results\".   You should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.", 
            "title": "Custom visualization results"
        }, 
        {
            "location": "/guides/tools/warden/#interpreting-results", 
            "text": "The complete output file specification is listed in the  overview section \nof this guide. Here, we will discuss each of the different output files in more detail.   Predicted gene fusions : The putative gene fusions will be\n  contained in the file  [SAMPLE].final_fusions.txt . This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.      Field Name  Description      sample  Sample name    gene*  Gene name    chr*  Chromosome name    pos*  Genomic Location    ort*  Strand    reads*  Supporting reads    medal  Estimated pathogenicity assessment using St. Jude Medal Ceremony      Coverage file : A standard\n     bigWig  file\n    used to describe genomic read coverage.  Splice junction read counts : A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.      Field Name  Description      junction  Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in  .bed  output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.    count  Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.    type  Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).    genes  Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.    transcripts  List of known transcript IDs matching the junction.    qc_flanking  Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).    qc_plus  Count of supporting reads aligned to the + strand.    qc_minus  Count of supporting reads aligned to the - strand.    qc_perfect_reads  Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).    qc_clean_reads  Count of supporting reads whose alignments are not perfect but which have a ratio of  = 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/warden/#primary-results", 
            "text": "", 
            "title": "Primary Results"
        }, 
        {
            "location": "/guides/tools/warden/#alignment-statistics", 
            "text": "Several files should be examined initially to determine the quality of\nthe results.  alignmentStatistics.txt  shows alignment statistics for\nall samples. This file is a plain text tab-delimited file that can be\nopened in Excel or a text editor such as Notepad++. This file contains\ninformation on the total reads per sample, the percantage of duplicate\nreads and the percentage of mapped reads. An example of this file is\nbelow. (Within the DNAnexus output directory structure, these files will\nbe in the COMBINED_FLAGSTAT directory.)", 
            "title": "Alignment statistics"
        }, 
        {
            "location": "/guides/tools/warden/#multidimensional-scaling-mds-plot", 
            "text": "The second set of files to look at are the Multidimensional scaling\n(MDS) plots ( https://en.wikipedia.org/wiki/Multidimensional_scaling )\nusing the plotMDS function within LIMMA. Similar to PCA, these graphs\nwill show how similar samples are to each other. There are different\nsets of MDS plots. For comparisons where there are 3 or more samples per\ncondition, an MDS plot using Voom (Limma) normalized values are\ngenerated. An example can be seen below. These files will be labeled mdsPlot.png . For all comparisons, regardless of sample size, and MDS\nplot will also be generated with Counts per million (CPM) normalized\ngene counts. These files will be labeled  mdsPlot.normCPM.png .\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)   MDS plot from just CPM normalized data.", 
            "title": "Multidimensional scaling (MDS) Plot"
        }, 
        {
            "location": "/guides/tools/warden/#proteinpaint-visualizations", 
            "text": "Several files on DNAnexus allow the data to be viewed in the Protein\nPaint viewer. (Note: We plan to have links downloaded in the future to\nallow the viewing of these files off of DNAnexus.)  LIMMA differential expression viewer  Within LIMMA/VIEWERS direcory (note if no comparisons meet the 3 sample\ncondition, the LIMMA folder will not exist), there will be a viewer file\nfor each valid comparison ( * results. .txt.viewer**). Simply select\nthe file and press 'Launch viewer' in the lower right. A viewer will pop\nup showing both the MA Plot and Volcano plot. By moving the mouse over a\ncircle, the circle will hilight and the corresponding gene on the other\ngraph will also hilight. Additional information about the gene and its\nexpression values will also be shown. One can also type in multiple gene\nsymbols in the provided text box. By pressing 'Show gene labels' all\nthese genes will show up on the plots.   Simple differential expression viewer  There will also be a viewer for the simple differential expresssion\nanalysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all\nbeen set to 1, so the volcano plot will not be relevant.  bigWig viewer  In the BIGWIG_VIEWER directory there will be a bigwigViewer file.\nSelect this file and then 'Launch viewer'. A graph of coverage for the\ngenome should be visible.", 
            "title": "ProteinPaint Visualizations"
        }, 
        {
            "location": "/guides/tools/warden/#secondary-results", 
            "text": "Interactive MA/Volcano Plots  In addition to viewing the MA and volcano plots through the visualization tool   Differential expression results    Other useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition,  results.*.txt  will be produced.   GSEA.input.txt  and  GSEA.tStat.txt    Input files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram.\n- (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)    For plain text results from the simple differential expresison analysis, the files will be named  simpleDE.*.txt . (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)    Prelabelled MA and volcano plots are provided for the analysis. These files are labeled  maPlot.*.png  and  volcanoPlot.*.png  where  *  is the comparison (e.g. ko_vs_wt)    The MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An exmaple MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)     The volcano plot shows the Log2Fold change between the conditions on the\nX-axis, and the -Log10 of the multiple testing corrected P-value on the\nY-axis.   An MA plot is generated for all comparisons regardless of number of\nsamples. This is the  simpleDEPlot.*.png  no statistics are shown\nand genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)   Differential analysis input  Inputs and commands are provided for rerunning differential expression\nanalysis on ones own computer. The R commands used for the analysis are\nfound in  voomLimma.R . An experienced R user can rerun the analysis\nwith any desired changes. This analysis requires the input countFile.txt  which contains counts per genes, the Rparameters.txt  file containing input parameters, and a processed\nsample list file  sampleList.txt  (Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)  The input for the simple differential analysis expression will be Rparameters_simple.txt ,  simpleDE.R ,  countFile.txt  and sampleList.txt .  countFile.txt  and  sampleList.txt  are the\nsame files used by the LIMMA analysis.  (Within the DNAnexus output directory structure, these files will be in\nthe SIMPLE_DIFEX directory.)   Coverage results  bigWig files will be generated for use in genome browsers (such as IGV http://software.broadinstitute.org/software/igv/ ). For each smaple,\nmultiple bigWig files will be found. For all types of sequencing\nstrandedness, there will be bigWig files labelled, *.sortedCoverageFile.bed.bw  where ' ' is the sample name. For\nstranded data there will also be*.sortedPosCoverageFile.bed.bw * and *.sortedNegCoverageFile.bed.bw  which contains coverage information\nfor the positive and negative strand of the genome.  (Within the DNAnexus output directory structure, these files will be in\nthe BIGWIG directory.)   Quality Control Results (FastQC)  Within the FastQC directory, foreach sample and read direction there\nwill be an html file and a zip file ( *.FastQc.html  *.FastQc.zip  where '*' is the base FastQ name), containing results\nfrom FastQTC. For the average user the html file is sufficient. This\nfile can give some basic statistics on the quality of the data.  (Within the DNAnexus output directory structure, these files will be in\nthe FastQC directory.)   BAM alignment files  There are two BAM files generated per sample that contain mapping\ninformation for all reads. The first is labeled *.Aligned.sortedByCoord.dup.bam  where ' ' is the sample name. The\nBAM file is sorted by coordinates and has duplicates marked. The second\nfile is*.Aligned.toTranscriptome.out.bam * and contains reads mapped\nto transcripts.  (Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)   Chimeric reads and junction files  Additional files created by STAR are provided. More information on these\nfiles can be found at http://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.posix/doc/STARmanual.pdf . *.SJ.out.tab  contain splice junction information. Fusion detectino\nfiles are labelled  *.Chimeric.out.bam  and *.Chimeric.out.junction .  (Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)   FPKM and count files (per sample)  Per sample files containing FPKM and raw count values for each gene can\nbe found in  *.fpkm.txt  and  *.htseq_counts.txt  where '*' is\nthe sample name.  (Within the DNAnexus output directory structure, these files will be in\nthe  NOTE FIND OUT WHICH DIRECTORY  directory.)", 
            "title": "Secondary Results"
        }, 
        {
            "location": "/guides/tools/warden/#methods-files", 
            "text": "A more human readable explanation is found in  methods.docx . Detailed\ndocumentation can be found in  methods.txt  (Within the DNAnexus output directory structure, these files will be in\nthe METHODS directory.)", 
            "title": "Methods Files"
        }, 
        {
            "location": "/guides/tools/warden/#auxilary-files", 
            "text": "This section describes the files that exist within the DNAnexus output\nfolder. Most of these files will not be of interest to the average user.\nHowever, interactive viewers are describe in  LIMMA differential\nexpression viewer  and  Simple\ndifferential expression viewer .  The output will be divided into multiple folders. The results being the\nmost useful will be the differential expression analysis results in the\nLIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage\nwill be in the BIGWIG folder. Other folder contain different types of\ndata and are explained in further detail below.     The following description of files is sorted by their output directory.   ALIGN  This directory contains the BAM files described in  BAM alignment\nfiles  and the chimeric and junction files are\ndescribed in  Chimeric reads and junction\nfiles . In adition there are 2 log\nfiles.  *Log.final.out  has relevant statistics for the alignemnt.\nThe  *.Log.out  file just contains a log of the analysis run,\nincluding input parameters. Per sample FLAGSTAT results are found in *.flagStatOut.txt . These flagstat files are combined into the file alignmentStatistics.txt  described in  Initial analysis of\nresults . Finally the ALIGN directory has\nmultiple  .starAlign.methods.txt files. These files can be ignored as\nthey are summarized in the finalmethods.docx * and  methods.txt \nfiles described in  Methods Files .   BIGWIG  All of the files here are described in section  Coverage\nresults . The  bgToBw.methods.txt  files can be\nignored as they are summarized in the files described in  Methods\nFiles .   BIGWIG_VIEWER  See  bigWig viewer   COMBINED_FLAGSTAT   Todo  Describe combined flagstat.    COMBINED_HTSEQ  Used for input in differential expression analysis. The\ncombineCountFile.txt is the same as countFile.txt described in Differential analysis input   COVERAGE  BED graph files used to generate bigWig files are here.   FastQC  See  Quality Control Results (FastQC)   HTSEQ  Per-sample HTSEQ-count results ( *.htseq_counts.txt ) and FPKM\nresults ( *.fpkm.txt ). Temporary methods files are found as\n*.htseq-count.methods.txt   LIMMA  mdsPlot.png ,  maPlot. .png,volcanoPlot. .png  are described in Initial analysis of results  results. .txt,GSEA.input. .txt  and * GSEA.tStat. .txt** are\ndescribe in  Differential expression\nresults  voomLimma.R ,  countFile.txt ,  Rparameters.txt , and sampleList.txt  are described in  Differential analysis\ninput  See  LIMMA differential expression\nviewer  for a description of the\nVIEWERS directory.  Other files in the LIMMA directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nlimmaMethods.txt is an intermediate file describing methods. Out.tar.gz\nis used for testing purposes. The sessionInfo.txt file describe the R\nsession working parameters and modules loaded. meanVariance.png is a\nplot for assessing quality of count data\n( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29 )   METHODS  The files here are described in  Methods Files .   SAMPLELIST  These files are used internally by the pipeline.   SIMPLE_DIFEX  mdsPlot.normCPM.png  and * simpleDEPlot. .png** are described in Initial analysis of results  * simpleDE. .txt** are describe in  Differential expression\nresults  simpleDE.R ,  countFile.txt ,  Rparameters_simple.txt , and sampleList.txt  are described in  Differential analysis\ninput  See  Simple differential expression\nviewer  for a description of the\nVIEWERS directory.  Other files in the SIMPLE_DIFEX directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nsimpleDifEx.methods.txt is an intermediate file describing methods.\nOut.tar.gz is used for testing purposes. The sessionInfo.txt file\ndescribe the R session working parameters and modules loaded.", 
            "title": "Auxilary Files"
        }, 
        {
            "location": "/guides/tools/warden/#known-issues", 
            "text": "Adapter contamination  This pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.    High coverage regions  Certain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.    Interactive Visualizations Exon vs Intron Nomenclature  When a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/guides/tools/warden/#frequently-asked-questions", 
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact\nform .", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/", 
            "text": "Authors\n\n\nScott Newman, Michael Macias\n\n\n\n\n\n\nPublication\n\n\nMutational Signatures employs MutationalPatterns: \"\nMutationalPatterns: comprehensive genome-wide analysis of mutational processes.\n\"\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nMutational Signatures\n finds and quantifies COSMIC mutational signatures\nacross samples. This is done by finding the optimal non-negative linear\ncombination of mutation signatures to reconstruct a mutation matrix. It\nbuilds the initial mutation matrix from multiple single-sample VCFs and, by\ndefault, fits it to \nmutational signatures from COSMIC\n. Mutational\nSignatures employs \nMutationalPatterns\n (\nBlokzijl, et al. (2018)\n) to\nachieve this.\n\n\nMutational Signatures supports both hg19 (GRCh37) and hg38 (GRCh38).\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVCF(s)\n\n\nArray of files\n\n\nList of VCF inputs. Can be single-sample or multi-sample and uncompressed or gzipped.\n\n\n[\n*.vcf\n, \n*.vcf.gz\n]\n\n\n\n\n\n\nSample sheet\n\n\nFile\n\n\nTab-delimited file (no headers) with sample ID and tag pairs [optional]\n\n\n*.txt\n\n\n\n\n\n\nGenome build\n\n\nString\n\n\nGenome build used as reference. Can be either \"GRCh37\" or \"GRCh38\". [default: \"GRCh38\"]\n\n\nGRCh38\n\n\n\n\n\n\nMinimum mutation burden\n\n\nInteger\n\n\nMinimum number of somatic SNVs a sample must have to be considered for analysis [default: 9]\n\n\n15\n\n\n\n\n\n\nMinimum signature contribution\n\n\nInteger\n\n\nMinimum number of mutations attributable to a single signature [default: 9]\n\n\n100\n\n\n\n\n\n\nOutput prefix\n\n\nString\n\n\nPrefix to append to output filenames [optional]\n\n\nmtsg\n\n\n\n\n\n\nDisabled VCF column\n\n\nInteger\n\n\nVCF column (starting from sample names, zero-based) to ignore when reading VCFS [optional]\n\n\n1\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nRaw signatures\n\n\nFile\n\n\nTab-delimited file of the raw results with sample contributions for each signature\n\n\n\n\n\n\nSignatures visualization\n\n\nFile\n\n\nHTML file for interactive plotting\n\n\n\n\n\n\nSample sheet\n\n\nFile\n\n\nTab-delimited file (no headers) with sample ID and tag pairs\n\n\n\n\n\n\n\n\nProcess\n\n\n\nMutational Signatures runs four steps using subcommands of \nmtsg\n.\n\n\n\n\nSplit VCFs (single or multi-sample) to multiple single-sample VCFs.\n\n\nIf not given, generate a sample sheet from the directory of single-sample\n     VCFs.\n\n\nBuild a mutation matrix and reconstruct/fit it using COSMIC mutation\n     signatures.\n\n\nCreate a visualization file using the fitted signatures.\n\n\n\n\nGetting started\n\n\nAfter logging in, click the \"Start\" button on the \nMutational Signatures tool\npage\n. This creates a new DNAnexus project and imports the tool.\n\n\nWith subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.\n\n\nInput configuration\n\n\nMutational Signatures only requires VCFs as inputs. This can be a single\nmulti-sample VCF, multiple single-sample VCFs, or a combination of both. All\nother inputs are optional.\n\n\nInput files can be uploaded via the \ndata transfer application\n or \ncommand\nline\n.\n\n\nVCF(s)\n\n\n\nVCF(s)\n is a list of VCF inputs. The inputs can be single-sample or\nmulti-sample and uncompressed or gzipped. Sample names are taken from the VCF\nheader.\n\n\nWhen using multi-sample VCFs, empty cells/absent variant calls must be\ndenoted with \n.:.\n.\n\n\ngVCFs are not supported.\n\n\nSample sheet\n\n\n\nSample sheet\n is a tab-delimited file (no headers) with two columns: the\nsample ID and a tag. The tag is an arbitrary identifier used to group the\nsamples, typically a disease abbreviation or tissue of origin.\n\n\nIf not given, a sample sheet will be generated automatically.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSJACT001_D\n\n\nACT\n\n\n\n\n\n\nSJACT002_D\n\n\nACT\n\n\n\n\n\n\nSJBALL063_D\n\n\nBALL\n\n\n\n\n\n\nSJHGG017_D\n\n\nHGG\n\n\n\n\n\n\n\n\nOutput prefix\n\n\n\nOutput prefix\n is the prefix to append to the output filenames. By default,\nif a single input VCF is given, its basename is used as the output prefix. If\nmultiple input VCFs are given, a default \"mtsg\" prefix is used. This behavior\ncan be overridden by a user-defined prefix.\n\n\nExample\n\n\n\n\n\n\n\n\n\nVCF(s)\n\n\nPrefix\n\n\nOutput filename for raw signatures\n\n\n\n\n\n\n\n\n\n\n[\npcgp.b38.refseq.goodbad.vcf\n]\n\n\npcgp.b38.refseq.goodbad\n\n\npcgp.b38.refseq.goodbad.signatures.txt\n\n\n\n\n\n\n[\nSJOS013_D.vcf\n, \nSJRHB007_D.vcf\n]\n\n\nmtsg\n\n\nmtsg.signatures.txt\n\n\n\n\n\n\n\n\nDisabled VCF column\n\n\n\nDisabled VCF column\n is the column index to ignore when reading VCFs. This\nis useful when the inputs are tumor-normal VCFs, and one column should be\nignored. Otherwise, the results would likely be duplicated.\n\n\nThe argument is a zero-based index relative to the sample names in the header\nof the VCF. For example, in a VCF with samples \nSJEPD003_D\n and \nSJEPD003_G\n,\nthe germline sample (\nSJEPD003_G\n) can be discarded by setting the \ndisabled\nVCF column\n to \n1\n.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n\n\n\n\n#CHROM\n\n\nPOS\n\n\nID\n\n\nREF\n\n\nALT\n\n\nQUAL\n\n\nFILTER\n\n\nINFO\n\n\nFORMAT\n\n\nSJEPD003_D\n\n\nSJEPD003_G\n\n\n\n\n\n\n\n\nUploading data\n\n\nMutational Signatures requires at least one VCF and an optional sample sheet\nto be uploaded. These files can be uploaded via the \ndata transfer\napplication\n or \ncommand line\n.\n\n\nAnalysis of results\n\n\nUpon a successful run of Mutational Signatures, three files are saved to the\nresults directory: raw signature contributions, a visualization file, and a\nsample sheet.\n\n\nInterpreting results\n\n\nRaw signatures\n\n\n\nRaw signatures\n is a tab-delimited file of the raw results with sample\ncontributions for each signature. Column 1 is the sample name, columns\n2-(N-1) are the COSMIC signatures contribution counts, and column N is the\ngroup tag, where N is the total number of columns. The number of columns is\nvariable since if the signature has no contributions for all samples, it is\ncompletely omitted.\n\n\nNote that the last column \ntissue\n is a misnomer. It aligns to the arbitrary\ntag given in the \nsample sheet\n.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nSignature.1\n\n\nSignature.2\n\n\n\u2026\n\n\nSignature.30\n\n\ntissue\n\n\n\n\n\n\n\n\n\n\nSJACT001_D\n\n\n1.71758029\n\n\n131.033723\n\n\n\u2026\n\n\n18.6910151\n\n\nACT\n\n\n\n\n\n\nSJAMLM7005_D\n\n\n51.9627312\n\n\n7.10850351\n\n\n\u2026\n\n\n0\n\n\nAMLM7\n\n\n\n\n\n\n\n\nSignatures visualization\n\n\n\nSignatures visualization\n is an HTML file that can be used for interactive\nplotting.\n\n\nWhen opened in a web browser, a set of controls allows plotting various\nstacked bar charts: total contributions by signature, total contributions by\ntag, and total contributions by sample per tag. The total contributions can be\nstacked as absolute values or as a percentage of the total.\n\n\nSample sheet\n\n\n\nWhen no sample sheet is given as an input, one is generated automatically,\nbut it is not guaranteed the derived tags will be of any use. This generated\nsample sheet is given as an output in the case the tags need to be manually\nedited, and the job is resubmitted with it as an input.\n\n\nWhen a sample sheet is given as an input, the sample sheet output is a copy\nof the input.\n\n\nSee also the description for the input \nsample sheet\n.\n\n\nTroubleshooting\n\n\nTo troubleshoot a failed run of Mutational Signatures, check the job log for\ndetails.\n\n\nWrong genome build\n\n\n\nIf the \"Building mutation matrix\" step during \nrun\n fails, it is likely that\nthe selected genome build does not match the input VCF(s). Rerun the job with\na matching genome build.\n\n\nExample\n\n\n\nR\n:\n \nBuilding\n \nmutation\n \nmatrix\n \nfrom\n \n6\n \nVCFs\n\n\nR\n:\n \nError\n \nin\n \nmut_matrix\n(\nvcf_list\n \n=\n \nfiltered_vcfs\n,\n \nref_genome\n \n=\n \nref_genome\n)\n \n:\n\n\nR\n:\n   \nError\n \nin\n \n.\nCall2\n(\nsolve_user_SEW\n,\n \nrefwidths\n,\n \nstart\n,\n \nend\n,\n \nwidth\n,\n \ntranslate\n.\nnegative\n.\ncoord\n,\n  \n:\n\n\nR\n:\n   \nsolving\n \nrow\n \n526\n:\n \nallow.nonnarrowing\n \nis\n \nFALSE\n \nand\n \nthe\n \nsupplied\n \nstart\n \n(\n79440206\n)\n \nis\n \n \nrefwidth\n \n+\n \n1\n\n\n\n\n\n\nReferences\n\n\n\n\nBlokzijl F, Janssen R, van Boxtel R, Cuppen E (2018). \"MutationalPatterns:\n    comprehensive genome-wide analysis of mutational processes.\" \nGenome\n    Medicine\n. doi: \n10.1186/s13073-018-0539-0\n. PMID: \n29695279\n.", 
            "title": "Mutational Signatures"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#getting-started", 
            "text": "After logging in, click the \"Start\" button on the  Mutational Signatures tool\npage . This creates a new DNAnexus project and imports the tool.  With subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#input-configuration", 
            "text": "Mutational Signatures only requires VCFs as inputs. This can be a single\nmulti-sample VCF, multiple single-sample VCFs, or a combination of both. All\nother inputs are optional.  Input files can be uploaded via the  data transfer application  or  command\nline .", 
            "title": "Input configuration"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#uploading-data", 
            "text": "Mutational Signatures requires at least one VCF and an optional sample sheet\nto be uploaded. These files can be uploaded via the  data transfer\napplication  or  command line .", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#analysis-of-results", 
            "text": "Upon a successful run of Mutational Signatures, three files are saved to the\nresults directory: raw signature contributions, a visualization file, and a\nsample sheet.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#interpreting-results", 
            "text": "", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#troubleshooting", 
            "text": "To troubleshoot a failed run of Mutational Signatures, check the job log for\ndetails.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/guides/tools/mutational-signatures/#references", 
            "text": "Blokzijl F, Janssen R, van Boxtel R, Cuppen E (2018). \"MutationalPatterns:\n    comprehensive genome-wide analysis of mutational processes.\"  Genome\n    Medicine . doi:  10.1186/s13073-018-0539-0 . PMID:  29695279 .", 
            "title": "References"
        }, 
        {
            "location": "/guides/tools/cis-x/", 
            "text": "Warning\n\n\ncis-X is an upcoming St. Jude Cloud tool and is not yet publicly available.\nSee \ncis-X on St. Jude Research\n for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nYu Liu, Chunliang Li, Shuhong Shen\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nActivating regular variants usually cause the cis-activation of target genes.\nTo find cis-activated genes, allelic specific/imbalance expressions (ASE) and\noutlier high expression (OHE) signals are used. Variants in the same\ntopologically associated domains with the candidates can then be searched,\nincluding structural variants (SV), copy number aberrations (CNA), and single\nnucleotide variations (SNV) and insertion/deletions (indel).\n\n\nA transcription factor binding analysis is also done, using motifs from\n\nHOCOMOCO\n v10 models.\n\n\ncis-X currently only works with hg19 (GRCh37).\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nSample ID\n\n\nString\n\n\nThe ID of the input sample\n\n\nSJALL018373_D1\n\n\n\n\n\n\nDisease subtype\n\n\nString\n\n\nThe disease name under analysis. Must be either TALL or AML.\n\n\nTALL\n\n\n\n\n\n\nSingle nucleotide variants\n\n\nFile\n\n\nTab-delimited file containing raw sequence variants\n\n\n*.txt\n\n\n\n\n\n\nCNV/LOH regions\n\n\nFile\n\n\nTab-delimited file containing any aneuploidy region existing in the tumor genome under analysis\n\n\n*.txt\n\n\n\n\n\n\nRNA-seq BAM\n\n\nFile\n\n\nBAM file aligned to hg19 (GRCh37)\n\n\n*.bam\n\n\n\n\n\n\nRNA-seq BAM index\n\n\nFile\n\n\nBAM index for the given BAM\n\n\n*.bam.bai\n\n\n\n\n\n\nGene expression table\n\n\nFile\n\n\nTab-delimited file containing gene level expressions for the tumor under analysis in FPKM\n\n\n*.txt\n\n\n\n\n\n\nSomatic SNV/indels\n\n\nFile\n\n\nTab-delimited file containing somatic SNV/indels in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nSomatic SVs\n\n\nFile\n\n\nTab-delimited file containing somatic acquired structural variants in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nSomatic CNVs\n\n\nFile\n\n\nTab-delimited file containing copy number aberrations in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nCNV/LOH action\n\n\nString\n\n\nThe behavior when handling markers in CNV/LOH regions. Can be either \nkeep\n or \ndrop\n.\n\n\ndrop\n\n\n\n\n\n\nMinimum coverage for WGS\n\n\nInteger\n\n\nThe minimum coverage in WGS to be included in the analysis\n\n\n10\n\n\n\n\n\n\nMinimum coverage for RNA-seq\n\n\nInteger\n\n\nThe minimum coverage in RNA-seq to be included in the analysis\n\n\n5\n\n\n\n\n\n\nCandidate FPKM threshold\n\n\nFloat\n\n\nThe FPKM threshold for the nomination of a cis-activated candidate\n\n\n0.1\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncis-activated candidates\n\n\ncis-activated candidates in the tumor genome under analysis\n\n\n\n\n\n\nSV candidates\n\n\nStructural variant (SV) candidates predicted as the causal for the cis-activated genes in the regulatory territory\n\n\n\n\n\n\nCNA candidates\n\n\nCopy number aberrations (CNA) predicted as the causal for the cis-activated genes in the regulatory territory\n\n\n\n\n\n\nSNV/indel candidates\n\n\nSNV/indel candidates predicted as functional and predicted transcription factors\n\n\n\n\n\n\nOHE results\n\n\nRaw outlier high expression (OHE) results\n\n\n\n\n\n\nGene level ASE results\n\n\nRaw gene level allelic specific expression (ASE) results\n\n\n\n\n\n\nSingle marker ASE results\n\n\nRaw single marker allelic specific expression (ASE) results\n\n\n\n\n\n\n\n\nGetting started\n\n\nAfter logging in, click the \"Start\" button on the \ncis-X tool page\n. This\ncreates a new DNAnexus project and imports the tool.\n\n\nWith subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.\n\n\nInput file configuration\n\n\ncis-X requires six tab-delimited input files to be prepared in advance. These\nfiles can be uploaded via the \ndata transfer application\n or \ncommand line\n.\n\n\n\n\nNote\n\n\nEven though CNV/LOH regions, somatic SNV/indels, somatic SVs, and\nsomatic CNVs can be \"empty\", using such inputs will produce results with a\nmuch higher false positive rate.\n\n\n\n\nSingle nucleotide variants\n\n\n\nA list of single nucleotide markers is a tab-delimited file with the\nfollowing columns:\n\n\n\n\nChr\n: chromosome name for the marker\n\n\nPos\n: genomic start location for the marker\n\n\nChr_Allele\n: reference allele\n\n\nAlternative_Allele\n: alternative allele\n\n\nreference_tumor_count\n: reference allele count in the tumor genome\n\n\nalternative_tumor_count\n: alterative allele count in the tumor genome\n\n\nreference_normal_count\n: reference allele count in the matched normal genome\n\n\nalternative_normal_count\n: alternative count in the matched normal genome\n\n\n\n\nThis file can be generated with Bambino.\n\n\nExample\n\n\n\n\n\n\n\n\n\nChr\n\n\nPos\n\n\nChr_Allele\n\n\nAlternative_Allele\n\n\nreference_tumor_count\n\n\nalternative_tumor_count\n\n\nreference_normal_count\n\n\nalternative_normal_count\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n61396\n\n\nTT\n\n\n\n\n0\n\n\n3\n\n\n0\n\n\n10\n\n\n\n\n\n\nchr11\n\n\n72981\n\n\n\n\nT\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n\n\n\n\n\n\nCNV/LOH regions\n\n\n\nThe CNV/LOH regions are all the genomic regions carrying copy number\nvariations (CNV) or loss of heterozygosity (LOH), which will be filtered out\nduring analysis.\n\n\nThis is a tab-delimited file in the bed format. It must have at least the\nfollowing three columns:\n\n\n\n\nchrom\n: chromosome name\n\n\nloc.start\n: genomic start location\n\n\nloc.end\n: genomic end location\n\n\n\n\nIf no CNV/LOH are in the genome under analysis, a file with no rows (but\nincluding headers) can be provided.\n\n\nThis file can be generated with CONSERTING.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\nloc.start\n\n\nloc.end\n\n\nSample\n\n\nseg.mean\n\n\nLogRatio\n\n\nsource\n\n\n\n\n\n\n\n\n\n\nchr9\n\n\n10712\n\n\n37855747\n\n\nSJALL018373_D1\n\n\n0.471181417\n\n\n\n\nLOH\n\n\n\n\n\n\nchr9\n\n\n20276901\n\n\n20703900\n\n\nSJALL018373_D1\n\n\n-0.978\n\n\n-5.696\n\n\nCNV\n\n\n\n\n\n\n\n\nGene expression table\n\n\n\nThe gene expression table is a tab-delimited file containing gene level\nexpressions for the tumor under analysis. The expressions are in FPKM\n(fragments per kilobase of transcript per million mapped reads).\n\n\n\n\nGeneID\n: gene \nEnsembl\n ID\n\n\nGeneName\n: gene symbol\n\n\nType\n: \ntranscript type\n\n\nStatus\n: transcript status (must be \nKNOWN\n, \nNOVEL\n, or \nPUTATIVE\n)\n\n\nChr\n: chromosome name\n\n\nStart\n genomic start location\n\n\nEnd\n: genomic end location\n\n\n[SampleID...]: FPKM for the given sample\n\n\n\n\nThis file can be generated with the output of HTseq-count preprocessed\nthrough \nmergeData_geneName.pl\n (available with the distribution of cis-X).\nThe data must be able to match values in the given gene specific reference\nexpression matrices generated from a larger cohort.\n\n\nExample\n\n\n\n\n\n\n\n\n\nGeneID\n\n\nGeneName\n\n\nType\n\n\nStatus\n\n\nChr\n\n\nStart\n\n\nEnd\n\n\nSJALL018373_D1\n\n\n\n\n\n\n\n\n\n\nENSG00000261122.2\n\n\n5S_rRNA\n\n\nlincRNA\n\n\nNOVEL\n\n\nchr16\n\n\n34977639\n\n\n34990886\n\n\n0.0000\n\n\n\n\n\n\nENSG00000249352.3\n\n\n7SK\n\n\nlincRNA\n\n\nNOVEL\n\n\nchr5\n\n\n68266266\n\n\n68325992\n\n\n4.5937\n\n\n\n\n\n\n\n\nSomatic SNV/indels\n\n\n\nThis is a tab-delimited file containing somatic sequence mutations present in\nthe genome under analysis. It includes both single nucleotide variants (SNV)\nand small insertion/deletions (indel). The file must have the following\ncolumns:\n\n\n\n\nchr\n: chromosome name\n\n\npos\n: genomic start location\n\n\nref\n: reference nucleotide\n\n\nmutant\n: mutant nucleotide\n\n\ntype\n: mutation type (must be either \nsnv\n or \nindel\n)\n\n\n\n\nNote that the coordinate used for an indel is after the inserted sequence.\n\n\nIf no SNV/indels are in the sample under analysis, a file with no rows\n(but including headers) can be provided.\n\n\nThis file can can be created with Bambino and then preprocessed using the\nsteps taken in \"\nThe genetic basis of early T-cell precursor acute lymphoblastic leukaemia\n\".\n\n\nExample\n\n\n\n\n\n\n\n\n\nchr\n\n\npos\n\n\nref\n\n\nmut\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nchr1\n\n\n24782720\n\n\nG\n\n\nA\n\n\nsnv\n\n\n\n\n\n\nchr11\n\n\n82896176\n\n\nT\n\n\nC\n\n\nsnv\n\n\n\n\n\n\n\n\nSomatic SVs\n\n\n\nThis is a tab-delimited file containing somatic-acquired structural variants\n(SV) in the cancer genome. The file must have the following columns:\n\n\n\n\nchrA\n: chromosome name of the left breakpoint\n\n\nposA\n: genomic location of the left breakpoint\n\n\nortA\n: strand orientation of the left breakpoint\n\n\nchrB\n: chromosome name of the right breakpoint\n\n\nposB\n: genomic location of the right breakpoint\n\n\nortB\n: strand orientation of the right breakpoint\n\n\n\n\nStrand orientations are denoted with a \n+\n for a sense or coding strand and\n\n-\n for a antisense or non-coding strand.\n\n\nIf no somatic SVs are in the sample under analysis, a file with no rows (but\nincluding headers) can be provided.\n\n\nThis file can be generated by CREST.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrA\n\n\nposA\n\n\nortA\n\n\nchrB\n\n\nposB\n\n\nortB\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n33913169\n\n\n+\n\n\nchr7\n\n\n142494049\n\n\n-\n\n\nCTX\n\n\n\n\n\n\nchr11\n\n\n64219334\n\n\n+\n\n\nchr2\n\n\n205042527\n\n\n-\n\n\nCTX\n\n\n\n\n\n\n\n\nSomatic CNVs\n\n\n\nThis is a tab-delimited file containing the genomic\nregions with somatic-acquired copy number aberrations (CNA) in the cancer\ngenome.\n\n\n\n\nchr\n: chromosome name\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\nlogR\n: log2 ratio\n\n\n\n\nIf no somatic CNVs are in the sample under analysis, a file with no rows\n(but including headers) can be provided.\n\n\nThis file can be generating by CONSERTING.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchr\n\n\nstart\n\n\nend\n\n\nlogR\n\n\n\n\n\n\n\n\n\n\nchr9\n\n\n20276901\n\n\n20703900\n\n\n-5.696\n\n\n\n\n\n\n\n\nUploading data\n\n\ncis-X requires a total of eight files to be uploaded, as described in \"\nInput\nfile configuration\n\". These files can be uploaded via the \ndata transfer\napplication\n or \ncommand line\n.\n\n\nRunning the tool\n\n\n\n\nTodo\n\n\n\n\nMonitoring run progress\n\n\n\n\nTodo\n\n\n\n\nAnalysis of results\n\n\nUpon a successful run of cis-X, seven tab-delimited files are saved to the\nresults directory. These raw results can be processed through external tools\nfor further analysis.\n\n\nInterpreting results\n\n\ncis-activated candidates\n\n\n\nThe main result file contains the cis-activated candidates in the tumor genome\nunder analysis.\n\n\n\n\ngene\n: gene accession number (\nRefSeq\n ID)\n\n\ngsym\n: gene symbol\n\n\nchrom\n: chromosome name\n\n\nstrand\n: strand orientation\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\ncdsStartStat\n: coding sequence (CDS) start status\n\n\ncdsEndStat\n: coding sequence (CDS) end status\n\n\nmarkers\n: number of heterozygous markers in this gene\n\n\nase_markers\n: number of heterozygous markers showing allelic specific expressions (ASE)\n\n\naverage_ai_all\n: average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers\n\n\naverage_ai_ase\n: average BAF difference between RNA and DNA for ASE markers\n\n\npval_all_markers\n: p-value for each marker in the ASE test\n\n\npval_ase_markers\n: p-value for ASE markers in the ASE test\n\n\nai_all_markers\n: BAF difference between RNA and DNA for all heterozygrous markers\n\n\nai_ase_markers\n: BAF difference between RNA and DNA for ASE markers\n\n\ncomb.pval\n: combined p-value for the ASE test\n\n\nmean.delta\n: average BAF difference between RNA and DNA for all markers\n\n\nrawp\n: raw p-value for the ASE test\n\n\nBonferroni\n: adjusted p-value for the ASE test (single-step Bonferroni)\n\n\nABH\n: adjusted p-value for the ASE test (Benjamini-Hochberg)\n\n\nFPKM\n: FPKM value\n\n\nloo.source\n: which reference expression matrix was used in the outlier high expression (OHE) test\n\n\nloo.cohort.size\n: number of cases in the reference expression matrix for this gene\n\n\nloo.pval\n: p-value of the OHE test\n\n\nloo.rank\n: rank of the case under analysis among the reference cases\n\n\nimprinting.status\n: imprinting status of the gene\n\n\ncandidate.group\n: status of the gene, combining both ASE and outlier tests\n\n\n\n\nStrand orientations are denoted with a \n+\n for a sense or coding strand and\n\n-\n for a antisense or non-coding strand.\n\n\nCoding sequence status is typically one of \"none\" (not specified), \"unk\"\n(unknown), \"incmpl\" (incomplete), or \"cmpl\" (complete).\n\n\nExample\n\n\n\n\n\n\n\n\n\ngene\n\n\ngsym\n\n\nchrom\n\n\nstrand\n\n\nstart\n\n\nend\n\n\ncdsStartStat\n\n\ncdsEndStat\n\n\nmarkers\n\n\nase_markers\n\n\naverage_ai_all\n\n\naverage_ai_ase\n\n\npval_all_markers\n\n\npval_ase_markers\n\n\nai_all_markers\n\n\nai_ase_markers\n\n\ncomb.pval\n\n\nmean.delta\n\n\nrawp\n\n\nBonferroni\n\n\nABH\n\n\nFPKM\n\n\nloo.source\n\n\nloo.cohort.size\n\n\nloo.pval\n\n\nloo.rank\n\n\nimprinting.status\n\n\ncandidate.group\n\n\n\n\n\n\n\n\n\n\nNM_145804\n\n\nABTB2\n\n\nchr11\n\n\n-\n\n\n34172533\n\n\n34379555\n\n\ncmpl\n\n\ncmpl\n\n\n5\n\n\n5\n\n\n0.5\n\n\n0.500\n\n\n0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625\n\n\n0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625\n\n\n0.5,0.5,0.5,0.5,0.5\n\n\n0.5,0.5,0.5,0.5,0.5\n\n\n0.000644290972057077\n\n\n0.5\n\n\n0.000644290972057077\n\n\n0.632049443587993\n\n\n0.0110866672927557\n\n\n7.6776\n\n\nbi_cohort\n\n\n40\n\n\n0.0367241086505276\n\n\n1\n\n\n\n\nase_outlier\n\n\n\n\n\n\nNM_003189\n\n\nTAL1\n\n\nchr1\n\n\n-\n\n\n47681961\n\n\n47698007\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n2\n\n\n0.482\n\n\n0.482\n\n\n6.66361745922277e-28,3.30872245021211e-24\n\n\n6.66361745922277e-28,3.30872245021211e-24\n\n\n0.464912280701754,0.5\n\n\n0.464912280701754,0.5\n\n\n4.69553625126628e-26\n\n\n0.482456140350877\n\n\n4.69553625126628e-26\n\n\n4.60632106249222e-23\n\n\n6.11761294450693e-24\n\n\n8.8168\n\n\nwhite_list\n\n\n167\n\n\n0.0139385771987089\n\n\n1\n\n\n\n\nase_outlier\n\n\n\n\n\n\n\n\nSV candidates\n\n\n\nStructural variant (SV) candidates include candidates predicted as the causal\nfor the cis-activated genes in the regulatory territory.\n\n\n\n\nleft.candidate.inTAD\n: cis-activated candidate near the left breakpoint\n\n\nright.candidate.inTAD\n: cis-activated candidate near the right breakpoint\n\n\nchrA\n: chromosome name of the left breakpoint\n\n\nposA\n: genomic location of the left breakpoint\n\n\nortA\n: strand orientation of the left breakpoint\n\n\nchrB\n: chromosome name of the right breakpoint\n\n\nposB\n: genomic location of the right breakpoint\n\n\nortB\n: strand orientation of the right breakpoint\n\n\ntype\n: type of translocation\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nleft.candidate.inTAD\n\n\nright.candidate.inTAD\n\n\nchrA\n\n\nposA\n\n\nortA\n\n\nchrB\n\n\nposB\n\n\nortB\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nLMO2\n\n\n\n\nchr11\n\n\n33913169\n\n\n+\n\n\nchr7\n\n\n142494049\n\n\n-\n\n\nCTX\n\n\n\n\n\n\n\n\nCNA candidates\n\n\n\nCopy number aberration (CNA) candidates include candidates predicted as the\ncausal for the cis-activated genes in the regulatory territory.\n\n\n\n\ncandidate.inTAD\n: cis-activated candidate by the CNA\n\n\nchr\n: chromosome name\n\n\nstart\n: genomic start position\n\n\nend\n: genomic end location\n\n\nlogR\n: log ratio of the CNA\n\n\n\n\nSNV/indel candidates\n\n\n\nSNV/indel candidates include predicted candidates as functional and predicted\ntranscription factors. The mutations are also annotated for known regulatory\nelements reported by the \nNIH Roadmap Epigenomics Project\n by collecting 111\ncell lines.\n\n\n\n\nchrom\n: chromosome name\n\n\npos\n: genomic start position\n\n\nref\n: reference allele genotype\n\n\nmut\n: mutant allele genotype\n\n\ntype\n: mutation type (either \nsnv\n or \nindel\n)\n\n\ntarget\n: cis-activated candidate\n\n\ndist\n: distance between the mutation and transcription start sites of the target gene\n\n\ntf\n: transcription factors predicted to have the binding motif introduced by the mutation\n\n\nEpiRoadmap_enhancer\n: enhancer regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\nEpiRoadmap_promoter\n: promoter regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\nEpiRoadmap_dyadic\n: dyadic regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\npos\n\n\nref\n\n\nmut\n\n\ntype\n\n\ntarget\n\n\ndist\n\n\ntf\n\n\nEpiRoadmap_enhancer\n\n\nEpiRoadmap_promoter\n\n\nEpiRoadmap_dyadic\n\n\n\n\n\n\n\n\n\n\nchr1\n\n\n47696311\n\n\nC\n\n\nT\n\n\nsnv\n\n\nTAL1\n\n\n1696\n\n\nBCL11A,CEBPG,PBX2,YY1,ZBTB4\n\n\n\n\nBrain,Digestive,ES-deriv,ESC,HSC \n B-cell,Heart,Muscle,Other,Sm. Muscle,iPSC\n\n\n\n\n\n\n\n\n\n\nOHE results\n\n\n\nOHE results are the raw results for the outlier expression test.\n\n\n\n\nGene\n: gene symbol\n\n\nfpkm.raw\n: FPKM value\n\n\nsize.bi\n: number of cases in the bi-allelic reference cohort\n\n\np.bi\n: p-value in the outlier test using the bi-allelic reference cohort\n\n\nrank.bi\n: rank of the expression level in the case under analysis compared to the bi-allelic reference cohort\n\n\nsize.cohort\n: number of cases in the entire reference cohort\n\n\np.cohort\n: p-value in the outlier test using the entire reference cohort\n\n\nrank.cohort\n: rank of the expression level in the case under analysis compared to the entire reference cohort\n\n\nsize.white\n: number of cases in the whitelist reference cohort\n\n\np.white\n: p-value in the outlier test using the whitelist reference cohort\n\n\nrank.white\n: rank of the expression level in the case under analysis compared to the whitelist reference cohort\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nGene\n\n\nfpkm.raw\n\n\nsize.bi\n\n\np.bi\n\n\nrank.bi\n\n\nsize.cohort\n\n\np.cohort\n\n\nrank.cohort\n\n\nsize.white\n\n\np.white\n\n\nrank.white\n\n\n\n\n\n\n\n\n\n\n7SK\n\n\n4.5937\n\n\nna\n\n\nna\n\n\nna\n\n\n264\n\n\n0.716284011918374\n\n\n162\n\n\nna\n\n\nna\n\n\nna\n\n\n\n\n\n\nA1BG\n\n\n0.2312\n\n\n24\n\n\n0.900132642257996\n\n\n21\n\n\n264\n\n\n0.84055666600945\n\n\n222\n\n\nna\n\n\nna\n\n\nna\n\n\n\n\n\n\n\n\nGene level ASE results\n\n\n\nGene level ASE results are the raw results from the gene level ASE test.\n\n\n\n\ngene\n: gene accession number (\nRefSeq\n ID)\n\n\ngsym\n: gene symbol\n\n\nchrom\n: chromosome name\n\n\nstrand\n: strand orientation\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\ncdsStartStat\n: coding sequence (CDS) start status\n\n\ncdsEndStat\n: coding sequence (CDS) end status\n\n\nmarkers\n: number of heterozygous markers in this gene\n\n\nase_markers\n: number of heterozygous markers showing allelic specific expressions (ASE)\n\n\naverage_ai_all\n: average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers\n\n\naverage_ai_ase\n: average BAF difference between RNA and DNA for ASE markers\n\n\npval_all_markers\n: p-value for each marker in the ASE test\n\n\npval_ase_markers\n: p-value for ASE markers in the ASE test\n\n\nai_all_markers\n: BAF difference between RNA and DNA for all heterozygrous markers\n\n\nai_ase_markers\n: BAF difference between RNA and DNA for ASE markers\n\n\ncomb.pval\n: combined p-value for the ASE test\n\n\nmean.delta\n: average BAF difference between RNA and DNA for all markers\n\n\nrawp\n: raw p-value for the ASE test\n\n\nBonferroni\n: adjusted p-value for the ASE test (single-step Bonferroni)\n\n\nABH\n: adjusted p-value for the ASE test (Benjamini-Hochberg)\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\ngene\n\n\ngsym\n\n\nchrom\n\n\nstrand\n\n\nstart\n\n\nend\n\n\ncdsStartStat\n\n\ncdsEndStat\n\n\nmarkers\n\n\nase_markers\n\n\naverage_ai_all\n\n\naverage_ai_ase\n\n\npval_all_markers\n\n\npval_ase_markers\n\n\nai_all_markers\n\n\nai_ase_markers\n\n\ncomb.pval\n\n\nmean.delta\n\n\nrawp\n\n\nBonferroni\n\n\nABH\n\n\n\n\n\n\n\n\n\n\nNM_024684\n\n\nAAMDC\n\n\nchr11\n\n\n+\n\n\n77532207\n\n\n77583398\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n0\n\n\n0.079\n\n\nna\n\n\n0.924775093657227,0.0331439677875056\n\n\nna\n\n\n0.00892857142857145,0.149122807017544\n\n\nna\n\n\n0.175073458624837\n\n\n0.0790256892230577\n\n\n0.175073458624837\n\n\n1\n\n\n0.480780882445856\n\n\n\n\n\n\nNM_015423\n\n\nAASDHPPT\n\n\nchr11\n\n\n+\n\n\n105948291\n\n\n105969419\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n0\n\n\n0.023\n\n\nna\n\n\n0.749258624760841,1\n\n\nna\n\n\n0.0384615384615384,0.00769230769230766\n\n\nna\n\n\n0.86559726476049\n\n\n0.023076923076923\n\n\n0.86559726476049\n\n\n1\n\n\n0.873257417545981\n\n\n\n\n\n\n\n\nSingle marker ASE results\n\n\n\nSingle marker ASE results are the raw results from the single marker ASE test.\n\n\n\n\nchrom\n: chromosome name\n\n\npos\n: genomic start position\n\n\nref\n: reference allele genotype\n\n\nmut\n: non-reference allele genotype\n\n\ncvg_wgs\n: coverage of the marker from the whole genome sequence (WGS)\n\n\nmut_freq_wgs\n: non-reference allele fraction in the WGS\n\n\ncvg_rna\n: coverage of the marker from the RNA-seq\n\n\nmut_freq_rna\n: non-reference allele fraction in the RNA-seq\n\n\nref.1\n: read count of the reference allele in the RNA-seq\n\n\nvar\n: read count of the non-reference allele in the RNA-seq\n\n\npvalue\n: p-value from the binomial test\n\n\ndelta.abs\n: absolute difference of the non-reference allele fraction between the WGS and RNA-seq\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\npos\n\n\nref\n\n\nmut\n\n\ncvg_wgs\n\n\nmut_freq_wgs\n\n\ncvg_rna\n\n\nmut_freq_rna\n\n\nref.1\n\n\nvar\n\n\npvalue\n\n\ndelta.abs\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n204147\n\n\nG\n\n\nA\n\n\n36\n\n\n0.472\n\n\n85\n\n\n0.553\n\n\n38\n\n\n47\n\n\n0.385669420119278\n\n\n0.0529411764705883\n\n\n\n\n\n\nchr11\n\n\n205198\n\n\nC\n\n\nA\n\n\n23\n\n\n0.522\n\n\n83\n\n\n0.313\n\n\n57\n\n\n26\n\n\n0.000877551780002863\n\n\n0.186746987951807", 
            "title": ""
        }, 
        {
            "location": "/guides/tools/cis-x/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/cis-x/#getting-started", 
            "text": "After logging in, click the \"Start\" button on the  cis-X tool page . This\ncreates a new DNAnexus project and imports the tool.  With subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/cis-x/#input-file-configuration", 
            "text": "cis-X requires six tab-delimited input files to be prepared in advance. These\nfiles can be uploaded via the  data transfer application  or  command line .   Note  Even though CNV/LOH regions, somatic SNV/indels, somatic SVs, and\nsomatic CNVs can be \"empty\", using such inputs will produce results with a\nmuch higher false positive rate.", 
            "title": "Input file configuration"
        }, 
        {
            "location": "/guides/tools/cis-x/#uploading-data", 
            "text": "cis-X requires a total of eight files to be uploaded, as described in \" Input\nfile configuration \". These files can be uploaded via the  data transfer\napplication  or  command line .", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/cis-x/#running-the-tool", 
            "text": "Todo", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/cis-x/#monitoring-run-progress", 
            "text": "Todo", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/cis-x/#analysis-of-results", 
            "text": "Upon a successful run of cis-X, seven tab-delimited files are saved to the\nresults directory. These raw results can be processed through external tools\nfor further analysis.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/cis-x/#interpreting-results", 
            "text": "", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/visualizations/bigwig-viewer/", 
            "text": "ProteinPaint BigWig Viewer\n\n\nThe ProteinPaint interactive coverage viewer is used to visualize any\nbigWig files information. You can follow these steps to get an\nunderstand of how it works.\n\n\n\n\nOpen up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.\n\n\n\n\nClick \"Launch\" in the bottom right corner to launch the custom\n    viewer.\n\n\n\n\n\n\n\n\nOnce the page has loaded, you will be able to see the bigWig viewer.\n    You can navigate around the genome by gene or genomic location.\n    Alongside the coverage track is the GENCODE gene reference.", 
            "title": "BigWig Viewer"
        }, 
        {
            "location": "/guides/visualizations/bigwig-viewer/#proteinpaint-bigwig-viewer", 
            "text": "The ProteinPaint interactive coverage viewer is used to visualize any\nbigWig files information. You can follow these steps to get an\nunderstand of how it works.   Open up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.   Click \"Launch\" in the bottom right corner to launch the custom\n    viewer.     Once the page has loaded, you will be able to see the bigWig viewer.\n    You can navigate around the genome by gene or genomic location.\n    Alongside the coverage track is the GENCODE gene reference.", 
            "title": "ProteinPaint BigWig Viewer"
        }, 
        {
            "location": "/guides/visualizations/fusion-viewer/", 
            "text": "ProteinPaint Fusion Viewer\n\n\nThe ProteinPaint interactive fusion viewer is used to visualize putative\nfusions called by CICERO. You can follow these steps to get an\nunderstand of how it works.\n\n\n\n\nOpen up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.\n\n\n\n\nClick \"Launch\" in the bottom right corner to launch the custom\n    viewer.\n\n\n\n\n\n\n\n\nOnce the page has finished loading, you will be presented with a\n    summary of all of the fusions produced by the pipeline. Each bullet\n    point is a separate category for the structural variants, with the\n    more interesting fusions at the top. Click one of the categories to\n    view the fusions in that category.\n\n\n\n\n\n\n\n\nYou can see all of the fusions in that category are now listed on\n    the screen. Hover over one of the fusions to see the detailed view.\n\n\n\n\n\n\n\n\nThe popup contains a large amount of information that might\n    interesting to you based on your use case, such as the transcript\n    and other metrics like read counts, quality metrics, and recurrence.", 
            "title": "Fusion Viewer"
        }, 
        {
            "location": "/guides/visualizations/fusion-viewer/#proteinpaint-fusion-viewer", 
            "text": "The ProteinPaint interactive fusion viewer is used to visualize putative\nfusions called by CICERO. You can follow these steps to get an\nunderstand of how it works.   Open up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.   Click \"Launch\" in the bottom right corner to launch the custom\n    viewer.     Once the page has finished loading, you will be presented with a\n    summary of all of the fusions produced by the pipeline. Each bullet\n    point is a separate category for the structural variants, with the\n    more interesting fusions at the top. Click one of the categories to\n    view the fusions in that category.     You can see all of the fusions in that category are now listed on\n    the screen. Hover over one of the fusions to see the detailed view.     The popup contains a large amount of information that might\n    interesting to you based on your use case, such as the transcript\n    and other metrics like read counts, quality metrics, and recurrence.", 
            "title": "ProteinPaint Fusion Viewer"
        }, 
        {
            "location": "/guides/portals/sickle-cell/", 
            "text": "The Sickle Cell Genomics Portal contains two viewers for the exploration of data from the  \nSickle Cell Genome Project (SGP)\n dataset.\n\n\nGenome Browser\n\n\nOverview\n\n\nUpon launching the browser, you will see an image similar to the one shown here.\n\n\n\nA description of the elements of the browser are as follows:\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNavigation tools and track selector. (\nSee Navigation Buttons section\n)\n\n\n\n\n\n\n2\n\n\nDNase hypersensitivity tracks.  By default, four epigenetic tracks are shown.  These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells.  Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below)\n\n\n\n\n\n\n3\n\n\nRefSeq genes.  Gene models from the RefSeq database are displayed in this tracks.\n\n\n\n\n\n\n4\n\n\n-log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease.  The analysis has only been performed around the  KIAA1109/Tenr/IL2/IL21 region.   Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)).  The Y-axis for the track represents the -log10 of the p-value.  The higher the value, the more stastically significant the association between the variant and pain rate is.  Clicking on a variant will open op a window that gives further details about the variant.  (See Figure 3).\n\n\n\n\n\n\n5\n\n\n-log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease.  See (4) above for more information on this type of track.\n\n\n\n\n\n\n6\n\n\nFilters: Filters allow variants within tracks to be filtered by numerous citeria.  \nSee Filter description\n\n\n\n\n\n\n\n\nNavigation buttons\n\n\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na\n\n\nLocation/Locus entry field.  One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID.\n\n\n\n\n\n\nb\n\n\nBrowser zoom in and out\n\n\n\n\n\n\nc\n\n\nTracks: Add or hide tracks (See section below on adding/hiding tracks)\n\n\n\n\n\n\nd\n\n\nMore:  Save svg image of browser, get DNA sequence or highlight regions of the browser.\n\n\n\n\n\n\n\n\nFilters\n\n\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na\n\n\nFilters for pain rate p-value track\n\n\n\n\n\n\nb\n\n\nFilters for age of first vaso-occlusive crisis (VOC) p-value\n\n\n\n\n\n\nc\n\n\nThe highlighted filter shows which value is  used for the Y-axis on the browser track.  The value can be changed.\n\n\n\n\n\n\nd\n\n\nA highligthed value within a filter shows which filter value is set.  The number next to the filter represents the number of individuals that meet the filter criteria.\n\n\n\n\n\n\n\n\nGetting Started\n\n\nFinding a variant of interest\n\n\nA user can navigate to a gene or to a variant ID.\nEnter in the variant ID rs13140464 into the search text field at the top of the browser. (See below)\n\n\n\nPressing enter will center the browser of the selected variant.  (see below)\n\n\n\nZooming in and out\n\n\nOne can use the buttons next to the search field to zoom in and out along the genome.  Press the x50 button to zoom out 50 fold\n\n\n\nThis will show a larger region of the chromosome.\n\n\n\nOne can now see three DNase peaks (1) around the rs13140464 variant(2).  In addition there is another variant (3) seen near one of the DNase peaks.\n\n\n\nObtaining additional variant information\n\n\nLeft clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.\n\n\n\nAdding and removing tracks\n\n\nSelect the tracks button from the top of the genome browser.\n\n\nA window displaying selected tracks and tracks available for selection will pop up.\n\n\nOne can scroll down to see additional tracks.  Try selecting and unselecting various tracks and observe the updated tracks on the browser.\n\n\nGetting DNA sequence\n\n\nSelect the 'More' button at the top of the browser.\n\n\nSeveral options will be avaialble.  Select the DNA sequence button.\n\n\nYou will be shown the DNA sequence for the region.\n\n\n\nVariants and Phenotype Viewer\n\n\nOverview\n\n\nWhen the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization.\n\n\nThe different elements of the view are as follows.\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nSettings and sort buttons.  In addition a link to this help document.\n\n\n\n\n\n\n2\n\n\nLegend for different tracks in the viewer\n\n\n\n\n\n\n3\n\n\nPhenotypic data displayed with an individual represented in each column\n\n\n\n\n\n\n4\n\n\nGentoypic data displayed with an individual represented in each column\n\n\n\n\n\n\n\n\nLabels\n\n\nSee glossary for further details\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHb\n\n\nHemoglobin\n\n\n\n\n\n\nHbF\n\n\nFetal Hemoglobin\n\n\n\n\n\n\nHbA2\n\n\nVariant of hemoglobin that contains two alpha subunits and two delta subunits\n\n\n\n\n\n\nMCV\n\n\nMean corpuscular  volume. This is the average size of red blood cells\n\n\n\n\n\n\nPainRate\n\n\nNumber of hospitalizations per year over a two year period.\n\n\n\n\n\n\nSickle cell genotype\n\n\n(SS_Genotype in legend.  Whether patient is SS or SB0\n\n\n\n\n\n\nAlpha deletion\n\n\nWhether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles)\n\n\n\n\n\n\nrs######\n\n\nSeveral variants that we have found to be associated with pain in Sickle Cell Disease\n\n\n\n\n\n\n\n\nGetting Started\n\n\nSorting\n\n\nHover your mouse over the MCV label in the graph.  A box will pop up with several icons.  Select the triangle that is pointed to the left to sort individuals by MCV.\n\n\n\nThe following graph shows individuals sorted by MCV.  Blank columns represent no data available.  Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.\n\n\n\nView values for all patients\n\n\nHovering over one column will enable the viewing of all  phenotypic values for that patient.\n\n\nUndo\n\n\nWhile exploring the data, one may inadvertently sort or remove data.  One can undo the changes by selecting the undo button at the top of the viewer.  The redo button will revert the undo.\n\n\n\nGlossary\n\n\n\n\n\n\n\n\nFetal hemoglobin (HbF)\n\nFetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subuints of beta-globin and two units of alpha-globin. \n\n\n\n\n\n\nHeriditary persistance of fetal hemoglobin (HPFH)\n\nIndividuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\nA \nmethod\n for reducing high dimensional data into low-dimensional representations.\n\n\n\n\n\n\nSC\n\nAn individual with one copy of the sickle cell allele \nrs334\n and one copy of \nhemoglobin C\n.\n\n\n\n\n\n\nS\n+\n\nAn individual with \nbeta-thalassemia\n who has one copy of the sickle cell allele \nrs334\n and one copy of a beta-globin gene that has reduced expression.\n\n\n\n\n\n\nS\n0\n\nAn individual with \nbeta-thalassemia\n who has one copy of the sickle cell allele \nrs334\n and one copy of a beta-globin gene that is not expressed or is deleted.\n\n\n\n\n\n\n\nSS\n \nAn individual with \nsickle cell disease\n who is homozygous for the sickle cell allele \nrs334\n.\n\n\n\n\n\n\nSCCRIP\n\nThe \nSickle Cell Research and Intervention Program\n.", 
            "title": "Sickle Cell Genomics Portal"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#genome-browser", 
            "text": "", 
            "title": "Genome Browser"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#overview", 
            "text": "Upon launching the browser, you will see an image similar to the one shown here.  A description of the elements of the browser are as follows:     #  Description      1  Navigation tools and track selector. ( See Navigation Buttons section )    2  DNase hypersensitivity tracks.  By default, four epigenetic tracks are shown.  These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells.  Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below)    3  RefSeq genes.  Gene models from the RefSeq database are displayed in this tracks.    4  -log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease.  The analysis has only been performed around the  KIAA1109/Tenr/IL2/IL21 region.   Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)).  The Y-axis for the track represents the -log10 of the p-value.  The higher the value, the more stastically significant the association between the variant and pain rate is.  Clicking on a variant will open op a window that gives further details about the variant.  (See Figure 3).    5  -log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease.  See (4) above for more information on this type of track.    6  Filters: Filters allow variants within tracks to be filtered by numerous citeria.   See Filter description", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#navigation-buttons", 
            "text": "#  Description      a  Location/Locus entry field.  One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID.    b  Browser zoom in and out    c  Tracks: Add or hide tracks (See section below on adding/hiding tracks)    d  More:  Save svg image of browser, get DNA sequence or highlight regions of the browser.", 
            "title": "Navigation buttons"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#filters", 
            "text": "#  Description      a  Filters for pain rate p-value track    b  Filters for age of first vaso-occlusive crisis (VOC) p-value    c  The highlighted filter shows which value is  used for the Y-axis on the browser track.  The value can be changed.    d  A highligthed value within a filter shows which filter value is set.  The number next to the filter represents the number of individuals that meet the filter criteria.", 
            "title": "Filters"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#finding-a-variant-of-interest", 
            "text": "A user can navigate to a gene or to a variant ID.\nEnter in the variant ID rs13140464 into the search text field at the top of the browser. (See below)  Pressing enter will center the browser of the selected variant.  (see below)", 
            "title": "Finding a variant of interest"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#zooming-in-and-out", 
            "text": "One can use the buttons next to the search field to zoom in and out along the genome.  Press the x50 button to zoom out 50 fold  This will show a larger region of the chromosome.  One can now see three DNase peaks (1) around the rs13140464 variant(2).  In addition there is another variant (3) seen near one of the DNase peaks.", 
            "title": "Zooming in and out"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#obtaining-additional-variant-information", 
            "text": "Left clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.", 
            "title": "Obtaining additional variant information"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#adding-and-removing-tracks", 
            "text": "Select the tracks button from the top of the genome browser. \nA window displaying selected tracks and tracks available for selection will pop up. \nOne can scroll down to see additional tracks.  Try selecting and unselecting various tracks and observe the updated tracks on the browser.", 
            "title": "Adding and removing tracks"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#getting-dna-sequence", 
            "text": "Select the 'More' button at the top of the browser. \nSeveral options will be avaialble.  Select the DNA sequence button. \nYou will be shown the DNA sequence for the region.", 
            "title": "Getting DNA sequence"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#variants-and-phenotype-viewer", 
            "text": "", 
            "title": "Variants and Phenotype Viewer"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#overview_1", 
            "text": "When the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization. \nThe different elements of the view are as follows.     #  Description      1  Settings and sort buttons.  In addition a link to this help document.    2  Legend for different tracks in the viewer    3  Phenotypic data displayed with an individual represented in each column    4  Gentoypic data displayed with an individual represented in each column", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#labels", 
            "text": "See glossary for further details     #  Description      Hb  Hemoglobin    HbF  Fetal Hemoglobin    HbA2  Variant of hemoglobin that contains two alpha subunits and two delta subunits    MCV  Mean corpuscular  volume. This is the average size of red blood cells    PainRate  Number of hospitalizations per year over a two year period.    Sickle cell genotype  (SS_Genotype in legend.  Whether patient is SS or SB0    Alpha deletion  Whether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles)    rs######  Several variants that we have found to be associated with pain in Sickle Cell Disease", 
            "title": "Labels"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#getting-started_1", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#sorting", 
            "text": "Hover your mouse over the MCV label in the graph.  A box will pop up with several icons.  Select the triangle that is pointed to the left to sort individuals by MCV.  The following graph shows individuals sorted by MCV.  Blank columns represent no data available.  Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.", 
            "title": "Sorting"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#view-values-for-all-patients", 
            "text": "Hovering over one column will enable the viewing of all  phenotypic values for that patient.", 
            "title": "View values for all patients"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#undo", 
            "text": "While exploring the data, one may inadvertently sort or remove data.  One can undo the changes by selecting the undo button at the top of the viewer.  The redo button will revert the undo.", 
            "title": "Undo"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#glossary", 
            "text": "Fetal hemoglobin (HbF) \nFetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subuints of beta-globin and two units of alpha-globin.     Heriditary persistance of fetal hemoglobin (HPFH) \nIndividuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease.    Principal Component Analysis (PCA) \nA  method  for reducing high dimensional data into low-dimensional representations.    SC \nAn individual with one copy of the sickle cell allele  rs334  and one copy of  hemoglobin C .    S + \nAn individual with  beta-thalassemia  who has one copy of the sickle cell allele  rs334  and one copy of a beta-globin gene that has reduced expression.    S 0 \nAn individual with  beta-thalassemia  who has one copy of the sickle cell allele  rs334  and one copy of a beta-globin gene that is not expressed or is deleted.    SS  \nAn individual with  sickle cell disease  who is homozygous for the sickle cell allele  rs334 .    SCCRIP \nThe  Sickle Cell Research and Intervention Program .", 
            "title": "Glossary"
        }, 
        {
            "location": "/guides/portals/pecan/", 
            "text": "PeCan Data Portal\n\n\nPeCan provides interactive visualizations of pediatric cancer mutations across various projects at St. Jude Children's Research Hospital and its collaborating institutions.\n\n\nHomepage\n\n\nThe \nPeCan homepage\n contains two main visualizations that work with each other to give a high level overview of the data being presented (SJ Cloud's \nPCGP\n dataset along with curated datasets from other instiutions such as \nTARGET\n, \ndkfz\n, and others).\n\n\nDonut Chart\n\n\nThe donut chart (shown below) gives an at-a-glance disease distribution and disease heiarchy. \n\n\n\n\nYou can hover over the various donut slices to glance at the number (and %) of samples being represented by that disease. The diseases are categorized in two three main root categories: 1) HM -Hematopoietic Malignancies, 2) BT -Brain Tumor, 3) ST -Solid Tumor.\n\n\nClick here\n for a full mapping of disease codes.\n\n\nBubble Chart\n\n\nAny slice (at any level) of the donut chart can be clicked on to select it, and reveal a bubble chart of related genes. \n\n\nNote that the dataset bar (shown below) on top of the bubble chart visualizes the distribution of selected data across the datsets used in this visualization. It will update dynamically as you interact with the donut chart and make different selctions.\n\n\n\n\nAn example of the bubble chart is shown below.\n\n\n\n\nYou can see the selected disease shown at the top (1). The bubbles represent the most prevalent genes in the selected disease sample set. The size of the bubble corresponds to the number of mutations in the set with that gene.\n\n\nFor some disease sets (like the one shown above), we have identified the most important disease pathway for the gene and have categorized them as such. This information is represented here via the use of colors. The legend at the bottom allows you to view the pathway information being shown (including the number of genes that are attached to each pathway).\n\n\nHovering over a pathway in the legend will highlight all matching genes. Clicking a gene will open it's ProteinPaint.\n\n\nProteinPaint\n\n\nProteinPaint\n is a web application for simultaneously visualizing genetic lesions (including sequence mutations and gene fusions) and RNA expression in pediatric cancers. You can find the ProteinPaint \npaper here\n. \n\n\nOverview\n\n\nThe image below shows an example ProteinPaint of the gene \nTP53\n annotated with descriptions of the many interactive elements of a ProteinPaint visualization. As you can see, there is a lot to explore.\n\n\n\n\n\n\nNote\n\n\nA detailed tutorial for ProteinPaint can be found \nhere\n. Please excuse the different location and formatting as we work to incorporate this into our main documentation pages. \n\n\n\n\nGlossary of Classes\n\n\nThe list below summarizes all classes of mutations used by ProteinPaint.\n\n\n\n\n\n\n\n\nMutation Class\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMISSENSE\n\n\na substitution variant in the coding region resulting in altered protein coding\n\n\n\n\n\n\nFRAMESHIFT\n\n\nan insertion or deletion variant that alters the protein coding frame\n\n\n\n\n\n\nNONSENSE\n\n\na variant altering protein coding to produce a premature stopgain or stoploss.\n\n\n\n\n\n\nPROTEINDEL\n\n\na deletion resulting in a loss of one or more codons from the product, but not altering the protein coding frame\n\n\n\n\n\n\nPROTEININS\n\n\nan insertion introducing one or more codons into the product, but not altering the protein coding frame\n\n\n\n\n\n\nSPLICE\n\n\na variant near an exon edge that may affect splicing functionality\n\n\n\n\n\n\nSILENT\n\n\na substitution variant in the coding region that does not alter protein coding\n\n\n\n\n\n\nSPLICE_REGION\n\n\na variant in an intron within 10 nt of an exon boundary\n\n\n\n\n\n\nUTR_5\n\n\na variant in the 5' untranslated region\n\n\n\n\n\n\nUTR_3\n\n\na variant in the 3' untranslated region\n\n\n\n\n\n\nEXON\n\n\na variant in the exon of a non-coding RNA\n\n\n\n\n\n\nINTRON\n\n\nan intronic variant\n\n\n\n\n\n\n\n\nGlossary of Origins\n\n\nThe list below summarizes all origins of mutations used by ProteinPaint.\n\n\n\n\n\n\n\n\nMutation Origin\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGermline\n\n\na variant found in a normal sample of a cancer patient.\n\n\n\n\n\n\nSomatic\n\n\na variant found only in a tumor sample.\n\n\n\n\n\n\nRelapse\n\n\na variant that arose in recurrence tumor.\n\n\n\n\n\n\n\n\nStudies\n\n\nHere\n you can explore recently published, interactive visualizations that have been created in ProteinPaint.\n\n\nPeCan Pie\n\n\nPeCan Pie documentation can be found \nhere\n.", 
            "title": "PeCan Data Portal"
        }, 
        {
            "location": "/guides/portals/pecan/#pecan-data-portal", 
            "text": "PeCan provides interactive visualizations of pediatric cancer mutations across various projects at St. Jude Children's Research Hospital and its collaborating institutions.", 
            "title": "PeCan Data Portal"
        }, 
        {
            "location": "/guides/portals/pecan/#homepage", 
            "text": "The  PeCan homepage  contains two main visualizations that work with each other to give a high level overview of the data being presented (SJ Cloud's  PCGP  dataset along with curated datasets from other instiutions such as  TARGET ,  dkfz , and others).", 
            "title": "Homepage"
        }, 
        {
            "location": "/guides/portals/pecan/#donut-chart", 
            "text": "The donut chart (shown below) gives an at-a-glance disease distribution and disease heiarchy.    You can hover over the various donut slices to glance at the number (and %) of samples being represented by that disease. The diseases are categorized in two three main root categories: 1) HM -Hematopoietic Malignancies, 2) BT -Brain Tumor, 3) ST -Solid Tumor.  Click here  for a full mapping of disease codes.", 
            "title": "Donut Chart"
        }, 
        {
            "location": "/guides/portals/pecan/#bubble-chart", 
            "text": "Any slice (at any level) of the donut chart can be clicked on to select it, and reveal a bubble chart of related genes.   Note that the dataset bar (shown below) on top of the bubble chart visualizes the distribution of selected data across the datsets used in this visualization. It will update dynamically as you interact with the donut chart and make different selctions.   An example of the bubble chart is shown below.   You can see the selected disease shown at the top (1). The bubbles represent the most prevalent genes in the selected disease sample set. The size of the bubble corresponds to the number of mutations in the set with that gene.  For some disease sets (like the one shown above), we have identified the most important disease pathway for the gene and have categorized them as such. This information is represented here via the use of colors. The legend at the bottom allows you to view the pathway information being shown (including the number of genes that are attached to each pathway).  Hovering over a pathway in the legend will highlight all matching genes. Clicking a gene will open it's ProteinPaint.", 
            "title": "Bubble Chart"
        }, 
        {
            "location": "/guides/portals/pecan/#proteinpaint", 
            "text": "ProteinPaint  is a web application for simultaneously visualizing genetic lesions (including sequence mutations and gene fusions) and RNA expression in pediatric cancers. You can find the ProteinPaint  paper here .", 
            "title": "ProteinPaint"
        }, 
        {
            "location": "/guides/portals/pecan/#overview", 
            "text": "The image below shows an example ProteinPaint of the gene  TP53  annotated with descriptions of the many interactive elements of a ProteinPaint visualization. As you can see, there is a lot to explore.    Note  A detailed tutorial for ProteinPaint can be found  here . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/portals/pecan/#glossary-of-classes", 
            "text": "The list below summarizes all classes of mutations used by ProteinPaint.     Mutation Class  Description      MISSENSE  a substitution variant in the coding region resulting in altered protein coding    FRAMESHIFT  an insertion or deletion variant that alters the protein coding frame    NONSENSE  a variant altering protein coding to produce a premature stopgain or stoploss.    PROTEINDEL  a deletion resulting in a loss of one or more codons from the product, but not altering the protein coding frame    PROTEININS  an insertion introducing one or more codons into the product, but not altering the protein coding frame    SPLICE  a variant near an exon edge that may affect splicing functionality    SILENT  a substitution variant in the coding region that does not alter protein coding    SPLICE_REGION  a variant in an intron within 10 nt of an exon boundary    UTR_5  a variant in the 5' untranslated region    UTR_3  a variant in the 3' untranslated region    EXON  a variant in the exon of a non-coding RNA    INTRON  an intronic variant", 
            "title": "Glossary of Classes"
        }, 
        {
            "location": "/guides/portals/pecan/#glossary-of-origins", 
            "text": "The list below summarizes all origins of mutations used by ProteinPaint.     Mutation Origin  Description      Germline  a variant found in a normal sample of a cancer patient.    Somatic  a variant found only in a tumor sample.    Relapse  a variant that arose in recurrence tumor.", 
            "title": "Glossary of Origins"
        }, 
        {
            "location": "/guides/portals/pecan/#studies", 
            "text": "Here  you can explore recently published, interactive visualizations that have been created in ProteinPaint.", 
            "title": "Studies"
        }, 
        {
            "location": "/guides/portals/pecan/#pecan-pie", 
            "text": "PeCan Pie documentation can be found  here .", 
            "title": "PeCan Pie"
        }, 
        {
            "location": "/guides/portals/genome-paint/", 
            "text": "Genome Paint\n\n\nGenomePaint\n is a visualization browser for simultaneously viewing genomic, transcriptomic, and epigenomic pediatric cancer mutation datasets across a multitude of disease cohorts. GenomePaint datasets include WGS, WES, RNA-Seq, SNP-chip, ChIP-Seq, and Hi-C data visualized over the hg19 reference genome. You can use GenomePaint to interpret the impact of somatic coding and noncoding alterations from ~3,800 pediatric tumors, make novel discoveries through visual exploration, and create publication ready figures!\n\n\nGetting Started\n\n\nThe GenomePaint browser homepage lands on a dense cohort view of the TAL1 gene region of chromosome 1. Each section of the display can be interacted with by clicking, dragging, or hovering. Filter the information displayed on the tracks by clicking buttons in the legend. Customize the legend display by hiding/showing classes. Click \nCONFIG\n to the right of tracks for additional display customization.\n\n\n\n\nTo navigate tracks,\n\n\n\n\nPan left or right by clicking on the middle part of the track and dragging\n\n\nZoom in by draging on the genomic coordinate ruler on top or zoom in 1 fold by clicking on the \nIN\n button\n\n\nZoom out by \nx\n fold by clicking on an \nOUT\n button.\n\n\n\n\n\n\nYou can even zoom in to display mutations at bp resolution. \n\n\n\n\nGenome Paint offers three different views: cohort view, sample view, and matrix view. The figure below summarizes how each view is connected.\n\n\n\n\nCohort View\n\n\nThe cohort view shows mutations from all samples over a genomic region, along with the gene expression ranks for each of the samples. By default the mutation track displays the cohort view in dense mode, a compact display showing density plots for SVs and SNV/indels. You can toggle the view to expanded mode by clicking the \nCONFIG\n button to the right of the mutation track and then clicking Expanded.\n\n\n\n\nIn Expanded mode (see below) all types of mutations are shown for each sample, one row per sample. Circles represent SV/fusion breakpoints, and x marks represent SNV/indels, each of which are displayed together with CNV/LOH. SNV/indels and breakpoints are always shown on top of CNV and LOH. Text labels can be shown for SV/fusion/SNV/indel, if available.\n\n\n\n\nSample View\n\n\nThe sample view shows mutations for one sample alone, along with any available genomic assay tracks. You can open a sample view from the expanded cohort view by clicking on any type of single mutation within the sample, and then selecting \nFocus\n. This brings up a new browser view showing data tracks from this sample in the region surrounding the mutation.\n\n\n\n\nOn the sample view you can explore expression rank, tumor mutations, structural variants, splice junctions, WES coverage, and RNA-Seq coverage. Customize the display by zooming in/out, hiding and/or rearraging tracks, or editing \nCONFIG\n options.\n\n\nMatrix View\n\n\nThe matrix view combines the mutation profiles of multiple genomic regions in one view, in the form of a sample-by-region matrix. Such a matrix can be generated for samples from one cancer type. To open a matrix view, select a disease cohort from the cohort view and then select Matrix view. This organizes the selected cohort tumors with mutations in the genomic region you are viewing into a single-column matrix.\n\n\n\n\nNext, go back to the cohort view and type another gene or region of interest into the genome coordinate box and press ENTER. The cohort view will now show data at the new genomic location. Click on the same disease cohort and then select Matrix view.\n\n\n\n\nThis will add the new genomic variant as a second column in the matrix. You can continue adding columns to this matrix in the same manner.\n\n\n\n\nNote\n\n\nA detailed tutorial for GenomePaint can be found \nhere\n. You'll also find instructions to create custom tracks, visualize your own data, and embed images on your web page. Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.", 
            "title": "Genome Paint"
        }, 
        {
            "location": "/guides/portals/genome-paint/#genome-paint", 
            "text": "GenomePaint  is a visualization browser for simultaneously viewing genomic, transcriptomic, and epigenomic pediatric cancer mutation datasets across a multitude of disease cohorts. GenomePaint datasets include WGS, WES, RNA-Seq, SNP-chip, ChIP-Seq, and Hi-C data visualized over the hg19 reference genome. You can use GenomePaint to interpret the impact of somatic coding and noncoding alterations from ~3,800 pediatric tumors, make novel discoveries through visual exploration, and create publication ready figures!", 
            "title": "Genome Paint"
        }, 
        {
            "location": "/guides/portals/genome-paint/#getting-started", 
            "text": "The GenomePaint browser homepage lands on a dense cohort view of the TAL1 gene region of chromosome 1. Each section of the display can be interacted with by clicking, dragging, or hovering. Filter the information displayed on the tracks by clicking buttons in the legend. Customize the legend display by hiding/showing classes. Click  CONFIG  to the right of tracks for additional display customization.   To navigate tracks,   Pan left or right by clicking on the middle part of the track and dragging  Zoom in by draging on the genomic coordinate ruler on top or zoom in 1 fold by clicking on the  IN  button  Zoom out by  x  fold by clicking on an  OUT  button.    You can even zoom in to display mutations at bp resolution.    Genome Paint offers three different views: cohort view, sample view, and matrix view. The figure below summarizes how each view is connected.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/portals/genome-paint/#cohort-view", 
            "text": "The cohort view shows mutations from all samples over a genomic region, along with the gene expression ranks for each of the samples. By default the mutation track displays the cohort view in dense mode, a compact display showing density plots for SVs and SNV/indels. You can toggle the view to expanded mode by clicking the  CONFIG  button to the right of the mutation track and then clicking Expanded.   In Expanded mode (see below) all types of mutations are shown for each sample, one row per sample. Circles represent SV/fusion breakpoints, and x marks represent SNV/indels, each of which are displayed together with CNV/LOH. SNV/indels and breakpoints are always shown on top of CNV and LOH. Text labels can be shown for SV/fusion/SNV/indel, if available.", 
            "title": "Cohort View"
        }, 
        {
            "location": "/guides/portals/genome-paint/#sample-view", 
            "text": "The sample view shows mutations for one sample alone, along with any available genomic assay tracks. You can open a sample view from the expanded cohort view by clicking on any type of single mutation within the sample, and then selecting  Focus . This brings up a new browser view showing data tracks from this sample in the region surrounding the mutation.   On the sample view you can explore expression rank, tumor mutations, structural variants, splice junctions, WES coverage, and RNA-Seq coverage. Customize the display by zooming in/out, hiding and/or rearraging tracks, or editing  CONFIG  options.", 
            "title": "Sample View"
        }, 
        {
            "location": "/guides/portals/genome-paint/#matrix-view", 
            "text": "The matrix view combines the mutation profiles of multiple genomic regions in one view, in the form of a sample-by-region matrix. Such a matrix can be generated for samples from one cancer type. To open a matrix view, select a disease cohort from the cohort view and then select Matrix view. This organizes the selected cohort tumors with mutations in the genomic region you are viewing into a single-column matrix.   Next, go back to the cohort view and type another gene or region of interest into the genome coordinate box and press ENTER. The cohort view will now show data at the new genomic location. Click on the same disease cohort and then select Matrix view.   This will add the new genomic variant as a second column in the matrix. You can continue adding columns to this matrix in the same manner.   Note  A detailed tutorial for GenomePaint can be found  here . You'll also find instructions to create custom tracks, visualize your own data, and embed images on your web page. Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.", 
            "title": "Matrix View"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-DAA/", 
            "text": "Filling Out The DAA\n\n\nPlease read carefully through the first 5 pages of the \nData Access Agreement (DAA)\n which consist of Terms and Conditions that you and your institution must agree to in order to access any data on St. Jude Cloud.\n\n\nNext, ensure that you have correctly filled out the 5 required sections of the DAA (Note that there are \n2 additional required sections\n if you intend to download data):\n\n\n\n\n\n\nPage 5\n \nThe Data Access Unit(s)\n you are applying for must be marked.\n\n\n\n\nTip: The DAU(s) associated to the data you requested are listed in the section Controlled Access Data, above the Download Data Request\n  Form button. This can be found on the third step of the data request\n  process. This is a dynamic feature\n  that allows the user to see exactly which Data Access Units (datasets)\n  they are requesting data from.\n\n\n\n\n\n\n\n\n\n\nPage 6\n Signature and information of the Principal Investigator. \n\n\n\n\nThis must be signed by a Principal Investigator or a faculty-level supervisor on the project.\n\n\n\n\n\n\n\n\n\n\nPage 7\n Signature and information of all other applicants. \n\n\n\n\nThis should include any person(s) who will have access to this data. They are legally bound to protecting and handling the data properly.\n\n\n\n\n\n\n\n\n\n\nPage 8\n Signature and information of Institutional or Administrative Authority. \n\n\n\n\nThis individual cannot be the same Principal Investigator that signed above, as this section is to provide a second-party authority of the instituion to ensure that the institution will uphold the terms of this agreement.\n\n\n\n\n\n\n\n\n\n\nPage 10\n Description of contemplated use of St. Jude data. \n\n\n\n\nHere, describe your research question and it's biological significance. The Contemplated Use will be evaluated by the \nData Access Committee(s)\n based on their own set of protocols. \n\n\nPlease \ncontact us\n if you have any questions regarding the protocols of the approval process. \n\n\n\n\n\n\n\n\nData Download Permission\n\n\nAdditionally, \nif and only if\n you would like to \ndownload\n the data,\nyou will also need to include the following:\n\n\n\n\n\n\nPage 5\n The applicant's initials on the line below the checkboxes for DAU(s).\n\n\n\n\n\n\n\n\nPage 9\n Signature and information of the Information Technology Director or Chief Information Security Officer.\n\n\n\n\n\n\n\n\nUploading A Revised DAA\n\n\nIf your DAA is incomplete (for example you missed a required signature or neglected to check the box next to a dataset for which you requested data), you will be contacted by a member of the St. Jude Cloud team. Once you have made the required edits, you can reupload a revised DAA through the \nManage Data page\n by clicking on the \nAdd a Form\n button.\n\n\n\n\nFrequently Asked Questions\n\n\nQ. Why do I need to sign the DAA?\n\n\nA.\n Although the DAA serves many purposes, the terms included in the data access\nagreement are ultimately in place to protect our patients. We take\npatient security very seriously, and we require that requestors are\ncommitted to protecting that privacy to the fullest extent.\n\n\nQ. Where can I find the latest version of the DAA?\n\n\nA.\n We keep \nour site\n up to date with the latest version on the Data Access Agreement for you to download, or you can download a copy\n\nhere\n. \n\n\nQ. Where do I submit the DAA??\n\n\nA.\n You can submit your Data Access Agreement in the drag and drop box on the last step of submitting your data request.", 
            "title": "Filling out a Data Access Agreement"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#filling-out-the-daa", 
            "text": "Please read carefully through the first 5 pages of the  Data Access Agreement (DAA)  which consist of Terms and Conditions that you and your institution must agree to in order to access any data on St. Jude Cloud.  Next, ensure that you have correctly filled out the 5 required sections of the DAA (Note that there are  2 additional required sections  if you intend to download data):    Page 5   The Data Access Unit(s)  you are applying for must be marked.   Tip: The DAU(s) associated to the data you requested are listed in the section Controlled Access Data, above the Download Data Request\n  Form button. This can be found on the third step of the data request\n  process. This is a dynamic feature\n  that allows the user to see exactly which Data Access Units (datasets)\n  they are requesting data from.      Page 6  Signature and information of the Principal Investigator.    This must be signed by a Principal Investigator or a faculty-level supervisor on the project.      Page 7  Signature and information of all other applicants.    This should include any person(s) who will have access to this data. They are legally bound to protecting and handling the data properly.      Page 8  Signature and information of Institutional or Administrative Authority.    This individual cannot be the same Principal Investigator that signed above, as this section is to provide a second-party authority of the instituion to ensure that the institution will uphold the terms of this agreement.      Page 10  Description of contemplated use of St. Jude data.    Here, describe your research question and it's biological significance. The Contemplated Use will be evaluated by the  Data Access Committee(s)  based on their own set of protocols.   Please  contact us  if you have any questions regarding the protocols of the approval process.", 
            "title": "Filling Out The DAA"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#data-download-permission", 
            "text": "Additionally,  if and only if  you would like to  download  the data,\nyou will also need to include the following:    Page 5  The applicant's initials on the line below the checkboxes for DAU(s).     Page 9  Signature and information of the Information Technology Director or Chief Information Security Officer.", 
            "title": "Data Download Permission"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#uploading-a-revised-daa", 
            "text": "If your DAA is incomplete (for example you missed a required signature or neglected to check the box next to a dataset for which you requested data), you will be contacted by a member of the St. Jude Cloud team. Once you have made the required edits, you can reupload a revised DAA through the  Manage Data page  by clicking on the  Add a Form  button.", 
            "title": "Uploading A Revised DAA"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#frequently-asked-questions", 
            "text": "Q. Why do I need to sign the DAA?  A.  Although the DAA serves many purposes, the terms included in the data access\nagreement are ultimately in place to protect our patients. We take\npatient security very seriously, and we require that requestors are\ncommitted to protecting that privacy to the fullest extent.  Q. Where can I find the latest version of the DAA?  A.  We keep  our site  up to date with the latest version on the Data Access Agreement for you to download, or you can download a copy here .   Q. Where do I submit the DAA??  A.  You can submit your Data Access Agreement in the drag and drop box on the last step of submitting your data request.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-Extension/", 
            "text": "Filling Out The Extension Addendum\n\n\nThe St. Jude Cloud \nData Access Agreement (DAA)\n is only valid for one year after the date it was approved. When your DAA is about to expire, you will get an automated email from notifications@stjude.cloud with the name of the DAA that is expiring and a link to the St. Jude Cloud Extension Addendum.\n\n\nIn order to extend your DAA, you must fill out an Extension Addendum. The Extension Addendum will extend the previous agreement for an additional year. Please note that if you do not fill out an Extension Addendum, you will be expected to delete all copies of the data subject to the expiring agreement.\n\n\nFollow the steps below to ensure that you have acurately filled out all sections of the Extension Addendum.\n\n\n\n\n\n\nPage 1\n\n\n\n\nIn the top section, enter the current date on which that the agreement is being filled out, your current institution, and the date on which you signed the expiring DAA or extension.\n\n\n\n\n\n\n\n\nIn the bottom section, enter a date that is one year after the date on which you signed the expiring DAA or extension. This can be found on the page linked from the email notifying you of agreement expiration.\n\n\n\n\n\n\n\n\n\n\nPage 2\n\n\n\n\n\n\nCheck exactly the datasets that you were granted access to by the terms of the original DAA. \nThe datasets checked on the original DAA and any extensions must match.\n If you would like to apply for access to additional datasets, please \nmake a new data request\n for the additional dataset(s).\n\n\n\n\n\n\nThe Principal Investigator (PI) or faculty level supervisor on the project must sign and date the extension. \nThe PI who signed the original DAA must match the PI signing any extensions.\n\n\n\n\n\n\nAll additional applicants (excluding the administrative authority and information security officer) that were included in the original DAA must sign and date the extension.\n\n\n\n\n\n\n\n\n\n\nPage 3\n\n\n\n\n\n\nEnter the name of your current institution. This must match the institution entered on page 1.\n\n\n\n\n\n\nThe Administrative Authority must sign and date the extension. This is usually the same administrative authority as the one on the original DAA.\n\n\n\n\n\n\nThe Information Security Officer must sign and date the extension.\n\n\n\n\n\n\nThe bottom section of page 3 is for St. Jude to sign and date. Do not fill out this section.\n     \n\n\n\n\n\n\n\n\n\n\nPage 4\n\n\n\n\nIf your research question(s) or contemplated use has changed since the original DAA, use this space to provide your updated project description. You may also use this space to explain why you need to extend your agreement. Finally, if the Administrative Authority or Information Security Officer has changed from the original DAA to this extension, please use this space to explain why.\n\n\n\n\n\n\n\n\nOnce you have finished filling out the Extension Addendum, you may upload the completed form on the extension addendum page linked in the notification of expiration email from notifications@stjude.cloud.\n\n\nFrequently Asked Questions\n\n\nQ. What if I did not fill out the Data Download Permission section of the origianl DAA, but now I want to download data?\n\n\nA.\n Since this would be a change in terms from the original agreement, you would need to fill out a new DAA (including the Data Download Permission section) for any datasets you want to download.", 
            "title": "Filling out an Extension Addendum"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-Extension/#filling-out-the-extension-addendum", 
            "text": "The St. Jude Cloud  Data Access Agreement (DAA)  is only valid for one year after the date it was approved. When your DAA is about to expire, you will get an automated email from notifications@stjude.cloud with the name of the DAA that is expiring and a link to the St. Jude Cloud Extension Addendum.  In order to extend your DAA, you must fill out an Extension Addendum. The Extension Addendum will extend the previous agreement for an additional year. Please note that if you do not fill out an Extension Addendum, you will be expected to delete all copies of the data subject to the expiring agreement.  Follow the steps below to ensure that you have acurately filled out all sections of the Extension Addendum.    Page 1   In the top section, enter the current date on which that the agreement is being filled out, your current institution, and the date on which you signed the expiring DAA or extension.     In the bottom section, enter a date that is one year after the date on which you signed the expiring DAA or extension. This can be found on the page linked from the email notifying you of agreement expiration.      Page 2    Check exactly the datasets that you were granted access to by the terms of the original DAA.  The datasets checked on the original DAA and any extensions must match.  If you would like to apply for access to additional datasets, please  make a new data request  for the additional dataset(s).    The Principal Investigator (PI) or faculty level supervisor on the project must sign and date the extension.  The PI who signed the original DAA must match the PI signing any extensions.    All additional applicants (excluding the administrative authority and information security officer) that were included in the original DAA must sign and date the extension.      Page 3    Enter the name of your current institution. This must match the institution entered on page 1.    The Administrative Authority must sign and date the extension. This is usually the same administrative authority as the one on the original DAA.    The Information Security Officer must sign and date the extension.    The bottom section of page 3 is for St. Jude to sign and date. Do not fill out this section.\n           Page 4   If your research question(s) or contemplated use has changed since the original DAA, use this space to provide your updated project description. You may also use this space to explain why you need to extend your agreement. Finally, if the Administrative Authority or Information Security Officer has changed from the original DAA to this extension, please use this space to explain why.     Once you have finished filling out the Extension Addendum, you may upload the completed form on the extension addendum page linked in the notification of expiration email from notifications@stjude.cloud.", 
            "title": "Filling Out The Extension Addendum"
        }, 
        {
            "location": "/guides/forms/how-to-fill-out-Extension/#frequently-asked-questions", 
            "text": "Q. What if I did not fill out the Data Download Permission section of the origianl DAA, but now I want to download data?  A.  Since this would be a change in terms from the original agreement, you would need to fill out a new DAA (including the Data Download Permission section) for any datasets you want to download.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/guides/glossary/data-access-unit/", 
            "text": "A St. Jude Cloud \nData Access Unit (DAU)\n is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. Each DAU has its own governing body of researchers, the \nData Access Committee\n, who preside over the data and who may grant or deny access.\n\n\nWe currently have the 5 DAUs detailed below.\n\n\nPediatric Cancer Genome Project (PCGP)\n\n\nPCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer.\n\nThe Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients. \n\n\nSt. Jude Lifetime (SJLIFE)\n\n\nSJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy.\n\nSt. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples. \n\n\nClinical Genomics (Clinical Pilot and G4K)\n\n\nClinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors.\n\nClinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of two studies: Clinical Pilot and Genomes4Kids. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. These studies aim to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors. \n\n\nSickle Cell Genome Project (SGP)\n\n\nSGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood.\n\nThe Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.\n\n\nChildhood Cancer Survivor Study (CCSS)\n\n\nCCSS is a germline-only dataset consisting of whole genome sequencing of childhood cancer survivors.\n\nCCSS is a multi-institutional, multi-disciplinary, NCI-funded collaborative resource established to evaluate long-term outcomes among survivors of childhood cancer. It is a retrospective cohort consisting of \n24,000 five-year survivors of childhood cancer who were diagnosed between 1970-1999 at one of 31 participating centers in the U.S. and Canada. The primary purpose of this sequencing of CCSS participants is to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy.\n\n\n\n\nPotential Bacterial Contamination\n\n\nSamples for the Childhood Cancer Survivorship Study were collected by sending out Buccal swab kits to enrolled participants and having them complete the kits at home. This mechanism of collecting saliva and buccal cells for sequencing is highly desirable because of its uninvasive nature and ease of execution. However, collection of samples in this manner also has higher probability of contamination from external sources (as compared to, say, samples collected using blood). We have observed some samples in this cohort which suffer from bacterial contamination. To address this issue, we have taken the following steps:\n\n\n\n\nWe have estimated the bacterial contamination rate and annotated each of the samples in the CCSS cohort. For each sample, you will find the estimated contamination rate in the \nDescription\n field of the \nSAMPLE_INFO.txt\n file that is vended with your data (and as a property on the DNAnexus file). For information on this field, see the \nMetadata specification\n.\n\n\nUsing this estimated contamination rate, we have removed 82 samples which exhibited large rates of bacterial contamination.\n\n\nFor the remaining samples, we have provided the \nBAM\n file as aligned with \nbwa mem\n with default parameters. We have observed that there are instances of reads originating from bacterial contamination that are erroneously mapped to the human genome and display a \nvery\n low mapping quality. Please be advised that we have kept these reads as they were aligned and have not yet made any attempt to unmap these reads. Any analysis you perform on these samples will need to take this into account!\n\n\nLast, we will be working over the coming months to unmap the reads originating from bacterial contamination and release updated \nBAM\n files along with the associated \ngVCF\n files from Microsoft Genomics Service.\n\n\n\n\nWith any questions on the nature or implications of this warning, please contact us at \nsupport@stjude.cloud\n.", 
            "title": "Data Access Unit"
        }, 
        {
            "location": "/guides/glossary/data-access-unit/#pediatric-cancer-genome-project-pcgp", 
            "text": "PCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer. \nThe Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients.", 
            "title": "Pediatric Cancer Genome Project (PCGP)"
        }, 
        {
            "location": "/guides/glossary/data-access-unit/#st-jude-lifetime-sjlife", 
            "text": "SJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy. \nSt. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples.", 
            "title": "St. Jude Lifetime (SJLIFE)"
        }, 
        {
            "location": "/guides/glossary/data-access-unit/#clinical-genomics-clinical-pilot-and-g4k", 
            "text": "Clinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors. \nClinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of two studies: Clinical Pilot and Genomes4Kids. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. These studies aim to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors.", 
            "title": "Clinical Genomics (Clinical Pilot and G4K)"
        }, 
        {
            "location": "/guides/glossary/data-access-unit/#sickle-cell-genome-project-sgp", 
            "text": "SGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood. \nThe Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.", 
            "title": "Sickle Cell Genome Project (SGP)"
        }, 
        {
            "location": "/guides/glossary/data-access-unit/#childhood-cancer-survivor-study-ccss", 
            "text": "CCSS is a germline-only dataset consisting of whole genome sequencing of childhood cancer survivors. \nCCSS is a multi-institutional, multi-disciplinary, NCI-funded collaborative resource established to evaluate long-term outcomes among survivors of childhood cancer. It is a retrospective cohort consisting of  24,000 five-year survivors of childhood cancer who were diagnosed between 1970-1999 at one of 31 participating centers in the U.S. and Canada. The primary purpose of this sequencing of CCSS participants is to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy.   Potential Bacterial Contamination  Samples for the Childhood Cancer Survivorship Study were collected by sending out Buccal swab kits to enrolled participants and having them complete the kits at home. This mechanism of collecting saliva and buccal cells for sequencing is highly desirable because of its uninvasive nature and ease of execution. However, collection of samples in this manner also has higher probability of contamination from external sources (as compared to, say, samples collected using blood). We have observed some samples in this cohort which suffer from bacterial contamination. To address this issue, we have taken the following steps:   We have estimated the bacterial contamination rate and annotated each of the samples in the CCSS cohort. For each sample, you will find the estimated contamination rate in the  Description  field of the  SAMPLE_INFO.txt  file that is vended with your data (and as a property on the DNAnexus file). For information on this field, see the  Metadata specification .  Using this estimated contamination rate, we have removed 82 samples which exhibited large rates of bacterial contamination.  For the remaining samples, we have provided the  BAM  file as aligned with  bwa mem  with default parameters. We have observed that there are instances of reads originating from bacterial contamination that are erroneously mapped to the human genome and display a  very  low mapping quality. Please be advised that we have kept these reads as they were aligned and have not yet made any attempt to unmap these reads. Any analysis you perform on these samples will need to take this into account!  Last, we will be working over the coming months to unmap the reads originating from bacterial contamination and release updated  BAM  files along with the associated  gVCF  files from Microsoft Genomics Service.   With any questions on the nature or implications of this warning, please contact us at  support@stjude.cloud .", 
            "title": "Childhood Cancer Survivor Study (CCSS)"
        }, 
        {
            "location": "/guides/glossary/data-access-agreement/", 
            "text": "A St. Jude Cloud \nData Access Agreement (DAA)\n is a legally binding document outlining a number of terms and conditions to which anyone working with St. Jude Cloud data must agree. We do not negotiate the terms of this document\nunless terms are found to be in conflict with the institution's state law. \nFilling out the Data Access Agreement carefully and completely is crucial \nto having your request approved promptly. \nClick Here\n  for a step by step guide on how to fill out the DAA.\n\n\nIf you have incompletely or incorrectly filled out your DAA and would like to upload a revised form, \nClick Here\n for instructions.\n\n\nOnce you have submitted a correctly filled out DAA and have been granted access to one or more \nData Acess Units (DAUs)\n, you can continue checking out files from those DAUs until your access expires. Access is generally grant for 1 year, at which point you must submit an Extension Addendum to continue using the data. \nClick Here\n for a step-by-step guide on how to fill out the Extension Addendum.", 
            "title": "Data Access Agreement"
        }, 
        {
            "location": "/guides/glossary/data-access-committee/", 
            "text": "A St. Jude Cloud \nData Access Committee (DAC)\n is group of St. Jude researchers who oversee access to a particular \nData Access Unit (DAU)\n and evaluate incoming data requests.\n\n\nThe first time you request access to files in \na DAU, it is required that you fill out a \nData Access Agreement (DAA)\n. Access is granted at the DAU level\nbased on the decision of each DAC upon reviewing the DAA.\n\n\n\n\nExample\n\n\nFor example, if you make a request asking for all of St. Jude's Acute \nLymphoblastic Leukemia sequencing data, you might be asking for data from \nmultiple different projects (DAUs) here at St. Jude. For the sake of the example,\nlet's say the data you want is spread across three different DAUs. Once\nyou place a request, your application will be routed to the corresponding\nthree data access committees for approval. Since each DAC is made up of\ndifferent individuals using different criteria for evaluation, you may or\nmay not be approved for access to all of the files.", 
            "title": "Data Access Committee"
        }, 
        {
            "location": "/guides/glossary/embargo-date/", 
            "text": "The \nEmbargo Date\n specifies the date that a publishing embargo on the file in question has been lifted. Publishing using any of the files \nbefore\n the embargo date has passed is strictly prohibited as outlined in the \nData Access Agreement (DAA)\n. Typically, samples from the same \nData Access Unit (DAU)\n all have the same embargo date, as they would have been released on St.\nJude Cloud at the same time.\n\n\nCurrent Embargo Dates\n\n\n\n\n\n\n\n\nData Access Unit\n\n\nEmbargo Date\n\n\n\n\n\n\n\n\n\n\nPediatric Cancer Genome Project\n\n\nJuly 23, 2018\n\n\n\n\n\n\nSt. Jude LIFE\n\n\nJanuary 15, 2019\n\n\n\n\n\n\nClinical Genomics\n\n\nJanuary 15, 2019\n\n\n\n\n\n\nSickle Cell Genome Project\n\n\nSeptember 1, 2019\n\n\n\n\n\n\nChildhood Cancer Survivor Study\n\n\nNovember 1, 2019", 
            "title": "Embargo Date"
        }, 
        {
            "location": "/faq/", 
            "text": "Q. Will I be charged for using St. Jude Cloud?\n\n\nA. Any copy of the St. Jude data you receive is considered \"sponsored\",\n  so you do not have to pay a fee to store this data in St. Jude\n  Cloud. Although you may be prompted to enter billing information, you will not\nbe charged for anything with the exception of the following actions:\n\n\n\n\nYou will be charged for any \nderivative\n files stored on the St. Jude Cloud, such as results files obtained through running analyses workflows.\n\n\nThere is a small monthly storage fee associated for any of your own data you upload to the cloud.\n\n\nIf you elect to \ndownload\n any data from SJCloud, you will be\n  charged an egress fee by DNAnexus. This fee is usually negligible\n  unless you are downloading entire cohorts. We are actively\n  investigating ways to minimize or eliminate these costs.\n\n\nIf you run any of our analysis workflows (such as Rapid RNA-Seq, WARDEN, etc) or you own workflows that you have uploaded and packaged into the cloud, you will be charged for the\n  compute resources used in producing the results as well as storage\n  fees associated with storing the results files.\n\n\n\n\nQ. How can I delete my account?\n\n\nA. If you'd like to delete your account, please email DNAnexus support at\n\n with the following email.\n\n\nHi DNAnexus,\n\n  Would you please assist me in deleting my St. Jude Cloud account? My username is _____.\n\nThank you!\n\n\n\n\n\nQ. Where can I find the Terms of Service or the Privacy Policy?\n\n\nA. You can find the Terms of Service\n\nhere\n and the Privacy Policy\n\nhere\n.\n\n\nQ. Where can I find the embargo dates?\n\n\nA.\n All of our samples are marked with an embargo date. \nYou can find this by looking at the tags for each file or in the\n\nSAMPLE_INFO.txt\n file that is included with each data request. \nSelect a sample and click info to see more.", 
            "title": "Frequently Asked Questions"
        }
    ]
}